{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Power Map Generation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25ddd2d58a03156c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import packages"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "711b35e2f4e172b2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pmnet_v3 import PMNet"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset preparation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4374c52ab41c6df2"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def load_maps(dir_base: str = \"usc\", indices: np.ndarray = np.arange(100, dtype=int)) -> dict[int, np.ndarray]:\n",
    "    \"\"\"Load pixel maps as np.ndarray from images.\n",
    "    \n",
    "    \"\"\"\n",
    "    arr_maps = {}\n",
    "    dir_maps = os.path.join(dir_base, \"map\")\n",
    "    for idx in indices:\n",
    "        filename = os.path.join(dir_maps, str(idx)) + \".png\"\n",
    "        arr_map = Image.open(filename).convert('L')\n",
    "        arr_maps[idx] = (np.array(arr_map, dtype=np.float32) - 0) / 255  # 256 x 256 matrix with value in [0,1] (grayscale)\n",
    "        # arr_maps[idx] = np.asarray(io.imread(filename))\n",
    "    \n",
    "    return arr_maps\n",
    "\n",
    "\n",
    "def generate_tx_layer(arr_map: np.ndarray, tx_size: int = 1, upsampling_factor: int = 4) -> dict[int, np.ndarray]:\n",
    "    \"\"\"Generate TX layers (same shape as the map) corresponding to all valid TX locations on a map.\n",
    "\n",
    "    \"\"\"\n",
    "    tx_layers = {}\n",
    "    map_size = arr_map.shape[0]\n",
    "    n_steps = map_size // upsampling_factor\n",
    "    for row in range(n_steps):\n",
    "        for col in range(n_steps):\n",
    "            # only generate upsampled TX location corresponding to the action in auto BS\n",
    "            y, x = row * upsampling_factor + (upsampling_factor - 1) // 2, col * upsampling_factor + (upsampling_factor - 1) // 2\n",
    "            if arr_map[y, x] == 1.:  # white pixel - building\n",
    "                arr_tx = np.zeros_like(arr_map, dtype=np.uint8)  # black background\n",
    "                y_top, y_bottom, x_left, x_right = max(0, y - (tx_size-1)//2), min(map_size, y + tx_size//2+1), max(0, x - (tx_size-1)//2), min(map_size, x + tx_size//2+1) \n",
    "                arr_tx[y_top : y_bottom, x_left : x_right] = 1  # white tx location\n",
    "                \n",
    "                idx = map_size * y + x  # 1d index of TX location\n",
    "                tx_layers[idx] = arr_tx\n",
    "                \n",
    "    return tx_layers\n",
    "\n",
    "\n",
    "def create_dataset(input_dir_base: str = \"usc\", output_dir_base: str = \"usc\", suffix: str = \"train\", indices: np.ndarray = np.arange(100, dtype=int), \n",
    "                   tx_size: int = 1, upsampling_factor: int = 4, device: str = \"cpu\") -> tuple[list[str],torch.Tensor]:\n",
    "    \"\"\"Create dataset for PMNet (cropped maps + TX locations).\n",
    "\n",
    "    \"\"\"\n",
    "    arr_maps = load_maps(input_dir_base, indices)\n",
    "    # print(f\"loaded {len(arr_maps)} array maps with indices {indices.tolist()}\")\n",
    "    idx_map_tx, tensors = [], []  # index (map index + tx index), tensor ([map, tx], ch=2)\n",
    "    for idx_map, arr_map in arr_maps.items():\n",
    "        tx_layers = generate_tx_layer(arr_map, tx_size, upsampling_factor)\n",
    "        for idx_tx, tx_layer in tx_layers.items():\n",
    "            idx_data = str(idx_map) + '_' + str(idx_tx)\n",
    "            idx_map_tx.append(idx_data)\n",
    "            # # save tx location as a separate image\n",
    "            # tx_layer_grayscale = tx_layer * 255\n",
    "            # img_tx = Image.fromarray(tx_layer_grayscale, mode='L')\n",
    "            # img_tx.save(os.path.join(output_dir_base, \"tx_\" + suffix, \"tx_\" + idx_data + \".png\"))  \n",
    "            # concatenate map and tx along channel-wisely\n",
    "            arr_input = np.stack([arr_map, tx_layer], axis=0, dtype=np.float32)\n",
    "            tensor_input = torch.from_numpy(arr_input).to(device)\n",
    "            tensors.append(tensor_input)\n",
    "\n",
    "    tensors = torch.stack(tensors, dim=0)\n",
    "    # print(f\"tensors shape: {tensors.shape}\")\n",
    "\n",
    "    return idx_map_tx, tensors"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T21:20:56.396361Z",
     "start_time": "2024-03-07T21:20:56.395878Z"
    }
   },
   "id": "f1be467249a1a7fe"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 1 array maps with indices [0]\n",
      "3687\n"
     ]
    }
   ],
   "source": [
    "# maps = load_maps(indices=np.arange(1))\n",
    "# maps[0][:10, :10]\n",
    "i, t = create_dataset(indices=np.arange(1), tx_size=12, upsampling_factor=4)\n",
    "print(len(i))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T11:05:47.573597Z",
     "start_time": "2024-03-07T11:05:44.817401Z"
    }
   },
   "id": "cbe3897a0e25086"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def inference_and_save(model: nn.Module, idx: list[str], tensors: torch.Tensor, batch_size: int = 256, dir_base: str = \"usc\", dir_img: str = \"pmap\"):\n",
    "    \"\"\"Use PMNet to generate power maps from the given dataset.\n",
    "\n",
    "    \"\"\"\n",
    "    assert len(idx) == tensors.size(dim=0)\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    n_batches = len(idx) // batch_size + 1 if len(idx) % batch_size != 0 else len(idx) // batch_size\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(n_batches)):\n",
    "            start = i * batch_size\n",
    "            end = min((i + 1) * batch_size, len(idx))\n",
    "\n",
    "            batch_idx = idx[start: end]\n",
    "            batch_tensors = tensors[start: end]\n",
    "\n",
    "            preds = model(batch_tensors)\n",
    "            # print(f\"preds shape: {preds.shape}\")\n",
    "            # print(f\"preds[0,0,:3,:3]: {preds[0,0,:3,:3]}\")\n",
    "            preds = torch.clip(preds, 0, 1)\n",
    "\n",
    "            for j in range(len(preds)):\n",
    "              file_name = 'pmap_' + batch_idx[j] + '.png'\n",
    "              file_path = os.path.join(dir_base, dir_img, file_name)\n",
    "              arr = preds[j, 0].cpu().numpy()\n",
    "              plt.imsave(file_path, arr, cmap='gray')\n",
    "              # img_gray = Image.fromarray(arr).convert('L')\n",
    "              # img_gray.save(file_path)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T21:21:20.960393Z",
     "start_time": "2024-03-07T21:21:20.953354Z"
    }
   },
   "id": "9c8ca719cf81adef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load PMNet Model Parameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6763bf86df75e055"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "pretrained_model = './checkpoints/summary_case4.pt'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = PMNet(n_blocks=[3, 3, 27, 3],\n",
    "            atrous_rates=[6, 12, 18],\n",
    "            multi_grids=[1, 2, 4],\n",
    "            output_stride=8,)\n",
    "model.load_state_dict(torch.load(pretrained_model, map_location=device))\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T21:21:30.497333Z",
     "start_time": "2024-03-07T21:21:30.248556Z"
    }
   },
   "id": "2896670ecead8fff"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save Power Map"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f56ac19dd99d3d6c"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'usc/map/0.png'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m idx, tensors \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_dir_base\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43musc\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marange\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m inference_and_save(model\u001B[38;5;241m=\u001B[39mmodel, idx\u001B[38;5;241m=\u001B[39midx, tensors\u001B[38;5;241m=\u001B[39mtensors, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m, dir_base\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124musc/pmap/\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      4\u001B[0m idx_start, idx_end \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1441\u001B[39m, \u001B[38;5;241m1441\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m32\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m1\u001B[39m\n",
      "Cell \u001B[0;32mIn[11], line 43\u001B[0m, in \u001B[0;36mcreate_dataset\u001B[0;34m(input_dir_base, output_dir_base, suffix, indices, tx_size, upsampling_factor, device)\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate_dataset\u001B[39m(input_dir_base: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124musc\u001B[39m\u001B[38;5;124m\"\u001B[39m, output_dir_base: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124musc\u001B[39m\u001B[38;5;124m\"\u001B[39m, suffix: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m, indices: np\u001B[38;5;241m.\u001B[39mndarray \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marange(\u001B[38;5;241m100\u001B[39m, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mint\u001B[39m), \n\u001B[1;32m     39\u001B[0m                    tx_size: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m, upsampling_factor: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m4\u001B[39m, device: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[\u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m],torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[1;32m     40\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Create dataset for PMNet (cropped maps + TX locations).\u001B[39;00m\n\u001B[1;32m     41\u001B[0m \n\u001B[1;32m     42\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 43\u001B[0m     arr_maps \u001B[38;5;241m=\u001B[39m \u001B[43mload_maps\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_dir_base\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindices\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     44\u001B[0m     \u001B[38;5;66;03m# print(f\"loaded {len(arr_maps)} array maps with indices {indices.tolist()}\")\u001B[39;00m\n\u001B[1;32m     45\u001B[0m     idx_map_tx, tensors \u001B[38;5;241m=\u001B[39m [], []  \u001B[38;5;66;03m# index (map index + tx index), tensor ([map, tx], ch=2)\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[11], line 9\u001B[0m, in \u001B[0;36mload_maps\u001B[0;34m(dir_base, indices)\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices:\n\u001B[1;32m      8\u001B[0m     filename \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(dir_maps, \u001B[38;5;28mstr\u001B[39m(idx)) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.png\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 9\u001B[0m     arr_map \u001B[38;5;241m=\u001B[39m \u001B[43mImage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mconvert(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mL\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     10\u001B[0m     arr_maps[idx] \u001B[38;5;241m=\u001B[39m (np\u001B[38;5;241m.\u001B[39marray(arr_map, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mfloat32) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m0\u001B[39m) \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m255\u001B[39m  \u001B[38;5;66;03m# 256 x 256 matrix with value in [0,1] (grayscale)\u001B[39;00m\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;66;03m# arr_maps[idx] = np.asarray(io.imread(filename))\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/wides/lib/python3.10/site-packages/PIL/Image.py:3218\u001B[0m, in \u001B[0;36mopen\u001B[0;34m(fp, mode, formats)\u001B[0m\n\u001B[1;32m   3215\u001B[0m     filename \u001B[38;5;241m=\u001B[39m fp\n\u001B[1;32m   3217\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m filename:\n\u001B[0;32m-> 3218\u001B[0m     fp \u001B[38;5;241m=\u001B[39m \u001B[43mbuiltins\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3219\u001B[0m     exclusive_fp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   3221\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'usc/map/0.png'"
     ]
    }
   ],
   "source": [
    "# idx, tensors = create_dataset(input_dir_base='usc', indices=np.arange(1, dtype=int), device=device)\n",
    "# inference_and_save(model=model, idx=idx, tensors=tensors, batch_size=8, dir_base='usc/pmap/')\n",
    "\n",
    "idx_start, idx_end = 1441, 1441 + 32*1\n",
    "# only do inference on one map at one time in case of OutOfMemoeryError\n",
    "for idx_eval in tqdm(range(idx_start, idx_end, 32)):\n",
    "    idx, tensors = create_dataset(input_dir_base='USC_original', output_dir_base='output', suffix=\"train\", indices=np.arange(idx_eval,idx_eval+1,dtype=int), \n",
    "                                  tx_size=12, upsampling_factor=4, device=device)\n",
    "    inference_and_save(model=model, idx=idx, tensors=tensors, batch_size=4, dir_base='output', dir_img='pmap_train')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T21:26:40.924128Z",
     "start_time": "2024-03-07T21:26:40.697239Z"
    }
   },
   "id": "797acbd987c19bc8"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[255, 255, 255, ..., 159, 159, 158],\n       [253, 255, 249, ..., 163, 162, 163],\n       [255, 255, 252, ..., 160, 163, 163],\n       ...,\n       [201, 201, 203, ..., 140, 140, 139],\n       [202, 203, 204, ..., 140, 141, 138],\n       [199, 203, 202, ..., 140, 140, 138]], dtype=uint8)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmap_path = os.path.join('output', 'pmap_train', 'pmap_1_289.png')\n",
    "pmap_img = Image.open(pmap_path).convert('L')\n",
    "pmap_array = np.array(pmap_img)\n",
    "pmap_array"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T07:30:47.541865Z",
     "start_time": "2024-03-08T07:30:47.443128Z"
    }
   },
   "id": "d9eca54b9b984f99"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f64dbcde3466c06c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
