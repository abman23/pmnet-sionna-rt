2024-02-21 23:31:28,464 INFO ================EVALUATION AT # 5================
2024-02-21 23:31:30,100 INFO ================EVALUATION AT # 10================
2024-02-21 23:31:30,705 INFO =============A WHOLE TRAINING PERIOD ENDED=============
2024-02-21 23:31:30,710 INFO agent_timesteps_total: 320
connector_metrics:
  ObsPreprocessorConnector_ms: 0.004839152097702026
  StateBufferConnector_ms: 0.003074854612350464
  ViewRequirementAgentConnector_ms: 0.07180199027061462
counters:
  num_agent_steps_sampled: 320
  num_agent_steps_trained: 320
  num_env_steps_sampled: 320
  num_env_steps_trained: 320
custom_metrics: {}
date: 2024-02-21_23-31-30
done: false
episode_len_mean: 10.0
episode_media: {}
episode_reward_max: -222.1077027627484
episode_reward_mean: -1408.778388988033
episode_reward_min: -1807.9883776089548
episodes_this_iter: 4
episodes_total: 32
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0030120213826497397
    StateBufferConnector_ms: 0.0021060307820638022
    ViewRequirementAgentConnector_ms: 0.04498163859049479
  custom_metrics: {}
  episode_len_mean: 10.0
  episode_media: {}
  episode_reward_max: -1697.4292530665577
  episode_reward_mean: -1779.259381887321
  episode_reward_min: -1890.1967482832465
  episodes_this_iter: 3
  hist_stats:
    episode_lengths:
    - 10
    - 10
    - 10
    episode_reward:
    - -1890.1967482832465
    - -1750.1521443121592
    - -1697.4292530665577
  num_agent_steps_sampled_this_iter: 30
  num_env_steps_sampled_this_iter: 30
  num_faulty_episodes: 0
  num_healthy_workers: 0
  num_in_flight_async_reqs: 0
  num_remote_worker_restarts: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.04479533336201652
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 10.90533616112881
    mean_inference_ms: 0.4411681753690126
    mean_raw_obs_processing_ms: 0.18217524544137423
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.0030120213826497397
      StateBufferConnector_ms: 0.0021060307820638022
      ViewRequirementAgentConnector_ms: 0.04498163859049479
    custom_metrics: {}
    episode_len_mean: 10.0
    episode_media: {}
    episode_reward_max: -1697.4292530665577
    episode_reward_mean: -1779.259381887321
    episode_reward_min: -1890.1967482832465
    episodes_this_iter: 3
    hist_stats:
      episode_lengths:
      - 10
      - 10
      - 10
      episode_reward:
      - -1890.1967482832465
      - -1750.1521443121592
      - -1697.4292530665577
    num_faulty_episodes: 0
    policy_reward_max: {}
    policy_reward_mean: {}
    policy_reward_min: {}
    sampler_perf:
      mean_action_processing_ms: 0.04479533336201652
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 10.90533616112881
      mean_inference_ms: 0.4411681753690126
      mean_raw_obs_processing_ms: 0.18217524544137423
  timesteps_this_iter: 30
hostname: OrangeBookPro14.lan
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 9.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00234375
        cur_lr: 5.0e-05
        entropy: 6.889841771125793
        entropy_coeff: 0.0
        grad_gnorm: 1.1094839870929718
        kl: 0.07080947475042194
        policy_loss: 0.20120200887322426
        total_loss: 10.20136775970459
        vf_explained_var: -1.1146068572998046e-06
        vf_loss: 10.0
      model: {}
      num_agent_steps_trained: 16.0
      num_grad_updates_lifetime: 190.5
  num_agent_steps_sampled: 320
  num_agent_steps_trained: 320
  num_env_steps_sampled: 320
  num_env_steps_trained: 320
iterations_since_restore: 10
node_ip: 127.0.0.1
num_agent_steps_sampled: 320
num_agent_steps_trained: 320
num_env_steps_sampled: 320
num_env_steps_sampled_this_iter: 32
num_env_steps_sampled_throughput_per_sec: 129.01729872307047
num_env_steps_trained: 320
num_env_steps_trained_this_iter: 32
num_env_steps_trained_throughput_per_sec: 129.01729872307047
num_faulty_episodes: 0
num_healthy_workers: 4
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 32
perf:
  cpu_util_percent: 40.5
  ram_util_percent: 83.1
pid: 94005
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.06770710670350825
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 9.473864006421369
  mean_inference_ms: 0.9146936966139017
  mean_raw_obs_processing_ms: 0.39647331537791153
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.004839152097702026
    StateBufferConnector_ms: 0.003074854612350464
    ViewRequirementAgentConnector_ms: 0.07180199027061462
  custom_metrics: {}
  episode_len_mean: 10.0
  episode_media: {}
  episode_reward_max: -222.1077027627484
  episode_reward_mean: -1408.778388988033
  episode_reward_min: -1807.9883776089548
  episodes_this_iter: 4
  hist_stats:
    episode_lengths: [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,
      10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]
    episode_reward: [-1173.5654670859942, -937.3756001338277, -1461.1527416704103,
      -1404.75663936473, -1388.2278345510852, -222.1077027627484, -1684.9820624195556,
      -1330.3538406167133, -823.0054944640259, -1231.5538513813742, -1423.108823397055,
      -1454.0865204668482, -1467.009345590327, -1445.9893810104468, -1709.3664875824695,
      -1340.1077027627482, -1752.5938759064047, -1143.2556436789503, -1608.2849839314597,
      -1739.1514803843838, -1541.473193175917, -823.0054944640259, -1644.4101966249682,
      -1807.9883776089548, -1608.2849839314597, -1277.0175425099137, -1765.9999999999995,
      -1681.0084884547477, -1551.0392060941738, -1484.3594362117867, -1702.113621693308,
      -1454.1724276862371]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.06770710670350825
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 9.473864006421369
    mean_inference_ms: 0.9146936966139017
    mean_raw_obs_processing_ms: 0.39647331537791153
time_since_restore: 3.314812421798706
time_this_iter_s: 0.5977709293365479
time_total_s: 3.314812421798706
timers:
  learn_throughput: 219.444
  learn_time_ms: 145.823
  load_throughput: 166152.176
  load_time_ms: 0.193
  sample_time_ms: 95.481
  synch_weights_time_ms: 15.444
  training_iteration_time_ms: 257.147
timestamp: 1708587090
timesteps_total: 320
training_iteration: 10
trial_id: default

2024-02-21 23:31:30,710 DEBUG {'env': {'coefficient_dict': {'p_b': 1.0, 'p_d': 1.0, 'r_c': 0.1}, 'cropped_map_size': 64, 'n_maps': 100, 'n_steps_per_map': 10, 'no_masking': True, 'original_map_path': './resource/usc.png', 'original_map_scale': 3.4375, 'preset_map_path': './resource/setup.json', 'ratio_coverage': 0.0125}, 'eval': {'evaluation_config': {'env_config': {'evaluation': True, 'n_maps': 3, 'preset_map_path': None}, 'explore': False}, 'evaluation_duration': 3, 'evaluation_interval': 5}, 'explore': {'exploration_config': {'epsilon_timesteps': 10, 'final_epsilon': 0.02, 'initial_epsilon': 1.0, 'type': 'EpsilonGreedy'}, 'explore': True}, 'report': {'min_sample_timesteps_per_iteration': 10}, 'resource': {'num_cpus_per_worker': 2, 'num_gpus': 0, 'num_gpus_per_worker': 0}, 'rollout': {'num_envs_per_worker': 1, 'num_rollout_workers': 4}, 'stop': {'training_iteration': 10}, 'train': {'lr': 5e-05, 'train_batch_size': 32, 'sgd_minibatch_size': 16, 'num_sgd_iter': 10}}
2024-02-22 13:14:01,388 INFO ================EVALUATION AT # 5================
2024-02-22 13:14:03,183 INFO ================EVALUATION AT # 10================
2024-02-22 13:14:03,810 INFO =============A WHOLE TRAINING PERIOD ENDED=============
2024-02-22 13:14:03,815 INFO agent_timesteps_total: 320
connector_metrics:
  ObsPreprocessorConnector_ms: 0.005742907524108887
  StateBufferConnector_ms: 0.003631412982940674
  ViewRequirementAgentConnector_ms: 0.08192658424377441
counters:
  num_agent_steps_sampled: 320
  num_agent_steps_trained: 320
  num_env_steps_sampled: 320
  num_env_steps_trained: 320
custom_metrics: {}
date: 2024-02-22_13-14-03
done: false
episode_len_mean: 10.0
episode_media: {}
episode_reward_max: -578.34255586866
episode_reward_mean: -1297.2069324454462
episode_reward_min: -1802.1841748206684
episodes_this_iter: 4
episodes_total: 32
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.003226598103841146
    StateBufferConnector_ms: 0.0023365020751953125
    ViewRequirementAgentConnector_ms: 0.04485448201497396
  custom_metrics: {}
  episode_len_mean: 10.0
  episode_media: {}
  episode_reward_max: -659.2753579347358
  episode_reward_mean: -1149.421873609328
  episode_reward_min: -1577.121933088197
  episodes_this_iter: 3
  hist_stats:
    episode_lengths:
    - 10
    - 10
    - 10
    episode_reward:
    - -1577.121933088197
    - -659.2753579347358
    - -1211.8683298050516
  num_agent_steps_sampled_this_iter: 30
  num_env_steps_sampled_this_iter: 30
  num_faulty_episodes: 0
  num_healthy_workers: 0
  num_in_flight_async_reqs: 0
  num_remote_worker_restarts: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.042180545994492814
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 11.554202095406955
    mean_inference_ms: 0.4532884378902247
    mean_raw_obs_processing_ms: 0.1925992183997983
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.003226598103841146
      StateBufferConnector_ms: 0.0023365020751953125
      ViewRequirementAgentConnector_ms: 0.04485448201497396
    custom_metrics: {}
    episode_len_mean: 10.0
    episode_media: {}
    episode_reward_max: -659.2753579347358
    episode_reward_mean: -1149.421873609328
    episode_reward_min: -1577.121933088197
    episodes_this_iter: 3
    hist_stats:
      episode_lengths:
      - 10
      - 10
      - 10
      episode_reward:
      - -1577.121933088197
      - -659.2753579347358
      - -1211.8683298050516
    num_faulty_episodes: 0
    policy_reward_max: {}
    policy_reward_mean: {}
    policy_reward_min: {}
    sampler_perf:
      mean_action_processing_ms: 0.042180545994492814
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 11.554202095406955
      mean_inference_ms: 0.4532884378902247
      mean_raw_obs_processing_ms: 0.1925992183997983
  timesteps_this_iter: 30
hostname: OrangeBookPro14.lan
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 9.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00234375
        cur_lr: 5.0e-05
        entropy: 6.569695520401001
        entropy_coeff: 0.0
        grad_gnorm: 1.3355473309755326
        kl: 0.0748743881471455
        policy_loss: 0.17392897605895996
        total_loss: 10.174104690551758
        vf_explained_var: -1.4603137969970703e-06
        vf_loss: 10.0
      model: {}
      num_agent_steps_trained: 16.0
      num_grad_updates_lifetime: 190.5
  num_agent_steps_sampled: 320
  num_agent_steps_trained: 320
  num_env_steps_sampled: 320
  num_env_steps_trained: 320
iterations_since_restore: 10
node_ip: 127.0.0.1
num_agent_steps_sampled: 320
num_agent_steps_trained: 320
num_env_steps_sampled: 320
num_env_steps_sampled_this_iter: 32
num_env_steps_sampled_throughput_per_sec: 128.5172182665681
num_env_steps_trained: 320
num_env_steps_trained_this_iter: 32
num_env_steps_trained_throughput_per_sec: 128.5172182665681
num_faulty_episodes: 0
num_healthy_workers: 4
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 32
perf:
  cpu_util_percent: 61.9
  ram_util_percent: 83.5
pid: 3134
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.06852327228130385
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 10.023141342442765
  mean_inference_ms: 1.036387535434295
  mean_raw_obs_processing_ms: 0.6026692983557314
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.005742907524108887
    StateBufferConnector_ms: 0.003631412982940674
    ViewRequirementAgentConnector_ms: 0.08192658424377441
  custom_metrics: {}
  episode_len_mean: 10.0
  episode_media: {}
  episode_reward_max: -578.34255586866
  episode_reward_mean: -1297.2069324454462
  episode_reward_min: -1802.1841748206684
  episodes_this_iter: 4
  hist_stats:
    episode_lengths: [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,
      10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]
    episode_reward: [-863.3035172230514, -985.655230608825, -1392.8831959023723, -982.6203318791953,
      -673.9301256742124, -1349.7800560721073, -1502.9461391117677, -1802.1841748206684,
      -1700.1967482832463, -1517.5863337187866, -1802.1841748206684, -699.4616584663205,
      -1613.2277660168384, -1695.5444029504401, -811.0041152089149, -578.34255586866,
      -1337.8304597359459, -1591.0308423547494, -1297.6225774829854, -1532.5382386916235,
      -1443.320112359526, -1312.4166406412633, -1689.5102382966913, -1427.394846967484,
      -654.4937810560444, -1380.2374841615667, -1502.4101966249689, -1480.2091952673163,
      -1522.5585369269927, -578.34255586866, -1333.4331697709329, -1456.422435421457]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.06852327228130385
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 10.023141342442765
    mean_inference_ms: 1.036387535434295
    mean_raw_obs_processing_ms: 0.6026692983557314
time_since_restore: 3.583927631378174
time_this_iter_s: 0.6196420192718506
time_total_s: 3.583927631378174
timers:
  learn_throughput: 199.138
  learn_time_ms: 160.693
  load_throughput: 207735.224
  load_time_ms: 0.154
  sample_time_ms: 101.503
  synch_weights_time_ms: 17.374
  training_iteration_time_ms: 279.949
timestamp: 1708636443
timesteps_total: 320
training_iteration: 10
trial_id: default

2024-02-22 13:14:03,815 DEBUG {'env': {'coefficient_dict': {'p_b': 1.0, 'p_d': 1.0, 'r_c': 0.1}, 'cropped_map_size': 64, 'n_maps': 100, 'n_steps_per_map': 10, 'no_masking': True, 'original_map_path': './resource/usc.png', 'original_map_scale': 3.4375, 'preset_map_path': './resource/setup.json', 'ratio_coverage': 0.0125}, 'eval': {'evaluation_config': {'env_config': {'evaluation': True, 'n_maps': 3, 'preset_map_path': None}, 'explore': False}, 'evaluation_duration': 3, 'evaluation_interval': 5}, 'explore': {'exploration_config': {'epsilon_timesteps': 10, 'final_epsilon': 0.02, 'initial_epsilon': 1.0, 'type': 'EpsilonGreedy'}, 'explore': True}, 'report': {'min_sample_timesteps_per_iteration': 10}, 'resource': {'num_cpus_per_worker': 2, 'num_gpus': 0, 'num_gpus_per_worker': 0}, 'rollout': {'num_envs_per_worker': 1, 'num_rollout_workers': 4}, 'stop': {'training_iteration': 10}, 'train': {'lr': 5e-05, 'train_batch_size': 32, 'sgd_minibatch_size': 16, 'num_sgd_iter': 10}}
2024-02-22 13:14:27,405 INFO average coverage reward for test 0: 535.0
2024-02-22 13:14:27,634 INFO average coverage reward for test 1: 565.5
2024-02-22 13:14:27,871 INFO average coverage reward for test 2: 437.0
2024-02-22 13:17:33,976 INFO ================EVALUATION AT # 5================
2024-02-22 13:20:21,005 INFO ================EVALUATION AT # 10================
2024-02-22 13:23:03,510 INFO ================EVALUATION AT # 15================
2024-02-22 13:25:44,816 INFO ================EVALUATION AT # 20================
2024-02-22 13:28:22,905 INFO ================EVALUATION AT # 25================
2024-02-22 13:31:06,580 INFO ================EVALUATION AT # 30================
2024-02-22 13:33:44,644 INFO ================EVALUATION AT # 35================
2024-02-22 13:36:22,736 INFO ================EVALUATION AT # 40================
2024-02-22 13:39:03,683 INFO ================EVALUATION AT # 45================
2024-02-22 13:41:42,398 INFO ================EVALUATION AT # 50================
2024-02-22 13:44:20,398 INFO ================EVALUATION AT # 55================
2024-02-22 13:46:58,704 INFO ================EVALUATION AT # 60================
2024-02-22 13:49:40,918 INFO ================EVALUATION AT # 65================
2024-02-22 13:52:20,318 INFO ================EVALUATION AT # 70================
2024-02-22 13:54:58,579 INFO ================EVALUATION AT # 75================
2024-02-22 13:57:37,521 INFO ================EVALUATION AT # 80================
2024-02-22 14:00:18,197 INFO ================EVALUATION AT # 85================
2024-02-22 14:02:58,153 INFO ================EVALUATION AT # 90================
2024-02-22 14:05:37,176 INFO ================EVALUATION AT # 95================
2024-02-22 14:08:16,201 INFO ================EVALUATION AT # 100================
2024-02-22 14:10:55,480 INFO ================EVALUATION AT # 105================
2024-02-22 14:13:36,823 INFO ================EVALUATION AT # 110================
2024-02-22 14:16:17,288 INFO ================EVALUATION AT # 115================
2024-02-22 14:19:01,090 INFO ================EVALUATION AT # 120================
2024-02-22 14:21:42,545 INFO ================EVALUATION AT # 125================
2024-02-22 14:24:23,547 INFO ================EVALUATION AT # 130================
2024-02-22 14:27:03,175 INFO ================EVALUATION AT # 135================
2024-02-22 14:29:43,481 INFO ================EVALUATION AT # 140================
2024-02-22 14:32:23,086 INFO ================EVALUATION AT # 145================
2024-02-22 14:35:03,358 INFO ================EVALUATION AT # 150================
2024-02-22 14:37:44,239 INFO ================EVALUATION AT # 155================
2024-02-22 14:40:24,551 INFO ================EVALUATION AT # 160================
2024-02-22 14:43:04,764 INFO ================EVALUATION AT # 165================
2024-02-22 14:45:46,794 INFO ================EVALUATION AT # 170================
2024-02-22 14:48:38,135 INFO ================EVALUATION AT # 175================
2024-02-22 14:51:23,836 INFO ================EVALUATION AT # 180================
2024-02-22 14:54:07,522 INFO ================EVALUATION AT # 185================
2024-02-22 14:56:53,693 INFO ================EVALUATION AT # 190================
2024-02-22 14:59:38,832 INFO ================EVALUATION AT # 195================
2024-02-22 15:02:24,218 INFO ================EVALUATION AT # 200================
2024-02-22 15:05:09,824 INFO ================EVALUATION AT # 205================
2024-02-22 15:07:54,344 INFO ================EVALUATION AT # 210================
2024-02-22 15:10:41,179 INFO ================EVALUATION AT # 215================
2024-02-22 15:13:31,633 INFO ================EVALUATION AT # 220================
2024-02-22 15:16:16,863 INFO ================EVALUATION AT # 225================
2024-02-22 15:19:06,037 INFO ================EVALUATION AT # 230================
2024-02-22 15:21:52,511 INFO ================EVALUATION AT # 235================
2024-02-22 15:24:37,776 INFO ================EVALUATION AT # 240================
2024-02-22 15:27:23,010 INFO ================EVALUATION AT # 245================
2024-02-22 15:30:08,253 INFO ================EVALUATION AT # 250================
2024-02-22 15:32:54,033 INFO ================EVALUATION AT # 255================
2024-02-22 15:35:40,839 INFO ================EVALUATION AT # 260================
2024-02-22 15:38:25,151 INFO ================EVALUATION AT # 265================
2024-02-22 15:41:10,255 INFO ================EVALUATION AT # 270================
2024-02-22 15:43:54,771 INFO ================EVALUATION AT # 275================
2024-02-22 15:46:40,635 INFO ================EVALUATION AT # 280================
2024-02-22 15:49:22,649 INFO ================EVALUATION AT # 285================
2024-02-22 15:51:57,472 INFO ================EVALUATION AT # 290================
2024-02-22 15:54:34,475 INFO ================EVALUATION AT # 295================
2024-02-22 15:57:14,135 INFO ================EVALUATION AT # 300================
2024-02-22 15:59:51,062 INFO ================EVALUATION AT # 305================
2024-02-22 16:02:26,038 INFO ================EVALUATION AT # 310================
2024-02-22 16:05:01,323 INFO ================EVALUATION AT # 315================
2024-02-22 16:07:36,536 INFO ================EVALUATION AT # 320================
2024-02-22 16:10:12,297 INFO ================EVALUATION AT # 325================
2024-02-22 16:12:47,449 INFO ================EVALUATION AT # 330================
2024-02-22 16:15:22,599 INFO ================EVALUATION AT # 335================
2024-02-22 16:18:01,803 INFO ================EVALUATION AT # 340================
2024-02-22 16:20:38,902 INFO ================EVALUATION AT # 345================
2024-02-22 16:23:14,893 INFO ================EVALUATION AT # 350================
2024-02-22 16:25:52,805 INFO ================EVALUATION AT # 355================
2024-02-22 16:28:30,240 INFO ================EVALUATION AT # 360================
2024-02-22 16:31:07,661 INFO ================EVALUATION AT # 365================
2024-02-22 16:33:48,287 INFO ================EVALUATION AT # 370================
2024-02-22 16:36:29,384 INFO ================EVALUATION AT # 375================
2024-02-22 16:39:08,379 INFO ================EVALUATION AT # 380================
2024-02-22 16:41:38,647 INFO ================EVALUATION AT # 385================
2024-02-22 16:44:14,691 INFO ================EVALUATION AT # 390================
2024-02-22 16:46:51,541 INFO ================EVALUATION AT # 395================
2024-02-22 16:49:43,960 INFO ================EVALUATION AT # 400================
2024-02-22 16:52:27,223 INFO ================EVALUATION AT # 405================
2024-02-22 16:55:01,570 INFO ================EVALUATION AT # 410================
2024-02-22 16:57:32,158 INFO ================EVALUATION AT # 415================
2024-02-22 17:00:01,224 INFO ================EVALUATION AT # 420================
2024-02-22 17:02:30,619 INFO ================EVALUATION AT # 425================
2024-02-22 17:05:12,005 INFO ================EVALUATION AT # 430================
2024-02-22 17:07:47,826 INFO ================EVALUATION AT # 435================
2024-02-22 17:10:42,481 INFO ================EVALUATION AT # 440================
2024-02-22 17:13:21,070 INFO ================EVALUATION AT # 445================
2024-02-22 17:15:54,511 INFO ================EVALUATION AT # 450================
2024-02-22 17:18:32,566 INFO ================EVALUATION AT # 455================
2024-02-22 17:21:13,198 INFO ================EVALUATION AT # 460================
2024-02-22 17:23:43,299 INFO ================EVALUATION AT # 465================
2024-02-22 17:26:22,091 INFO ================EVALUATION AT # 470================
2024-02-22 17:28:54,328 INFO ================EVALUATION AT # 475================
2024-02-22 17:31:24,747 INFO ================EVALUATION AT # 480================
2024-02-22 17:34:04,937 INFO ================EVALUATION AT # 485================
2024-02-22 17:36:52,863 INFO ================EVALUATION AT # 490================
2024-02-22 17:39:35,935 INFO ================EVALUATION AT # 495================
2024-02-22 17:42:20,132 INFO ================EVALUATION AT # 500================
2024-02-22 17:42:56,648 INFO =============A WHOLE TRAINING PERIOD ENDED=============
2024-02-22 17:42:56,654 INFO agent_timesteps_total: 512000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.005667686462402344
  StateBufferConnector_ms: 0.003023386001586914
  ViewRequirementAgentConnector_ms: 0.08183836936950684
counters:
  num_agent_steps_sampled: 512000
  num_agent_steps_trained: 512000
  num_env_steps_sampled: 512000
  num_env_steps_trained: 512000
custom_metrics: {}
date: 2024-02-22_17-42-56
done: false
episode_len_mean: 19.62
episode_media: {}
episode_reward_max: 0.0
episode_reward_mean: -1362.7590811889108
episode_reward_min: -3592.875745240818
episodes_this_iter: 53
episodes_total: 25811
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0037590662638346353
    StateBufferConnector_ms: 0.0023603439331054688
    ViewRequirementAgentConnector_ms: 0.05312760670979818
  custom_metrics: {}
  episode_len_mean: 20.0
  episode_media: {}
  episode_reward_max: -484.98221281347026
  episode_reward_mean: -1334.934848266287
  episode_reward_min: -2349.198563445667
  episodes_this_iter: 3
  hist_stats:
    episode_lengths:
    - 20
    - 20
    - 20
    episode_reward:
    - -484.98221281347026
    - -1170.623768539724
    - -2349.198563445667
  num_agent_steps_sampled_this_iter: 60
  num_env_steps_sampled_this_iter: 60
  num_faulty_episodes: 0
  num_healthy_workers: 0
  num_in_flight_async_reqs: 0
  num_remote_worker_restarts: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.03519707412605305
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 12.120225274191363
    mean_inference_ms: 0.4064952149190142
    mean_raw_obs_processing_ms: 0.14052798680078385
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.0037590662638346353
      StateBufferConnector_ms: 0.0023603439331054688
      ViewRequirementAgentConnector_ms: 0.05312760670979818
    custom_metrics: {}
    episode_len_mean: 20.0
    episode_media: {}
    episode_reward_max: -484.98221281347026
    episode_reward_mean: -1334.934848266287
    episode_reward_min: -2349.198563445667
    episodes_this_iter: 3
    hist_stats:
      episode_lengths:
      - 20
      - 20
      - 20
      episode_reward:
      - -484.98221281347026
      - -1170.623768539724
      - -2349.198563445667
    num_faulty_episodes: 0
    policy_reward_max: {}
    policy_reward_mean: {}
    policy_reward_min: {}
    sampler_perf:
      mean_action_processing_ms: 0.03519707412605305
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 12.120225274191363
      mean_inference_ms: 0.4064952149190142
      mean_raw_obs_processing_ms: 0.14052798680078385
  timesteps_this_iter: 60
hostname: OrangeBookPro14.lan
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 59.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.5012520769810797e-143
        cur_lr: 5.0000000000000016e-05
        entropy: 0.06984279545965061
        entropy_coeff: 0.0
        grad_gnorm: 0.31749645541589283
        kl: 0.0001705871492102033
        policy_loss: -0.0821199615796407
        total_loss: 7.460207082827886
        vf_explained_var: -0.08815455337365469
        vf_loss: 7.542327016592026
      model: {}
      num_agent_steps_trained: 8.0
      num_grad_updates_lifetime: 1919940.5
  num_agent_steps_sampled: 512000
  num_agent_steps_trained: 512000
  num_env_steps_sampled: 512000
  num_env_steps_trained: 512000
iterations_since_restore: 500
node_ip: 127.0.0.1
num_agent_steps_sampled: 512000
num_agent_steps_trained: 512000
num_env_steps_sampled: 512000
num_env_steps_sampled_this_iter: 1024
num_env_steps_sampled_throughput_per_sec: 28.687557022380698
num_env_steps_trained: 512000
num_env_steps_trained_this_iter: 1024
num_env_steps_trained_throughput_per_sec: 28.687557022380698
num_faulty_episodes: 0
num_healthy_workers: 4
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 1024
perf:
  cpu_util_percent: 47.119230769230775
  ram_util_percent: 82.88653846153846
pid: 3238
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.04645801359935527
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 9.327791941899468
  mean_inference_ms: 0.6883089065983792
  mean_raw_obs_processing_ms: 0.2950659357814296
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.005667686462402344
    StateBufferConnector_ms: 0.003023386001586914
    ViewRequirementAgentConnector_ms: 0.08183836936950684
  custom_metrics: {}
  episode_len_mean: 19.62
  episode_media: {}
  episode_reward_max: 0.0
  episode_reward_mean: -1362.7590811889108
  episode_reward_min: -3592.875745240818
  episodes_this_iter: 53
  hist_stats:
    episode_lengths: [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,
      20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,
      20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,
      20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,
      20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 1, 1, 20, 20, 20, 20, 20,
      20, 20, 20, 20, 20, 20, 20, 20]
    episode_reward: [-953.9311410035102, -2346.814495343266, -526.4342047700779, -628.9780757793814,
      -2365.365280503242, -193.6643375224736, -642.6342439892262, -2601.738750882996,
      -282.92422502470635, -3101.4342230381476, -2702.444102037119, -416.1264008865607,
      -913.4848328356901, -375.51023387301245, -1153.7208798105562, -2953.4415128223004,
      -2812.8964600958984, -3328.772439983705, -2838.4761515876235, -658.7896939349687,
      -257.1077027627484, -699.2608495961765, -774.3124237432851, -493.3945729601885,
      -595.7358552246078, -2614.3297567789523, -730.333240792145, -456.65491900843114,
      -3592.875745240818, -939.3999276002803, -868.8029308359326, -871.8442884530657,
      -2602.310562561767, -2856.126423342397, -2489.8467265781323, -1126.6249938994622,
      -2119.0223822757353, -2434.7133769523643, -3411.268487978451, -628.4971769034255,
      -392.71337695236406, -765.3945729601884, -735.0619512757764, -734.0000000000002,
      -245.70329614269, -167.89352521910584, -2739.543263536458, -243.24515496597093,
      -2799.312115349617, -2744.4657282054122, -2678.831891575846, -484.49030993194214,
      -383.42471832484017, -209.65525060596445, -2411.8488797889954, -505.21952191620244,
      -2865.4139616173243, -2080.8706946213038, -3411.268487978451, -480.3564212655271,
      -2671.264116732914, -546.3105625617662, -2369.481028320378, -268.04650534085243,
      -2812.8964600958984, -3204.010988928051, -2433.1098884280705, -878.7966415668213,
      -1304.384473250307, -1601.2945721019264, -784.9253902261876, -2292.760482563257,
      -399.25353003264144, -655.4500931320961, -680.1452920561187, -849.5417527999326,
      -799.3552483535377, -173.99999999999997, -602.587727318528, -716.7128425310541,
      -2393.2535300326426, -319.5249202999535, -2926.949386427841, -44.72135954999578,
      -618.4642778942908, 0.0, 0.0, -826.6486583132389, -344.8800749063506, -373.91214233792294,
      -2108.882965458469, -2702.444102037119, -161.44271909999154, -2921.601782270357,
      -244.32582403567255, -231.65525060596445, -422.7680962081059, -161.44271909999154,
      -663.0443535372822, -3326.8612089887483]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.04645801359935527
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 9.327791941899468
    mean_inference_ms: 0.6883089065983792
    mean_raw_obs_processing_ms: 0.2950659357814296
time_since_restore: 16039.29258108139
time_this_iter_s: 36.5042941570282
time_total_s: 16039.29258108139
timers:
  learn_throughput: 33.018
  learn_time_ms: 969.163
  load_throughput: 82814.665
  load_time_ms: 0.386
  sample_time_ms: 94.995
  synch_weights_time_ms: 7.126
  training_iteration_time_ms: 1071.894
timestamp: 1708652576
timesteps_total: 512000
training_iteration: 500
trial_id: default

2024-02-22 17:42:56,654 DEBUG {'env': {'coefficient_dict': {'p_b': 1.0, 'p_d': 1.0, 'r_c': 0.1}, 'cropped_map_size': 64, 'n_maps': 400, 'n_steps_per_map': 20, 'no_masking': True, 'original_map_path': './resource/usc.png', 'original_map_scale': 3.4375, 'preset_map_path': './resource/setup_400.json', 'ratio_coverage': 0.0125}, 'eval': {'evaluation_config': {'env_config': {'evaluation': True, 'n_maps': 3, 'preset_map_path': None}, 'explore': False}, 'evaluation_duration': 3, 'evaluation_interval': 5}, 'explore': {'exploration_config': {'epsilon_timesteps': 100000, 'final_epsilon': 0.02, 'initial_epsilon': 1.0, 'type': 'EpsilonGreedy'}, 'explore': True}, 'report': {'min_sample_timesteps_per_iteration': 1000}, 'resource': {'num_cpus_per_worker': 2, 'num_gpus': 0, 'num_gpus_per_worker': 0}, 'rollout': {'num_envs_per_worker': 1, 'num_rollout_workers': 4}, 'stop': {'training_iteration': 500}, 'train': {'lr': 5e-05, 'gamma': 0.1, 'grad_clip': 40.0, 'train_batch_size': 32, 'sgd_minibatch_size': 8, 'num_sgd_iter': 30}}
2024-02-22 17:43:13,117 INFO average coverage reward for test 0: 681.0
2024-02-22 17:43:13,589 INFO average coverage reward for test 1: 681.0
2024-02-22 17:43:14,056 INFO average coverage reward for test 2: 694.0
2024-02-22 19:56:25,877 INFO ================EVALUATION AT # 5================
2024-02-22 19:59:02,414 INFO ================EVALUATION AT # 10================
2024-02-22 20:01:44,007 INFO ================EVALUATION AT # 15================
2024-02-22 20:04:28,642 INFO ================EVALUATION AT # 20================
2024-02-22 20:07:13,281 INFO ================EVALUATION AT # 25================
2024-02-22 20:09:42,126 INFO ================EVALUATION AT # 30================
2024-02-22 20:12:11,008 INFO ================EVALUATION AT # 35================
2024-02-22 20:14:39,953 INFO ================EVALUATION AT # 40================
2024-02-22 20:17:08,092 INFO ================EVALUATION AT # 45================
2024-02-22 20:19:36,529 INFO ================EVALUATION AT # 50================
2024-02-22 20:22:04,762 INFO ================EVALUATION AT # 55================
2024-02-22 20:24:33,742 INFO ================EVALUATION AT # 60================
2024-02-22 20:27:02,410 INFO ================EVALUATION AT # 65================
2024-02-22 20:29:29,182 INFO ================EVALUATION AT # 70================
2024-02-22 20:31:55,816 INFO ================EVALUATION AT # 75================
2024-02-22 20:34:23,247 INFO ================EVALUATION AT # 80================
2024-02-22 20:36:49,122 INFO ================EVALUATION AT # 85================
2024-02-22 20:39:15,464 INFO ================EVALUATION AT # 90================
2024-02-22 20:41:42,002 INFO ================EVALUATION AT # 95================
2024-02-22 20:44:08,865 INFO ================EVALUATION AT # 100================
2024-02-22 20:46:35,654 INFO ================EVALUATION AT # 105================
2024-02-22 20:49:06,332 INFO ================EVALUATION AT # 110================
2024-02-22 20:51:37,305 INFO ================EVALUATION AT # 115================
2024-02-22 20:54:08,618 INFO ================EVALUATION AT # 120================
2024-02-22 20:56:46,561 INFO ================EVALUATION AT # 125================
2024-02-22 20:59:33,316 INFO ================EVALUATION AT # 130================
2024-02-22 21:02:17,315 INFO ================EVALUATION AT # 135================
2024-02-22 21:04:56,507 INFO ================EVALUATION AT # 140================
2024-02-22 21:07:34,320 INFO ================EVALUATION AT # 145================
2024-02-22 21:10:12,276 INFO ================EVALUATION AT # 150================
2024-02-22 21:12:50,520 INFO ================EVALUATION AT # 155================
2024-02-22 21:15:28,447 INFO ================EVALUATION AT # 160================
2024-02-22 21:18:05,495 INFO ================EVALUATION AT # 165================
2024-02-22 21:20:42,711 INFO ================EVALUATION AT # 170================
2024-02-22 21:23:20,372 INFO ================EVALUATION AT # 175================
2024-02-22 21:25:58,065 INFO ================EVALUATION AT # 180================
2024-02-22 21:28:31,368 INFO ================EVALUATION AT # 185================
2024-02-22 21:30:58,577 INFO ================EVALUATION AT # 190================
2024-02-22 21:33:27,445 INFO ================EVALUATION AT # 195================
2024-02-22 21:35:56,146 INFO ================EVALUATION AT # 200================
2024-02-22 21:38:38,176 INFO ================EVALUATION AT # 205================
2024-02-22 21:41:18,002 INFO ================EVALUATION AT # 210================
2024-02-22 21:44:01,463 INFO ================EVALUATION AT # 215================
2024-02-22 21:46:42,156 INFO ================EVALUATION AT # 220================
2024-02-22 21:49:27,329 INFO ================EVALUATION AT # 225================
2024-02-22 21:52:10,240 INFO ================EVALUATION AT # 230================
2024-02-22 21:54:48,392 INFO ================EVALUATION AT # 235================
2024-02-22 21:57:29,577 INFO ================EVALUATION AT # 240================
2024-02-22 22:00:16,229 INFO ================EVALUATION AT # 245================
2024-02-22 22:03:10,098 INFO ================EVALUATION AT # 250================
2024-02-22 22:06:03,033 INFO ================EVALUATION AT # 255================
2024-02-22 22:09:02,354 INFO ================EVALUATION AT # 260================
2024-02-22 22:11:53,819 INFO ================EVALUATION AT # 265================
2024-02-22 22:14:39,734 INFO ================EVALUATION AT # 270================
2024-02-22 22:17:33,605 INFO ================EVALUATION AT # 275================
2024-02-22 22:20:14,309 INFO ================EVALUATION AT # 280================
2024-02-22 22:22:51,649 INFO ================EVALUATION AT # 285================
2024-02-22 22:25:30,928 INFO ================EVALUATION AT # 290================
2024-02-22 22:28:13,365 INFO ================EVALUATION AT # 295================
2024-02-22 22:30:56,011 INFO ================EVALUATION AT # 300================
2024-02-22 22:33:40,668 INFO ================EVALUATION AT # 305================
2024-02-22 22:36:22,089 INFO ================EVALUATION AT # 310================
2024-02-22 22:39:06,333 INFO ================EVALUATION AT # 315================
2024-02-22 22:41:45,848 INFO ================EVALUATION AT # 320================
2024-02-22 22:44:29,441 INFO ================EVALUATION AT # 325================
2024-02-22 22:47:15,819 INFO ================EVALUATION AT # 330================
2024-02-22 22:49:58,659 INFO ================EVALUATION AT # 335================
2024-02-22 22:52:42,984 INFO ================EVALUATION AT # 340================
2024-02-22 22:55:22,955 INFO ================EVALUATION AT # 345================
2024-02-22 22:58:06,138 INFO ================EVALUATION AT # 350================
2024-02-22 23:00:47,868 INFO ================EVALUATION AT # 355================
2024-02-22 23:03:29,314 INFO ================EVALUATION AT # 360================
2024-02-22 23:06:14,793 INFO ================EVALUATION AT # 365================
2024-02-22 23:09:03,882 INFO ================EVALUATION AT # 370================
2024-02-22 23:11:49,635 INFO ================EVALUATION AT # 375================
2024-02-22 23:14:36,345 INFO ================EVALUATION AT # 380================
2024-02-22 23:17:09,623 INFO ================EVALUATION AT # 385================
2024-02-22 23:19:50,724 INFO ================EVALUATION AT # 390================
2024-02-22 23:22:30,233 INFO ================EVALUATION AT # 395================
2024-02-22 23:25:19,596 INFO ================EVALUATION AT # 400================
2024-02-22 23:27:58,014 INFO ================EVALUATION AT # 405================
2024-02-22 23:30:42,497 INFO ================EVALUATION AT # 410================
2024-02-22 23:33:28,477 INFO ================EVALUATION AT # 415================
2024-02-22 23:36:06,201 INFO ================EVALUATION AT # 420================
2024-02-22 23:38:48,555 INFO ================EVALUATION AT # 425================
2024-02-22 23:41:21,276 INFO ================EVALUATION AT # 430================
2024-02-22 23:43:53,727 INFO ================EVALUATION AT # 435================
2024-02-22 23:46:26,914 INFO ================EVALUATION AT # 440================
2024-02-22 23:48:59,628 INFO ================EVALUATION AT # 445================
2024-02-22 23:51:33,891 INFO ================EVALUATION AT # 450================
2024-02-22 23:54:10,771 INFO ================EVALUATION AT # 455================
2024-02-22 23:56:43,755 INFO ================EVALUATION AT # 460================
2024-02-22 23:59:16,010 INFO ================EVALUATION AT # 465================
2024-02-23 00:01:48,175 INFO ================EVALUATION AT # 470================
2024-02-23 00:04:21,063 INFO ================EVALUATION AT # 475================
2024-02-23 00:06:57,709 INFO ================EVALUATION AT # 480================
2024-02-23 00:09:26,126 INFO ================EVALUATION AT # 485================
2024-02-23 00:11:57,260 INFO ================EVALUATION AT # 490================
2024-02-23 00:14:28,562 INFO ================EVALUATION AT # 495================
2024-02-23 00:16:59,759 INFO ================EVALUATION AT # 500================
2024-02-23 00:19:30,124 INFO ================EVALUATION AT # 505================
2024-02-23 00:22:02,384 INFO ================EVALUATION AT # 510================
2024-02-23 00:24:34,817 INFO ================EVALUATION AT # 515================
2024-02-23 00:27:06,796 INFO ================EVALUATION AT # 520================
2024-02-23 00:29:47,160 INFO ================EVALUATION AT # 525================
2024-02-23 00:32:25,731 INFO ================EVALUATION AT # 530================
2024-02-23 00:35:14,534 INFO ================EVALUATION AT # 535================
2024-02-23 00:38:03,007 INFO ================EVALUATION AT # 540================
2024-02-23 00:40:41,615 INFO ================EVALUATION AT # 545================
2024-02-23 00:43:13,172 INFO ================EVALUATION AT # 550================
2024-02-23 00:45:47,424 INFO ================EVALUATION AT # 555================
2024-02-23 00:48:21,394 INFO ================EVALUATION AT # 560================
2024-02-23 00:50:57,484 INFO ================EVALUATION AT # 565================
2024-02-23 00:53:46,111 INFO ================EVALUATION AT # 570================
2024-02-23 00:56:34,328 INFO ================EVALUATION AT # 575================
2024-02-23 00:59:14,910 INFO ================EVALUATION AT # 580================
2024-02-23 01:01:59,606 INFO ================EVALUATION AT # 585================
2024-02-23 01:04:48,872 INFO ================EVALUATION AT # 590================
2024-02-23 01:07:35,019 INFO ================EVALUATION AT # 595================
2024-02-23 01:10:15,999 INFO ================EVALUATION AT # 600================
2024-02-23 01:12:57,460 INFO ================EVALUATION AT # 605================
2024-02-23 01:15:40,474 INFO ================EVALUATION AT # 610================
2024-02-23 01:18:21,190 INFO ================EVALUATION AT # 615================
2024-02-23 01:21:07,963 INFO ================EVALUATION AT # 620================
2024-02-23 01:23:55,475 INFO ================EVALUATION AT # 625================
2024-02-23 01:26:46,012 INFO ================EVALUATION AT # 630================
2024-02-23 01:29:41,553 INFO ================EVALUATION AT # 635================
2024-02-23 01:32:06,467 INFO ================EVALUATION AT # 640================
2024-02-23 01:34:32,381 INFO ================EVALUATION AT # 645================
2024-02-23 01:37:00,082 INFO ================EVALUATION AT # 650================
2024-02-23 01:39:29,195 INFO ================EVALUATION AT # 655================
2024-02-23 01:41:54,447 INFO ================EVALUATION AT # 660================
2024-02-23 01:44:19,939 INFO ================EVALUATION AT # 665================
2024-02-23 01:46:44,986 INFO ================EVALUATION AT # 670================
2024-02-23 01:49:22,359 INFO ================EVALUATION AT # 675================
2024-02-23 01:52:04,482 INFO ================EVALUATION AT # 680================
2024-02-23 01:54:43,513 INFO ================EVALUATION AT # 685================
2024-02-23 01:57:11,729 INFO ================EVALUATION AT # 690================
2024-02-23 01:59:40,728 INFO ================EVALUATION AT # 695================
2024-02-23 02:02:09,247 INFO ================EVALUATION AT # 700================
2024-02-23 02:04:36,616 INFO ================EVALUATION AT # 705================
2024-02-23 02:07:03,913 INFO ================EVALUATION AT # 710================
2024-02-23 02:09:31,157 INFO ================EVALUATION AT # 715================
2024-02-23 02:11:57,569 INFO ================EVALUATION AT # 720================
2024-02-23 02:14:25,058 INFO ================EVALUATION AT # 725================
2024-02-23 02:16:51,206 INFO ================EVALUATION AT # 730================
2024-02-23 02:19:18,095 INFO ================EVALUATION AT # 735================
2024-02-23 02:21:46,293 INFO ================EVALUATION AT # 740================
2024-02-23 02:24:15,910 INFO ================EVALUATION AT # 745================
2024-02-23 02:26:43,419 INFO ================EVALUATION AT # 750================
2024-02-23 02:29:10,019 INFO ================EVALUATION AT # 755================
2024-02-23 02:31:36,997 INFO ================EVALUATION AT # 760================
2024-02-23 02:34:05,031 INFO ================EVALUATION AT # 765================
2024-02-23 02:36:33,595 INFO ================EVALUATION AT # 770================
2024-02-23 02:39:01,751 INFO ================EVALUATION AT # 775================
2024-02-23 02:41:49,308 INFO ================EVALUATION AT # 780================
2024-02-23 02:44:31,944 INFO ================EVALUATION AT # 785================
2024-02-23 02:47:18,265 INFO ================EVALUATION AT # 790================
2024-02-23 02:50:08,497 INFO ================EVALUATION AT # 795================
2024-02-23 02:52:48,464 INFO ================EVALUATION AT # 800================
2024-02-23 02:53:18,760 INFO =============A WHOLE TRAINING PERIOD ENDED=============
2024-02-23 02:53:18,767 INFO agent_timesteps_total: 819200
connector_metrics:
  ObsPreprocessorConnector_ms: 0.007817506790161133
  StateBufferConnector_ms: 0.002782583236694336
  ViewRequirementAgentConnector_ms: 0.07033848762512207
counters:
  num_agent_steps_sampled: 819200
  num_agent_steps_trained: 819200
  num_env_steps_sampled: 819200
  num_env_steps_trained: 819200
custom_metrics: {}
date: 2024-02-23_02-53-18
done: false
episode_len_mean: 20.0
episode_media: {}
episode_reward_max: -272.3549864614954
episode_reward_mean: -2517.1822295625457
episode_reward_min: -3756.6211251235313
episodes_this_iter: 50
episodes_total: 40964
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.001430511474609375
    StateBufferConnector_ms: 0.0012874603271484375
    ViewRequirementAgentConnector_ms: 0.029166539510091145
  custom_metrics: {}
  episode_len_mean: 20.0
  episode_media: {}
  episode_reward_max: -1120.1576201698779
  episode_reward_mean: -2330.258784539586
  episode_reward_min: -3000.198926981712
  episodes_this_iter: 3
  hist_stats:
    episode_lengths:
    - 20
    - 20
    - 20
    episode_reward:
    - -2870.4198064671677
    - -1120.1576201698779
    - -3000.198926981712
  num_agent_steps_sampled_this_iter: 60
  num_env_steps_sampled_this_iter: 60
  num_faulty_episodes: 0
  num_healthy_workers: 0
  num_in_flight_async_reqs: 0
  num_remote_worker_restarts: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.036075107505726026
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 10.935081540141402
    mean_inference_ms: 0.4337979485474332
    mean_raw_obs_processing_ms: 0.13414212483041624
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.001430511474609375
      StateBufferConnector_ms: 0.0012874603271484375
      ViewRequirementAgentConnector_ms: 0.029166539510091145
    custom_metrics: {}
    episode_len_mean: 20.0
    episode_media: {}
    episode_reward_max: -1120.1576201698779
    episode_reward_mean: -2330.258784539586
    episode_reward_min: -3000.198926981712
    episodes_this_iter: 3
    hist_stats:
      episode_lengths:
      - 20
      - 20
      - 20
      episode_reward:
      - -2870.4198064671677
      - -1120.1576201698779
      - -3000.198926981712
    num_faulty_episodes: 0
    policy_reward_max: {}
    policy_reward_mean: {}
    policy_reward_min: {}
    sampler_perf:
      mean_action_processing_ms: 0.036075107505726026
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 10.935081540141402
      mean_inference_ms: 0.4337979485474332
      mean_raw_obs_processing_ms: 0.13414212483041624
  timesteps_this_iter: 60
hostname: OrangeBookPro14.lan
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 59.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.004370056651609245
        cur_lr: 1.0e-05
        entropy: 0.46846919356224437
        entropy_coeff: 0.0
        grad_gnorm: 0.7034661247283415
        kl: 0.0012972943314225086
        policy_loss: -0.007589339651167393
        total_loss: 9.992416230837504
        vf_explained_var: -7.504721482594808e-06
        vf_loss: 10.0
      model: {}
      num_agent_steps_trained: 8.0
      num_grad_updates_lifetime: 3071940.5
  num_agent_steps_sampled: 819200
  num_agent_steps_trained: 819200
  num_env_steps_sampled: 819200
  num_env_steps_trained: 819200
iterations_since_restore: 800
node_ip: 127.0.0.1
num_agent_steps_sampled: 819200
num_agent_steps_trained: 819200
num_env_steps_sampled: 819200
num_env_steps_sampled_this_iter: 1024
num_env_steps_sampled_throughput_per_sec: 34.58930230985656
num_env_steps_trained: 819200
num_env_steps_trained_this_iter: 1024
num_env_steps_trained_throughput_per_sec: 34.58930230985656
num_faulty_episodes: 0
num_healthy_workers: 4
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 1024
perf:
  cpu_util_percent: 28.325581395348838
  ram_util_percent: 77.31860465116279
pid: 8702
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.045448909555362765
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 9.418087131211546
  mean_inference_ms: 0.6753273923241281
  mean_raw_obs_processing_ms: 0.2776718331867969
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.007817506790161133
    StateBufferConnector_ms: 0.002782583236694336
    ViewRequirementAgentConnector_ms: 0.07033848762512207
  custom_metrics: {}
  episode_len_mean: 20.0
  episode_media: {}
  episode_reward_max: -272.3549864614954
  episode_reward_mean: -2517.1822295625457
  episode_reward_min: -3756.6211251235313
  episodes_this_iter: 50
  hist_stats:
    episode_lengths: [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,
      20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,
      20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,
      20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,
      20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,
      20, 20, 20, 20, 20, 20, 20, 20, 20]
    episode_reward: [-2494.806130178212, -1381.6504112789605, -2770.0, -2925.895759938962,
      -2685.7358552246083, -1115.139419883527, -2937.853027239253, -2579.4182110716783,
      -2838.3595560468866, -1419.7669102286054, -3117.793664279147, -3156.2621983083927,
      -2980.1624198095215, -3232.0480422297483, -2838.3595560468866, -1221.0169768410656,
      -3274.256368125246, -2450.3604614807095, -2466.6243908376273, -2418.9158493299537,
      -3106.3871289238, -2559.90035028819, -3756.6211251235313, -2727.597448159377,
      -3468.5244492586958, -2596.863720927116, -1223.0679286326194, -2902.5507158694736,
      -2992.277724214764, -1489.0169768410665, -2720.6659275674583, -3025.205832509385,
      -1018.3297567789525, -1232.5507158694718, -2961.9129471453293, -2851.166314423326,
      -2615.491246732178, -1407.9954193435135, -3096.010988928053, -926.6342439892263,
      -2981.4477994640447, -3204.2468448052637, -2866.777178564959, -2518.3564212655265,
      -3025.205832509385, -1366.2777242147647, -1127.9999999999998, -2413.1077027627484,
      -3064.657355607051, -2622.475920832573, -2717.7358552246087, -2838.3595560468866,
      -2600.4104611466146, -3167.152823843989, -1283.224520712844, -2170.6549190084306,
      -2527.38633753706, -3153.999999999999, -2428.7083075883947, -827.0589071449374,
      -2971.0684968677665, -3024.0891564139497, -2723.4185966332607, -1165.975895536156,
      -2945.1528238439887, -2939.2051136131495, -2424.5042131359187, -1072.2748591897664,
      -2744.5675335663136, -1135.9937654087767, -3389.7980912173325, -2827.956784543041,
      -2483.08017898198, -3468.5244492586958, -3005.3664636426297, -2749.0166568919885,
      -2568.6342439892264, -2364.8854381999818, -3134.9379331573978, -2750.8768014512207,
      -2829.619368923694, -2867.2099788303067, -2419.9370590994668, -1386.122245341753,
      -3059.401153701775, -3014.5629906907197, -3073.0789656434144, -2938.5565981523423,
      -3327.1419257183243, -2773.7358552246073, -2933.756723055381, -2918.0487786742683,
      -2697.1726674375723, -2568.000000000001, -2988.802627300768, -2364.8854381999818,
      -3159.45203648207, -272.3549864614954, -2771.8937625582475, -1576.7216610312348]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.045448909555362765
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 9.418087131211546
    mean_inference_ms: 0.6753273923241281
    mean_raw_obs_processing_ms: 0.2776718331867969
time_since_restore: 25122.475940704346
time_this_iter_s: 30.286498069763184
time_total_s: 25122.475940704346
timers:
  learn_throughput: 38.789
  learn_time_ms: 824.967
  load_throughput: 133789.601
  load_time_ms: 0.239
  sample_time_ms: 92.32
  synch_weights_time_ms: 6.244
  training_iteration_time_ms: 923.967
timestamp: 1708685598
timesteps_total: 819200
training_iteration: 800
trial_id: default

2024-02-23 02:53:18,767 DEBUG {'env': {'coefficient_dict': {'p_b': 1.0, 'p_d': 1.0, 'r_c': 0.1}, 'cropped_map_size': 64, 'n_maps': 400, 'n_steps_per_map': 20, 'no_masking': True, 'original_map_path': './resource/usc.png', 'original_map_scale': 3.4375, 'preset_map_path': './resource/setup_400.json', 'ratio_coverage': 0.0125}, 'eval': {'evaluation_config': {'env_config': {'evaluation': True, 'n_maps': 3, 'preset_map_path': None}, 'explore': False}, 'evaluation_duration': 3, 'evaluation_interval': 5}, 'explore': {'exploration_config': {'epsilon_timesteps': 100000, 'final_epsilon': 0.02, 'initial_epsilon': 1.0, 'type': 'EpsilonGreedy'}, 'explore': True}, 'report': {'min_sample_timesteps_per_iteration': 1000}, 'resource': {'num_cpus_per_worker': 2, 'num_gpus': 0, 'num_gpus_per_worker': 0}, 'rollout': {'num_envs_per_worker': 1, 'num_rollout_workers': 4}, 'stop': {'training_iteration': 800}, 'train': {'lr': 1e-05, 'gamma': 0.9, 'grad_clip': 40.0, 'train_batch_size': 32, 'sgd_minibatch_size': 8, 'num_sgd_iter': 30}}
2024-02-23 02:53:36,379 INFO average coverage reward for test 0: 498.0, optimal reward: 706
2024-02-23 02:53:36,860 INFO average coverage reward for test 1: 638.8, optimal reward: 782
2024-02-23 02:53:37,303 INFO average coverage reward for test 2: 661.95, optimal reward: 748
2024-02-23 05:01:04,415 INFO ================EVALUATION AT # 5================
2024-02-23 05:03:57,552 INFO ================EVALUATION AT # 10================
2024-02-23 05:06:48,283 INFO ================EVALUATION AT # 15================
2024-02-23 05:09:33,732 INFO ================EVALUATION AT # 20================
2024-02-23 05:12:17,978 INFO ================EVALUATION AT # 25================
2024-02-23 05:15:06,174 INFO ================EVALUATION AT # 30================
2024-02-23 05:17:57,061 INFO ================EVALUATION AT # 35================
2024-02-23 05:20:41,945 INFO ================EVALUATION AT # 40================
2024-02-23 05:23:34,457 INFO ================EVALUATION AT # 45================
2024-02-23 05:26:12,920 INFO ================EVALUATION AT # 50================
2024-02-23 05:29:07,629 INFO ================EVALUATION AT # 55================
2024-02-23 05:32:02,657 INFO ================EVALUATION AT # 60================
2024-02-23 05:34:47,960 INFO ================EVALUATION AT # 65================
2024-02-23 05:37:32,901 INFO ================EVALUATION AT # 70================
2024-02-23 05:40:23,131 INFO ================EVALUATION AT # 75================
2024-02-23 05:43:01,248 INFO ================EVALUATION AT # 80================
2024-02-23 05:45:39,415 INFO ================EVALUATION AT # 85================
2024-02-23 05:48:18,414 INFO ================EVALUATION AT # 90================
2024-02-23 05:50:52,100 INFO ================EVALUATION AT # 95================
2024-02-23 05:53:32,418 INFO ================EVALUATION AT # 100================
2024-02-23 05:56:22,962 INFO ================EVALUATION AT # 105================
2024-02-23 05:59:06,163 INFO ================EVALUATION AT # 110================
2024-02-23 06:01:50,968 INFO ================EVALUATION AT # 115================
2024-02-23 06:04:32,900 INFO ================EVALUATION AT # 120================
2024-02-23 06:07:08,130 INFO ================EVALUATION AT # 125================
2024-02-23 06:09:46,508 INFO ================EVALUATION AT # 130================
2024-02-23 06:12:21,125 INFO ================EVALUATION AT # 135================
2024-02-23 06:14:56,830 INFO ================EVALUATION AT # 140================
2024-02-23 06:17:33,940 INFO ================EVALUATION AT # 145================
2024-02-23 06:20:11,736 INFO ================EVALUATION AT # 150================
2024-02-23 06:22:46,524 INFO ================EVALUATION AT # 155================
2024-02-23 06:25:23,617 INFO ================EVALUATION AT # 160================
2024-02-23 06:28:01,861 INFO ================EVALUATION AT # 165================
2024-02-23 06:30:38,209 INFO ================EVALUATION AT # 170================
2024-02-23 06:33:16,130 INFO ================EVALUATION AT # 175================
2024-02-23 06:35:55,667 INFO ================EVALUATION AT # 180================
2024-02-23 06:38:34,669 INFO ================EVALUATION AT # 185================
2024-02-23 06:41:13,562 INFO ================EVALUATION AT # 190================
2024-02-23 06:43:50,917 INFO ================EVALUATION AT # 195================
2024-02-23 06:46:29,081 INFO ================EVALUATION AT # 200================
2024-02-23 06:49:08,995 INFO ================EVALUATION AT # 205================
2024-02-23 06:51:47,917 INFO ================EVALUATION AT # 210================
2024-02-23 06:54:27,488 INFO ================EVALUATION AT # 215================
2024-02-23 06:57:06,681 INFO ================EVALUATION AT # 220================
2024-02-23 06:59:48,275 INFO ================EVALUATION AT # 225================
2024-02-23 07:02:33,807 INFO ================EVALUATION AT # 230================
2024-02-23 07:05:07,603 INFO ================EVALUATION AT # 235================
2024-02-23 07:07:40,733 INFO ================EVALUATION AT # 240================
2024-02-23 07:10:13,549 INFO ================EVALUATION AT # 245================
2024-02-23 07:12:46,551 INFO ================EVALUATION AT # 250================
2024-02-23 07:15:18,514 INFO ================EVALUATION AT # 255================
2024-02-23 07:17:50,844 INFO ================EVALUATION AT # 260================
2024-02-23 07:20:23,302 INFO ================EVALUATION AT # 265================
2024-02-23 07:22:55,114 INFO ================EVALUATION AT # 270================
2024-02-23 07:25:26,744 INFO ================EVALUATION AT # 275================
2024-02-23 07:27:58,665 INFO ================EVALUATION AT # 280================
2024-02-23 07:30:31,466 INFO ================EVALUATION AT # 285================
2024-02-23 07:33:12,247 INFO ================EVALUATION AT # 290================
2024-02-23 07:35:59,677 INFO ================EVALUATION AT # 295================
2024-02-23 07:38:48,043 INFO ================EVALUATION AT # 300================
2024-02-23 07:41:30,700 INFO ================EVALUATION AT # 305================
2024-02-23 07:44:18,264 INFO ================EVALUATION AT # 310================
2024-02-23 08:05:07,823 INFO ================EVALUATION AT # 315================
2024-02-23 08:09:34,406 INFO ================EVALUATION AT # 320================
2024-02-23 08:28:28,670 INFO ================EVALUATION AT # 325================
2024-02-23 09:03:20,461 INFO ================EVALUATION AT # 330================
2024-02-23 09:06:07,477 INFO ================EVALUATION AT # 335================
2024-02-23 09:10:05,284 INFO ================EVALUATION AT # 340================
2024-02-23 09:24:26,697 INFO ================EVALUATION AT # 345================
2024-02-23 09:42:57,456 INFO ================EVALUATION AT # 350================
2024-02-23 09:47:30,042 INFO ================EVALUATION AT # 355================
2024-02-23 09:50:24,088 INFO ================EVALUATION AT # 360================
2024-02-23 09:53:21,971 INFO ================EVALUATION AT # 365================
2024-02-23 09:56:10,411 INFO ================EVALUATION AT # 370================
2024-02-23 09:59:07,741 INFO ================EVALUATION AT # 375================
2024-02-23 10:01:58,696 INFO ================EVALUATION AT # 380================
2024-02-23 10:04:47,049 INFO ================EVALUATION AT # 385================
2024-02-23 10:07:28,434 INFO ================EVALUATION AT # 390================
2024-02-23 10:10:11,825 INFO ================EVALUATION AT # 395================
2024-02-23 10:12:57,543 INFO ================EVALUATION AT # 400================
2024-02-23 10:15:42,524 INFO ================EVALUATION AT # 405================
2024-02-23 10:18:24,412 INFO ================EVALUATION AT # 410================
2024-02-23 10:21:03,465 INFO ================EVALUATION AT # 415================
2024-02-23 10:23:42,686 INFO ================EVALUATION AT # 420================
2024-02-23 10:26:22,455 INFO ================EVALUATION AT # 425================
2024-02-23 10:29:02,420 INFO ================EVALUATION AT # 430================
2024-02-23 10:31:44,225 INFO ================EVALUATION AT # 435================
2024-02-23 10:34:23,666 INFO ================EVALUATION AT # 440================
2024-02-23 10:37:04,231 INFO ================EVALUATION AT # 445================
2024-02-23 10:45:38,051 INFO ================EVALUATION AT # 450================
2024-02-23 11:04:17,851 INFO ================EVALUATION AT # 455================
2024-02-23 11:06:57,533 INFO ================EVALUATION AT # 460================
2024-02-23 11:09:43,273 INFO ================EVALUATION AT # 465================
2024-02-23 11:12:28,223 INFO ================EVALUATION AT # 470================
2024-02-23 11:15:06,732 INFO ================EVALUATION AT # 475================
2024-02-23 11:17:37,416 INFO ================EVALUATION AT # 480================
2024-02-23 11:20:19,023 INFO ================EVALUATION AT # 485================
2024-02-23 11:23:11,328 INFO ================EVALUATION AT # 490================
2024-02-23 11:25:52,403 INFO ================EVALUATION AT # 495================
2024-02-23 11:28:33,639 INFO ================EVALUATION AT # 500================
2024-02-23 11:31:10,145 INFO ================EVALUATION AT # 505================
2024-02-23 11:33:56,256 INFO ================EVALUATION AT # 510================
2024-02-23 11:36:39,440 INFO ================EVALUATION AT # 515================
2024-02-23 11:39:18,456 INFO ================EVALUATION AT # 520================
2024-02-23 11:41:57,180 INFO ================EVALUATION AT # 525================
2024-02-23 11:44:32,518 INFO ================EVALUATION AT # 530================
2024-02-23 11:47:12,949 INFO ================EVALUATION AT # 535================
2024-02-23 11:50:03,841 INFO ================EVALUATION AT # 540================
2024-02-23 11:52:38,709 INFO ================EVALUATION AT # 545================
2024-02-23 11:55:03,324 INFO ================EVALUATION AT # 550================
2024-02-23 11:57:29,631 INFO ================EVALUATION AT # 555================
2024-02-23 12:06:08,745 INFO ================EVALUATION AT # 560================
2024-02-23 12:08:36,015 INFO ================EVALUATION AT # 565================
2024-02-23 12:17:22,961 INFO ================EVALUATION AT # 570================
2024-02-25 02:17:28,598 INFO ================EVALUATION AT # 5================
2024-02-25 02:18:07,918 INFO ================EVALUATION AT # 10================
2024-02-25 02:18:51,693 INFO ================EVALUATION AT # 15================
2024-02-25 02:19:34,519 INFO ================EVALUATION AT # 20================
2024-02-25 02:20:19,254 INFO ================EVALUATION AT # 25================
2024-02-25 02:21:00,342 INFO ================EVALUATION AT # 30================
2024-02-25 02:21:42,571 INFO ================EVALUATION AT # 35================
2024-02-25 02:22:24,192 INFO ================EVALUATION AT # 40================
2024-02-25 02:23:06,333 INFO ================EVALUATION AT # 45================
2024-02-25 02:23:48,366 INFO ================EVALUATION AT # 50================
2024-02-25 02:24:30,274 INFO ================EVALUATION AT # 55================
2024-02-25 02:25:13,694 INFO ================EVALUATION AT # 60================
2024-02-25 02:25:55,616 INFO ================EVALUATION AT # 65================
2024-02-25 02:26:37,179 INFO ================EVALUATION AT # 70================
2024-02-25 02:27:18,779 INFO ================EVALUATION AT # 75================
2024-02-25 02:28:00,123 INFO ================EVALUATION AT # 80================
2024-02-25 02:28:41,733 INFO ================EVALUATION AT # 85================
2024-02-25 02:29:23,333 INFO ================EVALUATION AT # 90================
2024-02-25 02:30:04,706 INFO ================EVALUATION AT # 95================
2024-02-25 02:30:46,398 INFO ================EVALUATION AT # 100================
2024-02-25 02:31:28,137 INFO ================EVALUATION AT # 105================
2024-02-25 02:32:09,823 INFO ================EVALUATION AT # 110================
2024-02-25 02:32:51,696 INFO ================EVALUATION AT # 115================
2024-02-25 02:33:33,377 INFO ================EVALUATION AT # 120================
2024-02-25 02:34:15,276 INFO ================EVALUATION AT # 125================
2024-02-25 02:34:56,921 INFO ================EVALUATION AT # 130================
2024-02-25 02:35:38,562 INFO ================EVALUATION AT # 135================
2024-02-25 02:36:20,017 INFO ================EVALUATION AT # 140================
2024-02-25 02:37:02,076 INFO ================EVALUATION AT # 145================
2024-02-25 02:37:43,885 INFO ================EVALUATION AT # 150================
2024-02-25 02:38:25,762 INFO ================EVALUATION AT # 155================
2024-02-25 02:39:07,468 INFO ================EVALUATION AT # 160================
2024-02-25 02:39:48,801 INFO ================EVALUATION AT # 165================
2024-02-25 02:40:30,695 INFO ================EVALUATION AT # 170================
2024-02-25 02:41:12,156 INFO ================EVALUATION AT # 175================
2024-02-25 02:41:53,879 INFO ================EVALUATION AT # 180================
2024-02-25 02:42:35,550 INFO ================EVALUATION AT # 185================
2024-02-25 02:43:17,251 INFO ================EVALUATION AT # 190================
2024-02-25 02:43:58,706 INFO ================EVALUATION AT # 195================
2024-02-25 02:44:40,315 INFO ================EVALUATION AT # 200================
2024-02-25 02:45:22,236 INFO ================EVALUATION AT # 205================
2024-02-25 02:46:03,978 INFO ================EVALUATION AT # 210================
2024-02-25 02:46:45,638 INFO ================EVALUATION AT # 215================
2024-02-25 02:47:26,585 INFO ================EVALUATION AT # 220================
2024-02-25 02:48:38,292 INFO ================EVALUATION AT # 225================
2024-02-25 03:04:22,808 INFO ================EVALUATION AT # 230================
2024-02-25 03:05:04,788 INFO ================EVALUATION AT # 235================
2024-02-25 03:05:45,173 INFO ================EVALUATION AT # 240================
2024-02-25 03:23:54,852 INFO ================EVALUATION AT # 245================
2024-02-25 03:24:35,071 INFO ================EVALUATION AT # 250================
2024-02-25 03:25:15,514 INFO ================EVALUATION AT # 255================
2024-02-25 03:41:09,314 INFO ================EVALUATION AT # 260================
2024-02-25 03:56:56,233 INFO ================EVALUATION AT # 265================
2024-02-25 04:12:35,500 INFO ================EVALUATION AT # 270================
2024-02-25 04:13:16,573 INFO ================EVALUATION AT # 275================
2024-02-25 04:13:58,038 INFO ================EVALUATION AT # 280================
2024-02-25 04:14:39,301 INFO ================EVALUATION AT # 285================
2024-02-25 04:15:22,475 INFO ================EVALUATION AT # 290================
2024-02-25 04:31:43,394 INFO ================EVALUATION AT # 295================
2024-02-25 04:47:29,756 INFO ================EVALUATION AT # 300================
2024-02-25 05:05:33,395 INFO ================EVALUATION AT # 305================
2024-02-25 05:06:14,169 INFO ================EVALUATION AT # 310================
2024-02-25 05:06:54,970 INFO ================EVALUATION AT # 315================
2024-02-25 05:07:35,912 INFO ================EVALUATION AT # 320================
2024-02-25 05:23:17,232 INFO ================EVALUATION AT # 325================
2024-02-25 05:23:57,910 INFO ================EVALUATION AT # 330================
2024-02-25 05:24:38,523 INFO ================EVALUATION AT # 335================
2024-02-25 05:25:20,183 INFO ================EVALUATION AT # 340================
2024-02-25 05:42:21,362 INFO ================EVALUATION AT # 345================
2024-02-25 05:43:02,038 INFO ================EVALUATION AT # 350================
2024-02-25 05:43:42,800 INFO ================EVALUATION AT # 355================
2024-02-25 05:48:24,192 INFO ================EVALUATION AT # 360================
2024-02-25 05:49:04,713 INFO ================EVALUATION AT # 365================
2024-02-25 05:49:45,383 INFO ================EVALUATION AT # 370================
2024-02-25 06:06:14,554 INFO ================EVALUATION AT # 375================
2024-02-25 06:06:55,246 INFO ================EVALUATION AT # 380================
2024-02-25 06:07:35,985 INFO ================EVALUATION AT # 385================
2024-02-25 06:08:16,718 INFO ================EVALUATION AT # 390================
2024-02-25 06:25:23,981 INFO ================EVALUATION AT # 395================
2024-02-25 06:26:04,468 INFO ================EVALUATION AT # 400================
2024-02-25 06:26:45,128 INFO ================EVALUATION AT # 405================
2024-02-25 06:42:50,029 INFO ================EVALUATION AT # 410================
2024-02-25 06:43:31,288 INFO ================EVALUATION AT # 415================
2024-02-25 06:49:40,498 INFO ================EVALUATION AT # 420================
2024-02-25 06:50:19,798 INFO ================EVALUATION AT # 425================
2024-02-25 06:50:59,795 INFO ================EVALUATION AT # 430================
2024-02-25 07:07:51,295 INFO ================EVALUATION AT # 435================
2024-02-25 07:08:31,274 INFO ================EVALUATION AT # 440================
2024-02-25 07:09:11,374 INFO ================EVALUATION AT # 445================
2024-02-25 07:25:17,904 INFO ================EVALUATION AT # 450================
2024-02-25 07:25:57,958 INFO ================EVALUATION AT # 455================
2024-02-25 07:26:37,787 INFO ================EVALUATION AT # 460================
2024-02-25 07:27:17,545 INFO ================EVALUATION AT # 465================
2024-02-25 07:27:58,656 INFO ================EVALUATION AT # 470================
2024-02-25 07:28:41,885 INFO ================EVALUATION AT # 475================
2024-02-25 07:29:25,848 INFO ================EVALUATION AT # 480================
2024-02-25 07:45:54,606 INFO ================EVALUATION AT # 485================
2024-02-25 07:46:34,419 INFO ================EVALUATION AT # 490================
2024-02-25 07:47:14,335 INFO ================EVALUATION AT # 495================
2024-02-25 07:47:54,661 INFO ================EVALUATION AT # 500================
2024-02-25 07:48:03,139 INFO agent_timesteps_total: 512000
connector_metrics:
  StateBufferConnector_ms: 0.002263784408569336
  ViewRequirementAgentConnector_ms: 0.0968770980834961
counters:
  num_agent_steps_sampled: 512000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 512000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2024-02-25_07-48-03
done: false
episode_len_mean: 18.6
episode_media: {}
episode_reward_max: 0.0
episode_reward_mean: -2499.3675234371262
episode_reward_min: -7804.865227670726
episodes_this_iter: 56
episodes_total: 26652
evaluation:
  connector_metrics:
    StateBufferConnector_ms: 0.002304712931315104
    ViewRequirementAgentConnector_ms: 0.09641647338867188
  custom_metrics: {}
  episode_len_mean: 20.0
  episode_media: {}
  episode_reward_max: -1942.2154055254975
  episode_reward_mean: -2776.770945779457
  episode_reward_min: -3737.6756320423056
  episodes_this_iter: 3
  hist_stats:
    episode_lengths:
    - 20
    - 20
    - 20
    episode_reward:
    - -3737.6756320423056
    - -2650.421799770568
    - -1942.2154055254975
  num_agent_steps_sampled_this_iter: 60
  num_env_steps_sampled_this_iter: 60
  num_faulty_episodes: 0
  num_healthy_workers: 3
  num_in_flight_async_reqs: 0
  num_remote_worker_restarts: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.041553860165050936
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 464.36515172958053
    mean_inference_ms: 0.9629559600311538
    mean_raw_obs_processing_ms: 1.0066343787589034
  sampler_results:
    connector_metrics:
      StateBufferConnector_ms: 0.002304712931315104
      ViewRequirementAgentConnector_ms: 0.09641647338867188
    custom_metrics: {}
    episode_len_mean: 20.0
    episode_media: {}
    episode_reward_max: -1942.2154055254975
    episode_reward_mean: -2776.770945779457
    episode_reward_min: -3737.6756320423056
    episodes_this_iter: 3
    hist_stats:
      episode_lengths:
      - 20
      - 20
      - 20
      episode_reward:
      - -3737.6756320423056
      - -2650.421799770568
      - -1942.2154055254975
    num_faulty_episodes: 0
    policy_reward_max: {}
    policy_reward_mean: {}
    policy_reward_min: {}
    sampler_perf:
      mean_action_processing_ms: 0.041553860165050936
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 464.36515172958053
      mean_inference_ms: 0.9629559600311538
      mean_raw_obs_processing_ms: 1.0066343787589034
  timesteps_this_iter: 60
hostname: OrangeBookPro14.lan
info:
  learner:
    __all__:
      num_agent_steps_trained: 64.0
      num_env_steps_trained: 256.0
      total_loss: 9.38612224260966
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 0.867762565612793
      curr_lr: 5.0e-05
      default_optimizer_lr: 5.0000000000000016e-05
      entropy: 1.629571976015965
      gradients_default_optimizer_global_norm: 1.5188712049896518
      mean_kl_loss: 0.011449645605189573
      policy_loss: -0.10523508184899887
      total_loss: 9.38612224260966
      vf_explained_var: -9.705622990926108e-07
      vf_loss: 9.481421740849813
      vf_loss_unclipped: 1192772.8649739583
  num_agent_steps_sampled: 512000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 512000
  num_env_steps_trained: 0
iterations_since_restore: 500
node_ip: 127.0.0.1
num_agent_steps_sampled: 512000
num_agent_steps_trained: 0
num_env_steps_sampled: 512000
num_env_steps_sampled_this_iter: 1024
num_env_steps_sampled_throughput_per_sec: 125.46187691345425
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 4
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 37.78333333333334
  ram_util_percent: 78.95
pid: 45433
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.03863863509338663
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 48.45275108039018
  mean_inference_ms: 0.6672599794406875
  mean_raw_obs_processing_ms: 2.823691536033586
sampler_results:
  connector_metrics:
    StateBufferConnector_ms: 0.002263784408569336
    ViewRequirementAgentConnector_ms: 0.0968770980834961
  custom_metrics: {}
  episode_len_mean: 18.6
  episode_media: {}
  episode_reward_max: 0.0
  episode_reward_mean: -2499.3675234371262
  episode_reward_min: -7804.865227670726
  episodes_this_iter: 56
  hist_stats:
    episode_lengths: [1, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,
      20, 20, 20, 20, 1, 20, 20, 20, 20, 20, 20, 20, 20, 1, 20, 20, 20, 20, 20, 20,
      20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,
      6, 20, 20, 20, 20, 20, 6, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 1, 20,
      20, 20, 20, 20, 20, 20, 20, 2, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 2,
      20, 20, 20, 20, 20]
    episode_reward: [0.0, -1606.6808676561132, -48.28427124746189, -5373.093004704247,
      -4085.386190491889, -605.6021977856105, -584.3908891458577, -4549.668282769196,
      -4446.478373053004, -1182.6345334875143, -1969.2494294530375, -885.2535300326416,
      -564.2824932587188, -3398.3516090963594, -512.1110255092798, -2437.848879788996,
      -3427.8262286956456, -4039.374526952866, -5329.022536361677, -697.3991434434581,
      0.0, -572.5720918167332, -5211.1336498525425, -1482.1085444772036, -7804.865227670726,
      -6878.073710257509, -2020.3447251418763, -6042.6785246336685, -476.2049935181332,
      0.0, -1365.6021977856105, -1445.8167554957793, -3540.4543109109027, -1140.0,
      -2707.695526217004, -6120.530786906022, -1561.6637831516923, -4987.2407649983825,
      -2392.3105625617663, -2575.1666779889897, -5315.673255104738, -2248.010988928053,
      -3896.9687879018757, -300.0, -3034.4009029333856, -1873.9113089708733, -5918.026223088312,
      -1873.350264665395, -1661.7358552246073, -48.28427124746189, -1902.4089294905757,
      -797.7290753773409, -4642.717143105245, -947.9886131684378, -1867.6626721410998,
      -300.09060283938237, -567.6568542494924, -5501.987190955152, -1958.6342439892267,
      -1873.9113089708733, -632.1110255092798, -1202.077736062998, -5697.8635880152515,
      -2783.895106420352, -4800.070230570783, -748.679622641132, -5354.3172612320795,
      -1076.3330765278395, -7183.963522966351, -1481.332806552346, -3484.9992961846965,
      -663.9783301566339, -4161.4257793998895, 0.0, -1814.743037983565, -500.0, -1713.891878400344,
      -1858.3704097703771, -4547.940252460853, -2623.058928759319, -1416.1803979532963,
      -1018.3809798272988, -106.8806130178211, -63.24555320336757, -1883.8924713892823,
      -1464.3908891458573, -321.9803902718558, -3018.0712787060775, -647.5922680420301,
      -5867.38140429992, -1482.0806439525136, -4353.833264048516, -605.6021977856105,
      -4556.509259542322, -222.04834939252004, -1924.8853500094638, -4417.932916795096,
      -3977.546571308387, -2534.386793044563, -7126.329523693033]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.03863863509338663
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 48.45275108039018
    mean_inference_ms: 0.6672599794406875
    mean_raw_obs_processing_ms: 2.823691536033586
time_since_restore: 19861.451031923294
time_this_iter_s: 8.462440967559814
time_total_s: 19861.451031923294
timers:
  sample_time_ms: 916.758
  synch_weights_time_ms: 5.879
  training_iteration_time_ms: 2028.133
timestamp: 1708876083
timesteps_total: 512000
training_iteration: 500
trial_id: default

2024-02-25 07:48:03,140 DEBUG {'env': {'coefficient_dict': {'p_b': 1.0, 'p_d': 1.0, 'r_c': 1.0}, 'cropped_map_size': 64, 'action_space_size': 32, 'n_maps': 400, 'n_steps_per_map': 20, 'no_masking': False, 'original_map_path': './resource/usc.png', 'original_map_scale': 3.4375, 'preset_map_path': './resource/setup_400.json', 'ratio_coverage': 0.0125}, 'eval': {'evaluation_config': {'env_config': {'evaluation': True, 'n_maps': 1, 'preset_map_path': None}, 'exploration': {'explore': False}}, 'evaluation_duration': 3, 'evaluation_interval': 5, 'evaluation_num_workers': 3}, 'explore': {'exploration_config': {'type': 'StochasticSampling'}, 'explore': True}, 'report': {'min_sample_timesteps_per_iteration': 1000}, 'resource': {'num_cpus_per_worker': 2, 'num_gpus': 0, 'num_gpus_per_worker': 0}, 'rollout': {'num_envs_per_worker': 1, 'num_rollout_workers': 4}, 'stop': {'training_iteration': 500}, 'train': {'lr': 5e-05, 'gamma': 0.9, 'grad_clip': 40.0, 'train_batch_size': 256, 'sgd_minibatch_size': 64, 'num_sgd_iter': 30}}
2024-02-25 07:48:03,140 INFO =============TRAINING ENDED=============
2024-02-25 07:48:04,151 INFO average coverage reward for trained map 0: 703.0, optimal reward: 703
2024-02-25 07:48:04,826 INFO average coverage reward for trained map 1: 633.35, optimal reward: 770
2024-02-25 07:48:05,315 INFO average coverage reward for trained map 2: 729.6, optimal reward: 767
2024-02-25 07:48:25,583 INFO average coverage reward for new map 0: 666.95, optimal reward: 691
2024-02-25 07:48:26,063 INFO average coverage reward for new map 1: 687.0, optimal reward: 741
2024-02-25 07:48:26,508 INFO average coverage reward for new map 2: 570.15, optimal reward: 756
2024-02-25 15:47:47,696 INFO agent_timesteps_total: 512000
connector_metrics:
  StateBufferConnector_ms: 0.002753019332885742
  ViewRequirementAgentConnector_ms: 0.1084144115447998
counters:
  num_agent_steps_sampled: 512000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 512000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2024-02-25_15-47-47
done: false
episode_len_mean: 95.47
episode_media: {}
episode_reward_max: 0.0
episode_reward_mean: -12018.702435031419
episode_reward_min: -34172.79696416641
episodes_this_iter: 11
episodes_total: 5407
evaluation:
  connector_metrics:
    StateBufferConnector_ms: 0.006771087646484375
    ViewRequirementAgentConnector_ms: 0.1434485117594401
  custom_metrics: {}
  episode_len_mean: 74.0
  episode_media: {}
  episode_reward_max: -3016.5525060596374
  episode_reward_mean: -13007.079812050395
  episode_reward_min: -29156.846609575674
  episodes_this_iter: 3
  hist_stats:
    episode_lengths:
    - 22
    - 100
    - 100
    episode_reward:
    - -6847.840320515875
    - -29156.846609575674
    - -3016.5525060596374
  num_agent_steps_sampled_this_iter: 222
  num_env_steps_sampled_this_iter: 222
  num_faulty_episodes: 0
  num_healthy_workers: 3
  num_in_flight_async_reqs: 0
  num_remote_worker_restarts: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.04188322756944352
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 11.83636268529797
    mean_inference_ms: 0.78497498899875
    mean_raw_obs_processing_ms: 0.3627236792026796
  sampler_results:
    connector_metrics:
      StateBufferConnector_ms: 0.006771087646484375
      ViewRequirementAgentConnector_ms: 0.1434485117594401
    custom_metrics: {}
    episode_len_mean: 74.0
    episode_media: {}
    episode_reward_max: -3016.5525060596374
    episode_reward_mean: -13007.079812050395
    episode_reward_min: -29156.846609575674
    episodes_this_iter: 3
    hist_stats:
      episode_lengths:
      - 22
      - 100
      - 100
      episode_reward:
      - -6847.840320515875
      - -29156.846609575674
      - -3016.5525060596374
    num_faulty_episodes: 0
    policy_reward_max: {}
    policy_reward_mean: {}
    policy_reward_min: {}
    sampler_perf:
      mean_action_processing_ms: 0.04188322756944352
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 11.83636268529797
      mean_inference_ms: 0.78497498899875
      mean_raw_obs_processing_ms: 0.3627236792026796
  timesteps_this_iter: 222
hostname: OrangeBookPro14.lan
info:
  learner:
    __all__:
      num_agent_steps_trained: 128.0
      num_env_steps_trained: 256.0
      total_loss: 9.912464793523153
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 1.920204520225525
      curr_lr: 5.0e-05
      default_optimizer_lr: 5.0000000000000016e-05
      entropy: 2.6428273181120554
      gradients_default_optimizer_global_norm: 0.5714126269022624
      mean_kl_loss: 0.01502868209039055
      policy_loss: -0.11639343202114105
      total_loss: 9.912464793523153
      vf_explained_var: -9.5367431640625e-07
      vf_loss: 10.0
      vf_loss_unclipped: 2699900.625
  num_agent_steps_sampled: 512000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 512000
  num_env_steps_trained: 0
iterations_since_restore: 500
node_ip: 127.0.0.1
num_agent_steps_sampled: 512000
num_agent_steps_trained: 0
num_env_steps_sampled: 512000
num_env_steps_sampled_this_iter: 1024
num_env_steps_sampled_throughput_per_sec: 138.98580094712992
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 4
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 51.84166666666666
  ram_util_percent: 80.99166666666666
pid: 50128
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.04291015621480791
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 11.940599770817608
  mean_inference_ms: 0.723474160487934
  mean_raw_obs_processing_ms: 0.36739330161104455
sampler_results:
  connector_metrics:
    StateBufferConnector_ms: 0.002753019332885742
    ViewRequirementAgentConnector_ms: 0.1084144115447998
  custom_metrics: {}
  episode_len_mean: 95.47
  episode_media: {}
  episode_reward_max: 0.0
  episode_reward_mean: -12018.702435031419
  episode_reward_min: -34172.79696416641
  episodes_this_iter: 11
  hist_stats:
    episode_lengths: [100, 100, 100, 100, 100, 100, 100, 31, 100, 100, 100, 100, 100,
      100, 100, 100, 100, 100, 100, 100, 11, 100, 100, 100, 100, 100, 100, 100, 100,
      100, 100, 1, 100, 100, 100, 100, 100, 100, 100, 100, 18, 100, 100, 100, 100,
      100, 100, 100, 29, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,
      100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,
      100, 100, 100, 100, 100, 100, 100, 100, 57, 100, 100, 100, 100, 100, 100, 100,
      100, 100, 100, 100, 100, 100, 100]
    episode_reward: [-21647.190391685086, -17978.450356453402, -33283.45679032459,
      -600.0, -6186.502351221975, -18032.868751389324, -11848.417097267831, -5021.998946443655,
      -30360.625129772307, -10209.502310972875, -9438.455567635592, -30895.8688405038,
      -7056.923979175296, -3966.475881762793, -9100.477978458302, -4052.8794393763787,
      -28855.149940056584, -4508.066884266638, -22560.83739724787, -32192.949721474488,
      -2367.09313369858, -11380.675584264165, -24733.748797695458, -25423.64859614157,
      -6817.250465660463, -5586.3708429365715, -4061.481879299133, -8665.544690232678,
      -7490.055648516167, -9213.274595042158, -27277.446804668223, 0.0, -4576.305461424009,
      -8782.745906856178, -34172.79696416641, -2652.7692569068713, -26429.971342042423,
      -6115.009669683859, -5518.887554621346, -360.5551275463994, -5650.072131123969,
      -27487.480941688067, -11174.511272627462, -21094.61251249134, -9785.39074852835,
      -6545.8588687053325, -8188.551627855555, -3518.6362146049833, -4717.25530256016,
      -26281.598339512944, -9584.032966784147, -14874.460037868323, -5948.810496825744,
      -24475.31505179167, -3811.7242768623755, -7442.220510185597, -7523.23858399315,
      -8724.549454473874, -223.60679774997928, -10506.225774829854, -17203.138679970605,
      -21358.377431856334, -10440.114330198707, -3800.0, -8863.564212655267, -11667.796225105005,
      -6616.552506059636, -10403.916173898693, -3900.0, -12642.007817011292, -1861.9217691274748,
      -8567.751547301796, -10397.975990718503, -535.6204993518134, -11030.408839647118,
      -31589.044732486243, -1835.755975068582, -13852.786858606933, -19287.05090787519,
      -4138.516480713451, -13847.084548212191, -3140.093455903266, -8898.003971660704,
      -8115.47256313555, -12619.515048394243, -14865.965762277254, -5302.340955543637,
      -948.9265927625657, -16475.209856559588, -6440.175425099124, -4684.782254711239,
      -22316.61489712524, -24131.484675392323, -4138.516480713451, -28747.640458974718,
      -6189.877228616846, -10902.271554554512, -26349.651620660617, -20148.67246586693,
      -6634.760721370805]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.04291015621480791
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 11.940599770817608
    mean_inference_ms: 0.723474160487934
    mean_raw_obs_processing_ms: 0.36739330161104455
time_since_restore: 3343.7109110355377
time_this_iter_s: 8.889257192611694
time_total_s: 3343.7109110355377
timers:
  sample_time_ms: 939.053
  synch_weights_time_ms: 6.57
  training_iteration_time_ms: 1700.725
timestamp: 1708904867
timesteps_total: 512000
training_iteration: 500
trial_id: default

2024-02-25 15:47:47,701 DEBUG {'env': {'coefficient_dict': {'p_b': 1.0, 'p_d': 1.0, 'r_c': 1.0}, 'cropped_map_size': 64, 'action_space_size': 32, 'n_maps': 400, 'n_steps_per_map': 100, 'no_masking': False, 'original_map_path': './resource/usc.png', 'original_map_scale': 3.4375, 'preset_map_path': './resource/setup_400.json', 'ratio_coverage': 0.0125}, 'eval': {'evaluation_config': {'env_config': {'evaluation': True, 'n_maps': 1, 'preset_map_path': None}, 'exploration': {'explore': False}}, 'evaluation_duration': 3, 'evaluation_interval': 5, 'evaluation_num_workers': 3}, 'explore': {'exploration_config': {'type': 'StochasticSampling'}, 'explore': True}, 'report': {'min_sample_timesteps_per_iteration': 1000}, 'resource': {'num_cpus_per_worker': 2, 'num_gpus': 0, 'num_gpus_per_worker': 0}, 'rollout': {'num_envs_per_worker': 1, 'num_rollout_workers': 4}, 'stop': {'training_iteration': 500}, 'train': {'lr': 5e-05, 'gamma': 0.9, 'grad_clip': 40.0, 'train_batch_size': 256, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30}}
2024-02-25 15:47:47,701 INFO =============TRAINING ENDED=============
2024-02-25 23:22:54,776 INFO agent_timesteps_total: 520000
connector_metrics:
  StateBufferConnector_ms: 0.0022783279418945312
  ViewRequirementAgentConnector_ms: 0.09671854972839355
counters:
  num_agent_steps_sampled: 520000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 520000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2024-02-25_23-22-54
done: false
episode_len_mean: 20.0
episode_media: {}
episode_reward_max: 20.0
episode_reward_mean: 13.505365687031826
episode_reward_min: 3.802486680656591
episodes_this_iter: 52
episodes_total: 26000
evaluation:
  connector_metrics:
    StateBufferConnector_ms: 0.002193450927734375
    ViewRequirementAgentConnector_ms: 0.09483496348063152
  custom_metrics: {}
  episode_len_mean: 20.0
  episode_media: {}
  episode_reward_max: 11.500277770831982
  episode_reward_mean: 11.231445866304009
  episode_reward_min: 10.882890607661922
  episodes_this_iter: 3
  hist_stats:
    episode_lengths:
    - 20
    - 20
    - 20
    episode_reward:
    - 11.311169220418128
    - 11.500277770831982
    - 10.882890607661922
  num_agent_steps_sampled_this_iter: 60
  num_env_steps_sampled_this_iter: 60
  num_faulty_episodes: 0
  num_healthy_workers: 3
  num_in_flight_async_reqs: 0
  num_remote_worker_restarts: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.052881304391241064
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 11.618766946711581
    mean_inference_ms: 0.9346470204508545
    mean_raw_obs_processing_ms: 1.025705640959021
  sampler_results:
    connector_metrics:
      StateBufferConnector_ms: 0.002193450927734375
      ViewRequirementAgentConnector_ms: 0.09483496348063152
    custom_metrics: {}
    episode_len_mean: 20.0
    episode_media: {}
    episode_reward_max: 11.500277770831982
    episode_reward_mean: 11.231445866304009
    episode_reward_min: 10.882890607661922
    episodes_this_iter: 3
    hist_stats:
      episode_lengths:
      - 20
      - 20
      - 20
      episode_reward:
      - 11.311169220418128
      - 11.500277770831982
      - 10.882890607661922
    num_faulty_episodes: 0
    policy_reward_max: {}
    policy_reward_mean: {}
    policy_reward_min: {}
    sampler_perf:
      mean_action_processing_ms: 0.052881304391241064
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 11.618766946711581
      mean_inference_ms: 0.9346470204508545
      mean_raw_obs_processing_ms: 1.025705640959021
  timesteps_this_iter: 60
hostname: OrangeBookPro14.lan
info:
  learner:
    __all__:
      num_agent_steps_trained: 8.0
      num_env_steps_trained: 80.0
      total_loss: -0.03526782759775718
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 2.1112682819366455
      curr_lr: 1.0e-05
      default_optimizer_lr: 1.0e-05
      entropy: 1.8161107244218389
      gradients_default_optimizer_global_norm: 3.7223698790868123
      mean_kl_loss: 0.013700494990572073
      policy_loss: -0.10890257436782122
      total_loss: -0.03526782759775718
      vf_explained_var: .nan
      vf_loss: 0.04470932834005604
      vf_loss_unclipped: 0.04470932834005604
  num_agent_steps_sampled: 520000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 520000
  num_env_steps_trained: 0
iterations_since_restore: 500
node_ip: 127.0.0.1
num_agent_steps_sampled: 520000
num_agent_steps_trained: 0
num_env_steps_sampled: 520000
num_env_steps_sampled_this_iter: 1040
num_env_steps_sampled_throughput_per_sec: 34.23778715422225
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 4
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 29.825
  ram_util_percent: 81.225
pid: 53411
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.04221096373754368
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 11.904524849078111
  mean_inference_ms: 0.728942239546846
  mean_raw_obs_processing_ms: 0.9279475984941609
sampler_results:
  connector_metrics:
    StateBufferConnector_ms: 0.0022783279418945312
    ViewRequirementAgentConnector_ms: 0.09671854972839355
  custom_metrics: {}
  episode_len_mean: 20.0
  episode_media: {}
  episode_reward_max: 20.0
  episode_reward_mean: 13.505365687031826
  episode_reward_min: 3.802486680656591
  episodes_this_iter: 52
  hist_stats:
    episode_lengths: [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,
      20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,
      20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,
      20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,
      20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,
      20, 20, 20, 20, 20, 20, 20, 20, 20]
    episode_reward: [17.972346342471123, 17.64074500449394, 6.960517713069274, 13.247759448328134,
      9.665456071367585, 11.310793374973198, 11.71402184054301, 10.647769197546655,
      14.302456380499724, 10.013398438600865, 9.165031507881372, 11.359807742824357,
      18.709906878856394, 8.745479350789115, 14.062619903670386, 15.121659169558683,
      12.145251772317787, 8.38465132861036, 18.182263024120385, 13.664958767230193,
      11.257178225231236, 19.488492882929986, 19.53797748286737, 18.137566848092277,
      13.82392034650843, 16.543286233912657, 15.064494411624066, 9.12872382264041,
      16.951052526863, 18.621998540645926, 11.266141723423361, 4.451009819280308,
      18.316123930520916, 9.707528324995653, 12.144630793825485, 9.385798702229422,
      9.148642720736412, 10.569068644400001, 18.31488002363554, 19.281495778845954,
      11.062186044296867, 19.07602824735212, 10.00311659539229, 13.218846724609563,
      7.858214008690961, 18.239250490717282, 15.950931315023075, 9.972887051567229,
      18.945721784630223, 13.451486209069001, 14.146015993439315, 12.656384819651244,
      11.528781001026905, 20.0, 17.728767928414698, 13.097043822039385, 11.038385447135436,
      20.0, 6.1935639239499976, 18.876445040010136, 10.963096075909608, 11.428325728014961,
      13.204074631233913, 20.0, 11.454545713793403, 12.149408641127026, 14.136458527851122,
      16.754811602811078, 18.438312357514306, 12.407174603212718, 7.673503063187484,
      15.302591535549833, 15.362760990825045, 17.718381838052146, 14.555995592015313,
      18.27227087071803, 9.903211631933612, 10.961018765718913, 11.384490366906036,
      19.44511212498637, 14.09285616409981, 16.733122622683293, 3.802486680656591,
      13.520723348300027, 13.125529781128154, 18.17022535941271, 10.155456661276883,
      19.382812536317356, 11.155750505725917, 14.91252139499121, 20.0, 4.722062666865007,
      13.625279798787323, 14.88146110541587, 11.398367026807312, 6.738668972821513,
      12.75269300062399, 17.42234463880306, 4.367470329143291, 10.860229959985665]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.04221096373754368
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 11.904524849078111
    mean_inference_ms: 0.728942239546846
    mean_raw_obs_processing_ms: 0.9279475984941609
time_since_restore: 16320.399999141693
time_this_iter_s: 30.66309404373169
time_total_s: 16320.399999141693
timers:
  sample_time_ms: 283.987
  synch_weights_time_ms: 5.97
  training_iteration_time_ms: 2330.732
timestamp: 1708932174
timesteps_total: 520000
training_iteration: 500
trial_id: default

2024-02-25 23:22:54,781 DEBUG {'env': {'coefficient_dict': {'p_b': 1.0, 'p_d': 1.0, 'r_c': 1.0}, 'cropped_map_size': 64, 'action_space_size': 32, 'n_maps': 400, 'n_steps_per_map': 20, 'no_masking': False, 'original_map_path': './resource/usc.png', 'original_map_scale': 3.4375, 'preset_map_path': './resource/setup_400.json', 'ratio_coverage': 0.0125}, 'eval': {'evaluation_config': {'env_config': {'evaluation': True, 'n_maps': 1, 'preset_map_path': None}, 'exploration': {'explore': False}}, 'evaluation_duration': 3, 'evaluation_interval': 5, 'evaluation_num_workers': 3}, 'explore': {'exploration_config': {'type': 'StochasticSampling'}, 'explore': True}, 'report': {'min_sample_timesteps_per_iteration': 1000}, 'resource': {'num_cpus_per_worker': 2, 'num_gpus': 0, 'num_gpus_per_worker': 0}, 'rollout': {'num_envs_per_worker': 1, 'num_rollout_workers': 4}, 'stop': {'training_iteration': 500}, 'train': {'lr': 1e-05, 'gamma': 0.1, 'grad_clip': 40.0, 'train_batch_size': 32, 'sgd_minibatch_size': 8, 'num_sgd_iter': 30}}
2024-02-25 23:22:54,781 INFO =============TRAINING ENDED=============
2024-02-25 23:22:56,233 INFO average coverage reward for trained map 0: 658.1, optimal reward: 703
2024-02-25 23:22:56,753 INFO average coverage reward for trained map 1: 444.1, optimal reward: 770
2024-02-25 23:22:57,229 INFO average coverage reward for trained map 2: 711.0, optimal reward: 767
2024-02-25 23:23:15,089 INFO average coverage reward for new map 0: 574.35, optimal reward: 682
2024-02-25 23:23:15,528 INFO average coverage reward for new map 1: 639.4, optimal reward: 693
2024-02-25 23:23:16,040 INFO average coverage reward for new map 2: 489.6, optimal reward: 533
2024-02-26 01:54:53,159 INFO average coverage reward for trained map 0: 673.15, optimal reward: 703, ratio: 0.9575391180654338
2024-02-26 01:54:53,678 INFO average coverage reward for trained map 1: 586.85, optimal reward: 770, ratio: 0.7621428571428571
2024-02-26 01:54:54,163 INFO average coverage reward for trained map 2: 730.0, optimal reward: 767, ratio: 0.9517601043024772
2024-02-26 01:55:15,063 INFO average coverage reward for new map 0: 762.5, optimal reward: 786, ratio: 0.9701017811704835
2024-02-26 01:55:15,505 INFO average coverage reward for new map 1: 444.8, optimal reward: 700, ratio: 0.6354285714285715
2024-02-26 01:55:15,990 INFO average coverage reward for new map 2: 653.4, optimal reward: 719, ratio: 0.9087621696801113
2024-02-26 06:38:59,285 INFO agent_timesteps_total: 520000
connector_metrics:
  StateBufferConnector_ms: 0.002833843231201172
  ViewRequirementAgentConnector_ms: 0.10750508308410645
counters:
  num_agent_steps_sampled: 520000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 520000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2024-02-26_06-38-59
done: false
episode_len_mean: 20.0
episode_media: {}
episode_reward_max: 19.702682341501124
episode_reward_mean: 13.990610877075444
episode_reward_min: -6.983418381499163
episodes_this_iter: 52
episodes_total: 26000
evaluation:
  connector_metrics:
    StateBufferConnector_ms: 0.0024557113647460938
    ViewRequirementAgentConnector_ms: 0.1024166742960612
  custom_metrics: {}
  episode_len_mean: 20.0
  episode_media: {}
  episode_reward_max: 13.76799909545398
  episode_reward_mean: 12.194841527748727
  episode_reward_min: 11.174321002639603
  episodes_this_iter: 3
  hist_stats:
    episode_lengths:
    - 20
    - 20
    - 20
    episode_reward:
    - 13.76799909545398
    - 11.642204485152593
    - 11.174321002639603
  num_agent_steps_sampled_this_iter: 60
  num_env_steps_sampled_this_iter: 60
  num_faulty_episodes: 0
  num_healthy_workers: 3
  num_in_flight_async_reqs: 0
  num_remote_worker_restarts: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.054072483805602894
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 12.013555784414516
    mean_inference_ms: 0.9266882563916203
    mean_raw_obs_processing_ms: 1.0211154776335993
  sampler_results:
    connector_metrics:
      StateBufferConnector_ms: 0.0024557113647460938
      ViewRequirementAgentConnector_ms: 0.1024166742960612
    custom_metrics: {}
    episode_len_mean: 20.0
    episode_media: {}
    episode_reward_max: 13.76799909545398
    episode_reward_mean: 12.194841527748727
    episode_reward_min: 11.174321002639603
    episodes_this_iter: 3
    hist_stats:
      episode_lengths:
      - 20
      - 20
      - 20
      episode_reward:
      - 13.76799909545398
      - 11.642204485152593
      - 11.174321002639603
    num_faulty_episodes: 0
    policy_reward_max: {}
    policy_reward_mean: {}
    policy_reward_min: {}
    sampler_perf:
      mean_action_processing_ms: 0.054072483805602894
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 12.013555784414516
      mean_inference_ms: 0.9266882563916203
      mean_raw_obs_processing_ms: 1.0211154776335993
  timesteps_this_iter: 60
hostname: OrangeBookPro14.lan
info:
  learner:
    __all__:
      num_agent_steps_trained: 8.0
      num_env_steps_trained: 80.0
      total_loss: 0.0003334343961129586
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 1.4507486820220947
      curr_lr: 1.0e-05
      default_optimizer_lr: 1.0e-05
      entropy: 0.5212108367729199
      gradients_default_optimizer_global_norm: 4.527411999901136
      mean_kl_loss: 0.0021993141890861777
      policy_loss: -0.03801671673854192
      total_loss: 0.0003334343961129586
      vf_explained_var: .nan
      vf_loss: 0.031968853600944085
      vf_loss_unclipped: 0.031968853600944085
  num_agent_steps_sampled: 520000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 520000
  num_env_steps_trained: 0
iterations_since_restore: 500
node_ip: 127.0.0.1
num_agent_steps_sampled: 520000
num_agent_steps_trained: 0
num_env_steps_sampled: 520000
num_env_steps_sampled_this_iter: 1040
num_env_steps_sampled_throughput_per_sec: 31.973069558111455
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 4
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 40.64565217391304
  ram_util_percent: 82.90217391304348
pid: 58715
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.043533633242109085
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 11.953610367018518
  mean_inference_ms: 0.7426808031176242
  mean_raw_obs_processing_ms: 0.9345033472358353
sampler_results:
  connector_metrics:
    StateBufferConnector_ms: 0.002833843231201172
    ViewRequirementAgentConnector_ms: 0.10750508308410645
  custom_metrics: {}
  episode_len_mean: 20.0
  episode_media: {}
  episode_reward_max: 19.702682341501124
  episode_reward_mean: 13.990610877075444
  episode_reward_min: -6.983418381499163
  episodes_this_iter: 52
  hist_stats:
    episode_lengths: [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,
      20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,
      20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,
      20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,
      20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,
      20, 20, 20, 20, 20, 20, 20, 20, 20]
    episode_reward: [11.621485459594705, 15.71407396869379, 15.97282497213874, 16.68445774720378,
      16.21519985596001, 16.68445774720378, 9.4867332927063, 16.48558201897488, 12.618479620544358,
      16.877143644582763, 16.371834333473117, 13.598135006441831, 9.784689439720854,
      10.169476318045595, 10.824360871286846, 15.603215140381819, 15.971197298827368,
      15.9529695141856, 10.179345556224003, 12.702216329219425, 14.188660211670516,
      12.127721993982497, 18.63563798351848, 12.501746739524428, 10.681227420134396,
      17.205540166204976, 19.518727227041097, 13.661895470153935, 13.979567824468218,
      11.137592113569164, 12.255947236891375, 17.475813073246066, 10.810850070633194,
      5.476798024278032, 11.976149911662596, 13.178499217965888, 19.560439844858216,
      4.294715360039957, 12.472709837858627, 15.387121892281657, 15.736726943565671,
      12.405684932865057, 2.360153951001269, 17.50160236137353, 11.149635477903985,
      18.95180571281959, 13.540812906510146, 19.168144408480764, 10.300957879988525,
      18.014804908356943, 11.453942941818406, 14.074691586978059, 14.679562085449907,
      9.206693467236821, 12.113289314439156, 16.01858211616097, 9.853640315878623,
      -6.983418381499163, 18.014804908356943, 16.01858211616097, 17.494567054996857,
      18.1707073212749, 19.560439844858216, 14.831274808178565, 14.557571010763036,
      19.62835292687642, 16.084641477484823, 5.625903350921088, 11.637612084854771,
      19.168144408480764, 14.393341597393471, 18.76667965056243, 10.834042613285119,
      17.672310385374896, 15.861704839565004, 19.075553440291458, 14.411632576119104,
      13.3464502677081, 15.659146467039568, 14.010352119734506, 18.556268285763892,
      15.986129615490414, 19.702682341501124, 12.146174554870182, 19.459708263523304,
      9.053018409409944, 16.705236646494324, 10.564709579833373, 12.32472967022165,
      17.655904945867626, 11.980974057717583, 15.463108454799432, 17.901095435590005,
      9.98355732170203, 10.400545720769394, 13.31741095913166, 17.348352066627793,
      11.812882953896857, 16.450249438023956, 11.831981031307844]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.043533633242109085
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 11.953610367018518
    mean_inference_ms: 0.7426808031176242
    mean_raw_obs_processing_ms: 0.9345033472358353
time_since_restore: 16014.918388843536
time_this_iter_s: 32.839133977890015
time_total_s: 16014.918388843536
timers:
  sample_time_ms: 288.514
  synch_weights_time_ms: 5.828
  training_iteration_time_ms: 2504.18
timestamp: 1708958339
timesteps_total: 520000
training_iteration: 500
trial_id: default

2024-02-26 06:38:59,286 DEBUG {'env': {'coefficient_dict': {'p_b': 1.0, 'p_d': 1.0, 'r_c': 1.0}, 'cropped_map_size': 64, 'action_space_size': 32, 'n_maps': 400, 'n_steps_per_map': 20, 'no_masking': False, 'original_map_path': './resource/usc.png', 'original_map_scale': 3.4375, 'preset_map_path': './resource/setup_400.json', 'ratio_coverage': 0.0125}, 'eval': {'evaluation_config': {'env_config': {'evaluation': True, 'n_maps': 1, 'preset_map_path': None}, 'exploration': {'explore': False}}, 'evaluation_duration': 3, 'evaluation_interval': 5, 'evaluation_num_workers': 3}, 'explore': {'exploration_config': {'type': 'StochasticSampling'}, 'explore': True}, 'report': {'min_sample_timesteps_per_iteration': 1000}, 'resource': {'num_cpus_per_worker': 2, 'num_gpus': 0, 'num_gpus_per_worker': 0}, 'rollout': {'num_envs_per_worker': 1, 'num_rollout_workers': 4}, 'stop': {'training_iteration': 500}, 'train': {'lr': 1e-05, 'gamma': 0.1, 'grad_clip': 40.0, 'train_batch_size': 32, 'sgd_minibatch_size': 8, 'num_sgd_iter': 30}}
2024-02-26 06:38:59,286 INFO =============TRAINING ENDED=============
2024-02-26 06:39:00,911 INFO average coverage reward for trained map 0: 676.85, optimal reward: 703, ratio: 0.9628022759601708
2024-02-26 06:39:01,454 INFO average coverage reward for trained map 1: 746.0, optimal reward: 770, ratio: 0.9688311688311688
2024-02-26 06:39:01,951 INFO average coverage reward for trained map 2: 739.0, optimal reward: 767, ratio: 0.9634941329856584
2024-02-26 06:39:16,682 INFO average coverage reward for new map 0: 309.0, optimal reward: 768, ratio: 0.40234375
2024-02-26 06:39:17,160 INFO average coverage reward for new map 1: 708.35, optimal reward: 743, ratio: 0.9533647375504711
2024-02-26 06:39:17,647 INFO average coverage reward for new map 2: 719.75, optimal reward: 767, ratio: 0.9383963494132985
2024-03-11 23:21:37,327 INFO agent_timesteps_total: 3200
connector_metrics:
  StateBufferConnector_ms: 0.01105809211730957
  ViewRequirementAgentConnector_ms: 0.5275697708129883
counters:
  num_agent_steps_sampled: 3200
  num_agent_steps_trained: 0
  num_env_steps_sampled: 3200
  num_env_steps_trained: 0
custom_metrics: {}
date: 2024-03-11_23-21-37
done: false
episode_len_mean: 31.01
episode_media: {}
episode_reward_max: 15.578422298378277
episode_reward_mean: 3.852226221712982
episode_reward_min: -9.240046012825786
episodes_this_iter: 1
episodes_total: 101
evaluation:
  connector_metrics:
    StateBufferConnector_ms: 0.006858507792154948
    ViewRequirementAgentConnector_ms: 0.38483142852783203
  custom_metrics: {}
  episode_len_mean: 32.0
  episode_media: {}
  episode_reward_max: 7.015889752415603
  episode_reward_mean: 6.2528261491022015
  episode_reward_min: 5.354354777840215
  episodes_this_iter: 3
  hist_stats:
    episode_lengths:
    - 32
    - 32
    - 32
    episode_reward:
    - 7.015889752415603
    - 6.388233917050786
    - 5.354354777840215
  num_agent_steps_sampled_this_iter: 96
  num_env_steps_sampled_this_iter: 96
  num_faulty_episodes: 0
  num_healthy_workers: 3
  num_in_flight_async_reqs: 0
  num_remote_worker_restarts: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15604701789036873
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 73.15768130468949
    mean_inference_ms: 15.857580691702927
    mean_raw_obs_processing_ms: 1800.4443117937324
  sampler_results:
    connector_metrics:
      StateBufferConnector_ms: 0.006858507792154948
      ViewRequirementAgentConnector_ms: 0.38483142852783203
    custom_metrics: {}
    episode_len_mean: 32.0
    episode_media: {}
    episode_reward_max: 7.015889752415603
    episode_reward_mean: 6.2528261491022015
    episode_reward_min: 5.354354777840215
    episodes_this_iter: 3
    hist_stats:
      episode_lengths:
      - 32
      - 32
      - 32
      episode_reward:
      - 7.015889752415603
      - 6.388233917050786
      - 5.354354777840215
    num_faulty_episodes: 0
    policy_reward_max: {}
    policy_reward_mean: {}
    policy_reward_min: {}
    sampler_perf:
      mean_action_processing_ms: 0.15604701789036873
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 73.15768130468949
      mean_inference_ms: 15.857580691702927
      mean_raw_obs_processing_ms: 1800.4443117937324
  timesteps_this_iter: 96
hostname: d13-04.hpc.usc.edu
info:
  learner:
    __all__:
      num_agent_steps_trained: 8.0
      num_env_steps_trained: 32.0
      total_loss: -0.12623690863450368
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 0.569531261920929
      curr_lr: 1.0e-05
      default_optimizer_lr: 1.0e-05
      entropy: 4.153402649362882
      gradients_default_optimizer_global_norm: 10.360714990459382
      mean_kl_loss: 0.004881209926922262
      policy_loss: -0.18968579458693663
      total_loss: -0.12623690863450368
      vf_explained_var: -0.0003282199303309123
      vf_loss: 0.05788888053660912
      vf_loss_unclipped: 0.05788888053660912
  num_agent_steps_sampled: 3200
  num_agent_steps_trained: 0
  num_env_steps_sampled: 3200
  num_env_steps_trained: 0
iterations_since_restore: 100
node_ip: 10.125.19.125
num_agent_steps_sampled: 3200
num_agent_steps_trained: 0
num_env_steps_sampled: 3200
num_env_steps_sampled_this_iter: 32
num_env_steps_sampled_throughput_per_sec: 0.5841882969904274
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 4
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 9.231578947368423
  ram_util_percent: 14.766165413533832
pid: 21092
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.1596986534435773
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 190.4826189245036
  mean_inference_ms: 16.526760818767464
  mean_raw_obs_processing_ms: 1639.9173954217827
sampler_results:
  connector_metrics:
    StateBufferConnector_ms: 0.01105809211730957
    ViewRequirementAgentConnector_ms: 0.5275697708129883
  custom_metrics: {}
  episode_len_mean: 31.01
  episode_media: {}
  episode_reward_max: 15.578422298378277
  episode_reward_mean: 3.852226221712982
  episode_reward_min: -9.240046012825786
  episodes_this_iter: 1
  hist_stats:
    episode_lengths: [32, 32, 32, 9, 32, 32, 32, 32, 32, 31, 32, 32, 32, 32, 32, 32,
      32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,
      32, 32, 32, 32, 2, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,
      32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,
      32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3, 32, 32, 32, 16,
      32, 32, 32, 32, 32, 32]
    episode_reward: [2.5311359735837096, -2.1661088655657856, -0.8984936580340632,
      1.6013663700825709, -2.331732527550539, -0.9649735896138156, 0.1885983808363571,
      6.4733208298267595, 4.08065651338483, 4.076868270385967, 4.671540890650355,
      1.0965945147990035, 10.122692700399957, 9.78821434109516, 1.407622520505889,
      -3.7853921443751237, -1.800938076506583, -0.5151911602405725, 4.998462996714084,
      4.7470328402643345, -0.04850705886718548, -1.1412483250486036, -9.240046012825786,
      6.023694054078037, 1.5007476124863757, 4.969673003730374, 2.4158234440874624,
      0.8439479551332812, -2.0411896104299645, 0.8297336111523984, 5.856474994264126,
      3.2420855665770993, 3.6382907610051762, 4.766680477793058, -3.6323917747296206,
      6.12235839309044, 4.977735797344663, 10.664185503408035, 5.0160515191814214,
      0.9146929827694711, 10.748267756659994, 4.740162885031181, 7.546829732209198,
      7.405729659229624, 2.3535320872340737, 1.8934168826207272, 5.750189498617518,
      3.871847960603348, 0.3914964229877799, -6.4068864506308065, 5.235908595430084,
      4.053299150277809, 7.452919812404131, 3.4327315381902364, 7.138096425195237,
      1.8076254245296022, -1.196165198152772, 3.1345267645976254, 7.273240132539867,
      -0.32570189728272414, 4.309484507019167, -0.03023053922612129, 15.578422298378277,
      11.047313010326057, 6.311662292269742, 5.843498754965083, 12.80227073382489,
      4.161339289090231, 0.589001294223916, 7.689956924509669, 3.4392809357855088,
      3.4595123146751847, 2.990897993629078, 10.592574615894586, 10.375545443871502,
      0.14908603607801962, 4.5727278111847545, 8.20568946503093, 4.874933297677897,
      4.159038712172509, 13.597214579510792, 2.267852410701804, 3.368214853372305,
      -3.009297396953443, 2.2408224848891343, 9.107114292349461, 5.607012277126575,
      11.267276790784837, 0.17107161468370458, 0.8103173299731199, 5.863887574904498,
      3.9880959344765623, 1.9517155945861298, 5.360863383026167, 1.7422064359343272,
      8.029051464275078, 5.2677466746123525, 5.117080998726898, 10.001829715191988,
      6.049402770610539]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1596986534435773
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 190.4826189245036
    mean_inference_ms: 16.526760818767464
    mean_raw_obs_processing_ms: 1639.9173954217827
time_since_restore: 6472.474566221237
time_this_iter_s: 92.8505597114563
time_total_s: 6472.474566221237
timers:
  sample_time_ms: 32975.798
  synch_weights_time_ms: 192.755
  training_iteration_time_ms: 48095.931
timestamp: 1710224497
timesteps_total: 3200
training_iteration: 100
trial_id: default

2024-03-11 23:21:37,372 DEBUG {'env': {'coefficient_dict': {'p_d': 1.0, 'r_c': 1.0}, 'map_size': 256, 'action_space_size': 64, 'n_maps': 100, 'n_steps_per_map': 32, 'dataset_dir': 'resource/usc_old/'}, 'eval': {'evaluation_config': {'env_config': {'evaluation': True, 'n_maps': 50}, 'exploration': {'explore': False}}, 'evaluation_duration': 3, 'evaluation_interval': 5, 'evaluation_num_workers': 3}, 'explore': {'exploration_config': {'type': 'StochasticSampling'}, 'explore': True}, 'report': {'min_sample_timesteps_per_iteration': 32}, 'resource': {'num_cpus_per_worker': 1, 'num_gpus': 1, 'num_gpus_per_worker': 0}, 'rollout': {'num_envs_per_worker': 1, 'num_rollout_workers': 4}, 'stop': {'training_iteration': 100}, 'train': {'lr': 1e-05, 'gamma': 0, 'grad_clip': 1.0, 'train_batch_size': 32, 'sgd_minibatch_size': 8, 'num_sgd_iter': 30, 'model': {'fcnet_activation': 'relu'}}}
2024-03-11 23:21:37,373 INFO =============TRAINING ENDED=============
2024-03-11 23:32:43,665 INFO overall average coverage reward for trained maps: 7125.4106249999995, average optimal reward: 11106.900000000001, ratio: 0.6415300961564432
2024-03-11 23:32:43,679 INFO overall average coverage reward for new maps: 6790.81875, average optimal reward: 10849.599999999999, ratio: 0.6259049872806371
2024-03-12 00:14:32,903 INFO agent_timesteps_total: 3200
connector_metrics:
  StateBufferConnector_ms: 0.006423473358154297
  ViewRequirementAgentConnector_ms: 0.2508511543273926
counters:
  num_agent_steps_sampled: 3200
  num_agent_steps_trained: 0
  num_env_steps_sampled: 3200
  num_env_steps_trained: 0
custom_metrics: {}
date: 2024-03-12_00-14-32
done: false
episode_len_mean: 31.31
episode_media: {}
episode_reward_max: 12.164054583931302
episode_reward_mean: 4.009583519014174
episode_reward_min: -5.593458422114141
episodes_this_iter: 2
episodes_total: 101
evaluation:
  connector_metrics:
    StateBufferConnector_ms: 0.0035921732584635415
    ViewRequirementAgentConnector_ms: 0.19500255584716797
  custom_metrics: {}
  episode_len_mean: 32.0
  episode_media: {}
  episode_reward_max: 5.691468625747037
  episode_reward_mean: 5.329799601333345
  episode_reward_min: 5.110591850722759
  episodes_this_iter: 3
  hist_stats:
    episode_lengths:
    - 32
    - 32
    - 32
    episode_reward:
    - 5.18733832753024
    - 5.110591850722759
    - 5.691468625747037
  num_agent_steps_sampled_this_iter: 96
  num_env_steps_sampled_this_iter: 96
  num_faulty_episodes: 0
  num_healthy_workers: 3
  num_in_flight_async_reqs: 0
  num_remote_worker_restarts: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.07365414713347405
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2.4012011124873327
    mean_inference_ms: 6.320541643036857
    mean_raw_obs_processing_ms: 43.4895328372105
  sampler_results:
    connector_metrics:
      StateBufferConnector_ms: 0.0035921732584635415
      ViewRequirementAgentConnector_ms: 0.19500255584716797
    custom_metrics: {}
    episode_len_mean: 32.0
    episode_media: {}
    episode_reward_max: 5.691468625747037
    episode_reward_mean: 5.329799601333345
    episode_reward_min: 5.110591850722759
    episodes_this_iter: 3
    hist_stats:
      episode_lengths:
      - 32
      - 32
      - 32
      episode_reward:
      - 5.18733832753024
      - 5.110591850722759
      - 5.691468625747037
    num_faulty_episodes: 0
    policy_reward_max: {}
    policy_reward_mean: {}
    policy_reward_min: {}
    sampler_perf:
      mean_action_processing_ms: 0.07365414713347405
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 2.4012011124873327
      mean_inference_ms: 6.320541643036857
      mean_raw_obs_processing_ms: 43.4895328372105
  timesteps_this_iter: 96
hostname: OrangeBook-Pro-14.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 8.0
      num_env_steps_trained: 32.0
      total_loss: -0.16617891742304589
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 0.22500000894069672
      curr_lr: 1.0e-05
      default_optimizer_lr: 1.0e-05
      entropy: 5.3354817748069765
      gradients_default_optimizer_global_norm: 8.418694572461147
      mean_kl_loss: 0.011863036378053948
      policy_loss: -0.227245467590789
      total_loss: -0.16617891742304589
      vf_explained_var: 0.017768250405788423
      vf_loss: 0.05839735673119625
      vf_loss_unclipped: 0.05839735673119625
  num_agent_steps_sampled: 3200
  num_agent_steps_trained: 0
  num_env_steps_sampled: 3200
  num_env_steps_trained: 0
iterations_since_restore: 100
node_ip: 127.0.0.1
num_agent_steps_sampled: 3200
num_agent_steps_trained: 0
num_env_steps_sampled: 3200
num_env_steps_sampled_this_iter: 32
num_env_steps_sampled_throughput_per_sec: 3.3923083476992435
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 4
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 86.19999999999999
  ram_util_percent: 79.5875
pid: 97770
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.09583228822868588
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 6.551831712608119
  mean_inference_ms: 8.126655311504603
  mean_raw_obs_processing_ms: 48.288254317738485
sampler_results:
  connector_metrics:
    StateBufferConnector_ms: 0.006423473358154297
    ViewRequirementAgentConnector_ms: 0.2508511543273926
  custom_metrics: {}
  episode_len_mean: 31.31
  episode_media: {}
  episode_reward_max: 12.164054583931302
  episode_reward_mean: 4.009583519014174
  episode_reward_min: -5.593458422114141
  episodes_this_iter: 2
  hist_stats:
    episode_lengths: [32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,
      32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 22, 32, 32, 32, 32, 32, 32,
      32, 32, 32, 32, 32, 32, 32, 32, 32, 5, 32, 32, 28, 32, 32, 32, 32, 32, 32, 32,
      32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 4, 32,
      32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,
      32, 32, 32, 32, 32, 32, 32]
    episode_reward: [5.627736077705262, 0.5820071924995516, -5.593458422114141, -1.6056166843776951,
      -2.0880524516207446, 0.8555366096447012, 7.77654347112304, 3.315871450965387,
      2.5124118172790126, 1.3136593420224298, 1.2867614790776665, 3.494864600073517,
      3.7452703111413994, 1.8457318514757086, 5.489290135010019, -1.6802959977115786,
      2.063835444075755, 6.809167046728891, 8.832772328864843, 5.62610703249611, 7.058831860809398,
      0.8712261046590832, 7.3760007913598935, 2.1408405963774917, -1.842992974497136,
      9.550658500254501, 6.693803632292334, 4.0884346391686375, 7.30963024916602,
      4.4873063090240315, -0.8208014101938584, 7.116868499506722, 3.494469844560471,
      7.914873007212809, 1.7237995487361537, -4.430032098055743, 0.21345509178145444,
      -4.454557207368197, 6.486150692841219, 8.096194793859036, -2.789791997686711,
      -1.539909261151934, 8.208176065472541, 3.12205958550323, 1.452002386023958,
      8.046746574105416, 12.164054583931302, 7.729974077076739, 6.117646999459075,
      4.4296476652034, 8.948581330718591, 4.05746598947892, 0.8627865548413056, 4.734481611790204,
      2.404446533805328, 3.9466468900112317, 1.8803349764432378, 3.687778898702687,
      5.279508861319619, -3.215743711993725, 2.8446831386270306, -2.5119798752594305,
      6.736284668801939, 3.5695675748118396, 7.612176106422044, 1.4679744600841225,
      5.609576119708251, 0.8230515625531784, 8.85990564367464, 2.0043646816053715,
      11.537159958154815, 4.200333432042146, 2.102254288188025, 5.895738385798392,
      0.17091990063263174, 0.584390612521599, 2.2036340127460763, 10.310020379625053,
      9.544971216872206, 10.621738455825266, 3.7293886374747123, 3.4227625610912815,
      1.4878475401948086, 5.762564917880035, 7.182940512705121, 2.553112767328408,
      11.024526456728319, 3.2651753137522195, 1.126739209053616, 4.813806546899929,
      9.341478583523797, 3.5382503050631793, 5.501142895043092, 9.509014414864474,
      5.786133379925117, 4.874302275985614, 1.1255880610491584, 9.515022352710528,
      1.2094427535871357, 9.191149974213792]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09583228822868588
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 6.551831712608119
    mean_inference_ms: 8.126655311504603
    mean_raw_obs_processing_ms: 48.288254317738485
time_since_restore: 971.2418246269226
time_this_iter_s: 11.23358416557312
time_total_s: 971.2418246269226
timers:
  sample_time_ms: 1240.76
  synch_weights_time_ms: 77.311
  training_iteration_time_ms: 9338.615
timestamp: 1710227672
timesteps_total: 3200
training_iteration: 100
trial_id: default

2024-03-12 00:14:32,905 DEBUG {'env': {'coefficient_dict': {'p_d': 1.0, 'r_c': 1.0}, 'map_size': 256, 'action_space_size': 64, 'n_maps': 100, 'n_steps_per_map': 32, 'dataset_dir': 'resource/usc_old/'}, 'eval': {'evaluation_config': {'env_config': {'evaluation': True, 'n_maps': 50}, 'exploration': {'explore': False}}, 'evaluation_duration': 3, 'evaluation_interval': 5, 'evaluation_num_workers': 3}, 'explore': {'exploration_config': {'type': 'StochasticSampling'}, 'explore': True}, 'report': {'min_sample_timesteps_per_iteration': 32}, 'resource': {'num_cpus_per_worker': 1, 'num_gpus': 1, 'num_gpus_per_worker': 0}, 'rollout': {'num_envs_per_worker': 1, 'num_rollout_workers': 4}, 'stop': {'training_iteration': 100}, 'train': {'lr': 1e-05, 'gamma': 0, 'grad_clip': 1.0, 'train_batch_size': 32, 'sgd_minibatch_size': 8, 'num_sgd_iter': 30, 'model': {'fcnet_activation': 'relu'}}}
2024-03-12 00:14:32,905 INFO =============TRAINING ENDED=============
2024-03-12 00:15:01,617 INFO overall average coverage reward for trained maps: 6540.737499999999, average optimal reward: 10945.300000000001, ratio: 0.5975841228655221
2024-03-12 00:15:01,618 INFO overall average coverage reward for new maps: 6694.009375000001, average optimal reward: 10849.599999999999, ratio: 0.6169821352860936
2024-03-12 12:15:01,440 INFO ===========train and eval started at 0312_1215===========
2024-03-12 12:35:40,409 INFO ===========train and eval started at 0312_1235===========
2024-03-12 12:39:25,916 INFO ===========train and eval started at 0312_1239===========
2024-03-12 12:43:49,451 INFO ===========train and eval started at 0312_1243===========
2024-03-12 12:45:25,858 INFO ===========train and eval started at 0312_1245===========
2024-03-12 12:48:31,131 INFO ===========train and eval started at 0312_1248===========
2024-03-12 17:45:05,822 INFO ===========train and eval started at 0312_1745===========
2024-03-12 18:51:41,208 INFO ===========train and eval started at 0312_1851===========
2024-03-13 02:29:51,242 INFO ===========train and eval started at 0313_0229===========
2024-03-13 02:38:30,279 INFO ===========train and eval started at 0313_0238===========
2024-03-14 23:45:45,184 INFO ===========train and eval started at 0314_2345===========
2024-03-15 01:23:27,775 INFO ===========train and eval started at 0315_0123===========
2024-03-15 14:59:05,613 INFO ===========train and eval started at 0315_1459===========
2024-03-15 15:00:06,139 INFO ===========train and eval started at 0315_1500===========
2024-03-15 15:16:15,490 INFO ===========train and eval started at 0315_1516===========
2024-03-16 19:49:49,841 INFO ===========train and eval started at 0316_1949===========
2024-03-16 20:52:01,683 INFO ===========train and eval started at 0316_2052===========
2024-03-17 20:03:46,944 INFO ===========train and eval started at 0317_2003===========
2024-03-18 00:35:03,838 INFO ===========train and eval started at 0318_0035===========
2024-03-18 03:56:02,626 INFO ===========train and eval started at 0318_0356===========
2024-03-19 02:40:08,356 INFO ===========ppo train and eval started at 0319_0240===========
2024-03-19 05:13:55,994 INFO agent_timesteps_total: 200000
connector_metrics:
  StateBufferConnector_ms: 0.0017244815826416016
  ViewRequirementAgentConnector_ms: 0.11270403861999512
counters:
  num_agent_steps_sampled: 200000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 200000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2024-03-19_05-13-55
done: false
episode_len_mean: 1151.84
episode_media: {}
episode_reward_max: 1842.801061571131
episode_reward_mean: 795.5910754854905
episode_reward_min: 1.7606591932547668
episodes_this_iter: 1
episodes_total: 194
evaluation:
  connector_metrics:
    StateBufferConnector_ms: 0.0019311904907226562
    ViewRequirementAgentConnector_ms: 0.13263225555419922
  custom_metrics: {}
  episode_len_mean: 164.0
  episode_media: {}
  episode_reward_max: 124.39749759384004
  episode_reward_mean: 124.39749759384004
  episode_reward_min: 124.39749759384004
  episodes_this_iter: 1
  hist_stats:
    episode_lengths:
    - 164
    episode_reward:
    - 124.39749759384004
  num_agent_steps_sampled_this_iter: 164
  num_env_steps_sampled_this_iter: 164
  num_faulty_episodes: 0
  num_healthy_workers: 0
  num_in_flight_async_reqs: 0
  num_remote_worker_restarts: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.03904357044832064
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.0681038200062932
    mean_inference_ms: 2.619400176150047
    mean_raw_obs_processing_ms: 1.5702339675393426
  sampler_results:
    connector_metrics:
      StateBufferConnector_ms: 0.0019311904907226562
      ViewRequirementAgentConnector_ms: 0.13263225555419922
    custom_metrics: {}
    episode_len_mean: 164.0
    episode_media: {}
    episode_reward_max: 124.39749759384004
    episode_reward_mean: 124.39749759384004
    episode_reward_min: 124.39749759384004
    episodes_this_iter: 1
    hist_stats:
      episode_lengths:
      - 164
      episode_reward:
      - 124.39749759384004
    num_faulty_episodes: 0
    policy_reward_max: {}
    policy_reward_mean: {}
    policy_reward_min: {}
    sampler_perf:
      mean_action_processing_ms: 0.03904357044832064
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 0.0681038200062932
      mean_inference_ms: 2.619400176150047
      mean_raw_obs_processing_ms: 1.5702339675393426
  timesteps_this_iter: 164
hostname: OrangeBook-Pro-14.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 64.0
      num_env_steps_trained: 2000.0
      total_loss: -0.08425625458534465
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 3.4171876907348633
      curr_lr: 1.0e-05
      default_optimizer_lr: 9.999999999999999e-06
      entropy: 3.719040353923464
      gradients_default_optimizer_global_norm: 1.5747390067907794
      mean_kl_loss: 0.012046400283587161
      policy_loss: -0.1338191275308127
      total_loss: -0.08425625458534465
      vf_explained_var: 0.03192595600573493
      vf_loss: 0.008398063915487784
      vf_loss_unclipped: 0.008398063915487784
  num_agent_steps_sampled: 200000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 200000
  num_env_steps_trained: 0
iterations_since_restore: 100
node_ip: 127.0.0.1
num_agent_steps_sampled: 200000
num_agent_steps_trained: 0
num_env_steps_sampled: 200000
num_env_steps_sampled_this_iter: 2000
num_env_steps_sampled_throughput_per_sec: 22.18739736295287
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 83.79389312977099
  ram_util_percent: 73.97557251908397
pid: 43990
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.040191117380853326
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.03294696955569882
  mean_inference_ms: 2.658637736371945
  mean_raw_obs_processing_ms: 1.7387678507457336
sampler_results:
  connector_metrics:
    StateBufferConnector_ms: 0.0017244815826416016
    ViewRequirementAgentConnector_ms: 0.11270403861999512
  custom_metrics: {}
  episode_len_mean: 1151.84
  episode_media: {}
  episode_reward_max: 1842.801061571131
  episode_reward_mean: 795.5910754854905
  episode_reward_min: 1.7606591932547668
  episodes_this_iter: 1
  hist_stats:
    episode_lengths: [2000, 1565, 276, 633, 408, 261, 743, 276, 1101, 1065, 43, 35,
      327, 276, 42, 188, 222, 2000, 2000, 2000, 526, 2000, 22, 523, 2000, 94, 1168,
      2000, 13, 2000, 640, 2000, 2000, 2000, 2000, 79, 2000, 738, 272, 1036, 12, 994,
      307, 639, 2000, 2000, 2000, 488, 137, 2000, 2000, 174, 2000, 2000, 20, 2000,
      1049, 2000, 2000, 2000, 999, 18, 2000, 998, 2000, 2000, 2000, 2000, 2000, 2000,
      2000, 92, 2000, 2000, 2000, 368, 191, 2000, 427, 2000, 65, 19, 2000, 1680, 644,
      474, 2000, 439, 2000, 863, 181, 2000, 2000, 2, 2000, 29, 2000, 2000, 792, 511]
    episode_reward: [1330.0621169627093, 1023.839668223548, 159.2049187161316, 443.98937742471844,
      210.4445706497046, 145.98051010587096, 594.3688459351896, 175.01121408711748,
      698.5155260777814, 734.0939297758366, 24.47489189566188, 27.720302836425688,
      253.56065866481504, 196.09836654101613, 27.529039361409307, 112.78208629164392,
      157.13327196481538, 1120.328491438776, 1108.8276488982533, 1483.0672114402473,
      336.0016777116013, 1541.6626578472665, 14.56185205394029, 393.6004840594224,
      1366.7334998216222, 48.42040997557908, 658.9512764758, 1193.7007376185468, 8.132257324256098,
      1276.9635933445272, 463.58056399132465, 1468.5739077792402, 1150.9955605665525,
      1493.4869506651057, 1357.2183696699071, 45.773375246728584, 1394.2488196885965,
      543.7587965764686, 156.11893000279608, 735.3953864832039, 6.988719684151157,
      795.9122851365081, 214.64733284618197, 345.96847594216587, 1574.7009193054205,
      1581.3008137119102, 1318.4895813047751, 297.0783610114324, 102.3963057790782,
      1468.8988524152705, 1123.927349387811, 93.68075572241733, 1629.2670237805296,
      1227.7929392446515, 16.00493682537026, 1044.8328108672963, 623.9374243923907,
      980.3559372815422, 1386.5326674989528, 1397.0806095849198, 824.3516215015574,
      11.813532110091744, 1060.6392072139724, 755.8736864228662, 1541.3942190105483,
      1623.6534280936391, 1529.6328257191278, 1334.9111660195467, 1842.801061571131,
      1282.6914322723917, 1521.5168297455757, 57.06943443804035, 1574.726839597683,
      1575.6854714065125, 1582.1345671267306, 258.2525955044855, 158.0223445595856,
      1799.0160354467835, 265.8741392773481, 1668.7579676203723, 51.417617617617644,
      14.617851622874808, 1144.3989134013814, 1084.183363358655, 509.9943131132919,
      340.1659699468976, 1143.2403825717386, 306.48400635208714, 1465.9552742615954,
      645.7341168996153, 147.21631838706278, 913.6833145434119, 1296.7818142028452,
      1.7606591932547668, 1729.990543278992, 25.001262258471687, 1345.1037372375383,
      1191.674681238621, 574.4933110367923, 453.685837451371]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.040191117380853326
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.03294696955569882
    mean_inference_ms: 2.658637736371945
    mean_raw_obs_processing_ms: 1.7387678507457336
time_since_restore: 9226.834740638733
time_this_iter_s: 92.06049609184265
time_total_s: 9226.834740638733
timers:
  sample_time_ms: 8962.624
  synch_weights_time_ms: 32.081
  training_iteration_time_ms: 91028.301
timestamp: 1710850435
timesteps_total: 200000
training_iteration: 100
trial_id: default

2024-03-19 05:13:55,994 DEBUG {'env': {'coefficient_dict': {'p_d': 1.0, 'r_c': 1.0}, 'map_size': 256, 'action_space_size': 64, 'n_maps': 100, 'n_steps_per_map': 2000, 'dataset_dir': 'resource/usc_old', 'no_masking': False}, 'eval': {'evaluation_config': {'env_config': {'evaluation': True, 'n_maps': 50}, 'exploration': {'explore': False}}, 'evaluation_duration': 1, 'evaluation_interval': 5, 'evaluation_num_workers': 0}, 'explore': {'exploration_config': {'type': 'StochasticSampling'}, 'explore': True}, 'report': {'min_sample_timesteps_per_iteration': 2000}, 'resource': {'num_cpus_per_worker': 1, 'num_gpus': 0, 'num_gpus_per_worker': 0}, 'rollout': {'num_envs_per_worker': 1, 'num_rollout_workers': 0}, 'stop': {'training_iteration': 100}, 'train': {'lr': 1e-05, 'gamma': 0.1, 'grad_clip': 40.0, 'train_batch_size': 2000, 'sgd_minibatch_size': 64, 'num_sgd_iter': 30, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh'}}, 'agent': {'data_saving_interval': 10}}
2024-03-19 05:13:55,994 INFO =============TRAINING ENDED=============
2024-03-19 05:14:46,689 INFO overall average coverage reward for trained maps: 7023.830935840317, average optimal reward: 11217.0, ratio: 0.6261773144192134
2024-03-19 05:14:46,689 INFO overall average coverage reward for new maps: 6836.21350658305, average optimal reward: 11355.0, ratio: 0.6020443422794408
2024-03-19 13:44:49,391 INFO ===========PPO train and eval started at 0319_1344===========
2024-03-19 16:00:03,919 INFO agent_timesteps_total: 200000
connector_metrics:
  StateBufferConnector_ms: 0.0017313957214355469
  ViewRequirementAgentConnector_ms: 0.1302015781402588
counters:
  num_agent_steps_sampled: 200000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 200000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2024-03-19_16-00-03
done: false
episode_len_mean: 1328.14
episode_media: {}
episode_reward_max: 1724.0915074310044
episode_reward_mean: 897.197205428718
episode_reward_min: 1.0
episodes_this_iter: 3
episodes_total: 172
evaluation:
  connector_metrics:
    StateBufferConnector_ms: 0.0011205673217773438
    ViewRequirementAgentConnector_ms: 0.1245737075805664
  custom_metrics: {}
  episode_len_mean: 99.0
  episode_media: {}
  episode_reward_max: 65.8556067588326
  episode_reward_mean: 65.8556067588326
  episode_reward_min: 65.8556067588326
  episodes_this_iter: 1
  hist_stats:
    episode_lengths:
    - 99
    episode_reward:
    - 65.8556067588326
  num_agent_steps_sampled_this_iter: 99
  num_env_steps_sampled_this_iter: 99
  num_faulty_episodes: 0
  num_healthy_workers: 0
  num_in_flight_async_reqs: 0
  num_remote_worker_restarts: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.04136850527853312
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.06908672063161302
    mean_inference_ms: 2.6696788249379253
    mean_raw_obs_processing_ms: 1.7624908781067872
  sampler_results:
    connector_metrics:
      StateBufferConnector_ms: 0.0011205673217773438
      ViewRequirementAgentConnector_ms: 0.1245737075805664
    custom_metrics: {}
    episode_len_mean: 99.0
    episode_media: {}
    episode_reward_max: 65.8556067588326
    episode_reward_mean: 65.8556067588326
    episode_reward_min: 65.8556067588326
    episodes_this_iter: 1
    hist_stats:
      episode_lengths:
      - 99
      episode_reward:
      - 65.8556067588326
    num_faulty_episodes: 0
    policy_reward_max: {}
    policy_reward_mean: {}
    policy_reward_min: {}
    sampler_perf:
      mean_action_processing_ms: 0.04136850527853312
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 0.06908672063161302
      mean_inference_ms: 2.6696788249379253
      mean_raw_obs_processing_ms: 1.7624908781067872
  timesteps_this_iter: 99
hostname: OrangeBook-Pro-14.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 64.0
      num_env_steps_trained: 2000.0
      total_loss: -0.03430310446125612
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 3.8443360328674316
      curr_lr: 1.0e-05
      default_optimizer_lr: 9.999999999999999e-06
      entropy: 3.3295353902682567
      gradients_default_optimizer_global_norm: 2.0769807299825427
      mean_kl_loss: 0.007097169187893266
      policy_loss: -0.0788070550198351
      total_loss: -0.03430310446125612
      vf_explained_var: 0.026030365401493716
      vf_loss: 0.017220046458595088
      vf_loss_unclipped: 0.017220046458595088
  num_agent_steps_sampled: 200000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 200000
  num_env_steps_trained: 0
iterations_since_restore: 100
node_ip: 127.0.0.1
num_agent_steps_sampled: 200000
num_agent_steps_trained: 0
num_env_steps_sampled: 200000
num_env_steps_sampled_this_iter: 2000
num_env_steps_sampled_throughput_per_sec: 25.3098895618387
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 85.26260869565216
  ram_util_percent: 75.54434782608695
pid: 2140
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.04099299580917069
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.029131033681036735
  mean_inference_ms: 2.6776240748601423
  mean_raw_obs_processing_ms: 1.6644172914429762
sampler_results:
  connector_metrics:
    StateBufferConnector_ms: 0.0017313957214355469
    ViewRequirementAgentConnector_ms: 0.1302015781402588
  custom_metrics: {}
  episode_len_mean: 1328.14
  episode_media: {}
  episode_reward_max: 1724.0915074310044
  episode_reward_mean: 897.197205428718
  episode_reward_min: 1.0
  episodes_this_iter: 3
  hist_stats:
    episode_lengths: [2000, 2000, 609, 1768, 2000, 1017, 12, 2000, 1226, 2000, 2000,
      368, 2000, 2000, 2000, 69, 195, 539, 2000, 2000, 18, 954, 2000, 2000, 1264,
      1887, 220, 391, 2000, 2000, 1129, 904, 2000, 2000, 2000, 454, 573, 2000, 2000,
      675, 97, 1110, 416, 2000, 474, 30, 137, 1012, 635, 2000, 2000, 2000, 1706, 1109,
      13, 2000, 1630, 2000, 296, 2000, 2000, 2000, 1, 1031, 1531, 2000, 2000, 2000,
      1321, 2000, 169, 2000, 144, 2000, 975, 1025, 13, 2000, 713, 2000, 2000, 2000,
      2000, 108, 2000, 2000, 2000, 2000, 760, 2000, 2000, 1256, 2000, 521, 2000, 2000,
      754, 861, 18, 676]
    episode_reward: [1101.7113650883819, 1382.7031870481608, 428.71951951951826, 1277.2017967542502,
      1144.2575732630855, 587.3433069632631, 8.5635593220339, 1372.4196136029877,
      680.6675876726886, 1259.3860027223275, 1296.6963080168787, 275.5214951291821,
      1611.7251916412852, 898.5084554678663, 1184.4403107561002, 55.65536073584366,
      142.2623757599151, 462.6215166521038, 992.3433827778886, 1193.2381602914418,
      10.907299694634286, 712.5847965513623, 1338.0016619923115, 1299.3991786116915,
      769.9223843268045, 1293.842878256053, 116.60745115856427, 242.3458453641318,
      1621.3746446844802, 1275.5908248378198, 690.8796352125443, 616.7438008331682,
      1287.7563118984558, 1498.2770313499789, 1638.041265747433, 342.4089032489678,
      378.0013762730522, 1131.3478193024828, 1578.5799324946206, 370.13040655684586,
      52.82088575376336, 843.8480081716066, 258.62142437714965, 1508.025656444193,
      309.86491542598793, 22.453763979302288, 91.29771673207273, 518.1642122400666,
      384.7136989611012, 1174.9481910783295, 1184.1708865600756, 1201.650129751183,
      1181.1767462039045, 808.1436787407924, 7.61066873370446, 1536.2673850816611,
      1103.8851238945815, 1124.4778127056165, 204.22641888498237, 1469.3458113599095,
      1240.8038027775253, 1608.1946580331994, 1.0, 797.8100775193875, 1185.5966386554562,
      1175.4956150746573, 1538.6354443309472, 1514.5950484764599, 891.1571567672792,
      1276.6344821830965, 134.67520117044617, 1578.4899030335512, 88.70761371928972,
      1041.2612934479835, 808.976625273921, 585.5209359605899, 10.109948958246171,
      1013.6259143155621, 388.978005058837, 913.6927993608393, 1387.7428214731694,
      1374.8267495488205, 1584.4586850288777, 70.3760321100918, 1079.7903437910854,
      1435.7850987809718, 1575.9468593663178, 1584.85984531771, 560.6961082910332,
      1423.7083186257294, 1724.0915074310044, 834.2665618060792, 1511.975048923665,
      302.89616354466915, 1539.9382742191713, 1574.3432938348099, 569.791928063573,
      608.5175889527263, 15.333981001727116, 611.9715159827019]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.04099299580917069
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.029131033681036735
    mean_inference_ms: 2.6776240748601423
    mean_raw_obs_processing_ms: 1.6644172914429762
time_since_restore: 8113.68578004837
time_this_iter_s: 81.06780123710632
time_total_s: 8113.68578004837
timers:
  sample_time_ms: 9187.236
  synch_weights_time_ms: 44.056
  training_iteration_time_ms: 78907.683
timestamp: 1710889203
timesteps_total: 200000
training_iteration: 100
trial_id: default

2024-03-19 16:00:03,919 DEBUG {'env': {'coefficient_dict': {'p_d': 1.0, 'r_c': 1.0}, 'map_size': 256, 'action_space_size': 64, 'n_maps': 100, 'n_steps_per_map': 2000, 'dataset_dir': 'resource/usc_old', 'no_masking': False}, 'eval': {'evaluation_config': {'env_config': {'evaluation': True, 'n_maps': 50}, 'exploration': {'explore': False}}, 'evaluation_duration': 1, 'evaluation_interval': 5, 'evaluation_num_workers': 0}, 'explore': {'exploration_config': {'type': 'StochasticSampling'}, 'explore': True}, 'report': {'min_sample_timesteps_per_iteration': 2000}, 'resource': {'num_cpus_per_worker': 1, 'num_gpus': 0, 'num_gpus_per_worker': 0}, 'rollout': {'num_envs_per_worker': 1, 'num_rollout_workers': 0}, 'stop': {'training_iteration': 100}, 'train': {'lr': 1e-05, 'gamma': 0.1, 'grad_clip': 40.0, 'train_batch_size': 2000, 'sgd_minibatch_size': 64, 'num_sgd_iter': 30, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh'}}, 'agent': {'data_saving_interval': 10}}
2024-03-19 16:00:03,919 INFO =============TRAINING ENDED=============
2024-03-19 16:00:55,842 INFO overall average coverage reward for trained maps: 7602.459705095571, average optimal reward: 11217.0, ratio: 0.6777622987514996
2024-03-19 16:00:55,843 INFO overall average coverage reward for new maps: 6675.206591765924, average optimal reward: 10097.4, ratio: 0.6610817231926955
2024-03-19 16:50:49,088 INFO ===========PPO train and eval started at 0319_1650===========
2024-03-19 19:11:50,527 INFO agent_timesteps_total: 200000
connector_metrics:
  StateBufferConnector_ms: 0.0018131732940673828
  ViewRequirementAgentConnector_ms: 0.28120851516723633
counters:
  num_agent_steps_sampled: 200000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 200000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2024-03-19_19-11-50
done: false
episode_len_mean: 1166.5
episode_media: {}
episode_reward_max: 1601.107289798513
episode_reward_mean: 742.1834349991966
episode_reward_min: 1.0
episodes_this_iter: 2
episodes_total: 193
evaluation:
  connector_metrics:
    StateBufferConnector_ms: 0.0015020370483398438
    ViewRequirementAgentConnector_ms: 0.18181800842285156
  custom_metrics: {}
  episode_len_mean: 560.0
  episode_media: {}
  episode_reward_max: 417.42998335414137
  episode_reward_mean: 417.42998335414137
  episode_reward_min: 417.42998335414137
  episodes_this_iter: 1
  hist_stats:
    episode_lengths:
    - 560
    episode_reward:
    - 417.42998335414137
  num_agent_steps_sampled_this_iter: 560
  num_env_steps_sampled_this_iter: 560
  num_faulty_episodes: 0
  num_healthy_workers: 0
  num_in_flight_async_reqs: 0
  num_remote_worker_restarts: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.04081266778431037
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.11264090430403935
    mean_inference_ms: 2.6775470884989865
    mean_raw_obs_processing_ms: 1.7994849978871175
  sampler_results:
    connector_metrics:
      StateBufferConnector_ms: 0.0015020370483398438
      ViewRequirementAgentConnector_ms: 0.18181800842285156
    custom_metrics: {}
    episode_len_mean: 560.0
    episode_media: {}
    episode_reward_max: 417.42998335414137
    episode_reward_mean: 417.42998335414137
    episode_reward_min: 417.42998335414137
    episodes_this_iter: 1
    hist_stats:
      episode_lengths:
      - 560
      episode_reward:
      - 417.42998335414137
    num_faulty_episodes: 0
    policy_reward_max: {}
    policy_reward_mean: {}
    policy_reward_min: {}
    sampler_perf:
      mean_action_processing_ms: 0.04081266778431037
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 0.11264090430403935
      mean_inference_ms: 2.6775470884989865
      mean_raw_obs_processing_ms: 1.7994849978871175
  timesteps_this_iter: 560
hostname: OrangeBook-Pro-14.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 64.0
      num_env_steps_trained: 2000.0
      total_loss: 0.054663021667504995
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 2.278125047683716
      curr_lr: 1.0e-05
      default_optimizer_lr: 9.999999999999999e-06
      entropy: 5.127967923434813
      gradients_default_optimizer_global_norm: 3.692853624505529
      mean_kl_loss: 0.010270859129227473
      policy_loss: -0.08922013693994511
      total_loss: 0.054663021667504995
      vf_explained_var: 0.010787818985961393
      vf_loss: 0.1204848604471381
      vf_loss_unclipped: 0.13076365322494177
  num_agent_steps_sampled: 200000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 200000
  num_env_steps_trained: 0
iterations_since_restore: 100
node_ip: 127.0.0.1
num_agent_steps_sampled: 200000
num_agent_steps_trained: 0
num_env_steps_sampled: 200000
num_env_steps_sampled_this_iter: 2000
num_env_steps_sampled_throughput_per_sec: 22.744575732549944
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 90.77054263565891
  ram_util_percent: 77.315503875969
pid: 7112
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.04446312582401027
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.06091172036473544
  mean_inference_ms: 2.7633286748721697
  mean_raw_obs_processing_ms: 2.098789196644128
sampler_results:
  connector_metrics:
    StateBufferConnector_ms: 0.0018131732940673828
    ViewRequirementAgentConnector_ms: 0.28120851516723633
  custom_metrics: {}
  episode_len_mean: 1166.5
  episode_media: {}
  episode_reward_max: 1601.107289798513
  episode_reward_mean: 742.1834349991966
  episode_reward_min: 1.0
  episodes_this_iter: 2
  hist_stats:
    episode_lengths: [200, 2000, 508, 471, 1360, 521, 633, 665, 2000, 2000, 638, 258,
      1919, 604, 230, 1476, 2000, 202, 2000, 689, 929, 1704, 294, 118, 1324, 2000,
      716, 326, 2000, 238, 2000, 1052, 1192, 1102, 2000, 345, 466, 1969, 2000, 720,
      670, 2000, 1206, 696, 1877, 2000, 459, 2000, 2000, 175, 2000, 710, 312, 806,
      2000, 324, 315, 283, 2000, 2000, 237, 355, 157, 2000, 626, 1169, 377, 2000,
      2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 142, 2000, 366, 2000,
      723, 84, 2000, 2000, 300, 1, 2000, 2000, 2000, 220, 2000, 2000, 465, 122, 278,
      412, 1572, 2000, 342]
    episode_reward: [134.7657449269268, 1394.8247636854658, 291.25728780802075, 259.8320967069613,
      938.0241086273762, 271.325685294563, 345.672281039461, 502.316657191586, 1235.6200185356793,
      1069.8540850165837, 429.7100773656019, 130.10573301715723, 1460.1833013435723,
      410.89415732566215, 160.73353078441943, 937.6120745022498, 1068.486931419207,
      137.1798097575943, 1057.3502393897625, 345.97425641771497, 675.0712972420835,
      963.6408858317245, 202.96301864101034, 65.71876217943719, 949.0648472709045,
      1309.7215483410666, 364.19329534522365, 185.40742530401872, 1143.531085353004,
      129.26749789642778, 1225.8774996183843, 662.9864642082423, 834.0206840883862,
      587.6197590021842, 1479.4710388954304, 216.7944137991418, 241.9138094889974,
      1358.956805625314, 1311.6972421544028, 453.4517662410284, 479.38344799676196,
      1025.5580936266233, 890.7818503538965, 514.3090062111798, 1068.5281662321252,
      1481.2533197139985, 310.8666897506924, 1396.750632911388, 1154.5674705833248,
      129.777615215801, 1445.0921626189825, 402.8303737938869, 151.0000000000001,
      614.6781917052165, 1095.39261083744, 235.1973893398042, 144.45524207593175,
      162.15858352578906, 954.0854888644768, 1336.6098626716605, 166.25315821134953,
      233.78953798311824, 98.06387614678903, 958.5482810445245, 440.25809163514145,
      834.4933852140076, 281.3371655518396, 1278.9761985335642, 1177.7366749029313,
      1586.1547770700554, 1306.8440044411632, 1441.2871819960767, 1067.573937319887,
      1600.583483324514, 1359.3047398248364, 1462.8587411125043, 1382.984477371229,
      101.4195811744387, 1601.107289798513, 197.95272236814094, 1374.021586396496,
      521.8414414414419, 60.82003477588876, 1143.3105037866328, 1084.7488838411505,
      227.25401427297038, 1.0, 1121.5689691817274, 1208.4707350272213, 1274.1861814346094,
      165.4885641677256, 1510.8744093247935, 954.2788049605385, 256.6127585074954,
      102.21490849861085, 194.111550709254, 345.3876104476166, 768.8127528414577,
      1197.4451730418934, 191.99912752653745]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.04446312582401027
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.06091172036473544
    mean_inference_ms: 2.7633286748721697
    mean_raw_obs_processing_ms: 2.098789196644128
time_since_restore: 8460.58885216713
time_this_iter_s: 91.50003218650818
time_total_s: 8460.58885216713
timers:
  sample_time_ms: 9839.032
  synch_weights_time_ms: 38.418
  training_iteration_time_ms: 84678.741
timestamp: 1710900710
timesteps_total: 200000
training_iteration: 100
trial_id: default

2024-03-19 19:11:50,529 DEBUG {'env': {'coefficient_dict': {'p_d': 1.0, 'r_c': 1.0}, 'map_size': 256, 'action_space_size': 64, 'n_maps': 100, 'n_steps_per_map': 2000, 'dataset_dir': 'resource/usc_old', 'no_masking': False}, 'eval': {'evaluation_config': {'env_config': {'evaluation': True, 'n_maps': 50}, 'exploration': {'explore': False}}, 'evaluation_duration': 1, 'evaluation_interval': 5, 'evaluation_num_workers': 0}, 'explore': {'exploration_config': {'type': 'StochasticSampling'}, 'explore': True}, 'report': {'min_sample_timesteps_per_iteration': 2000}, 'resource': {'num_cpus_per_worker': 1, 'num_gpus': 0, 'num_gpus_per_worker': 0}, 'rollout': {'num_envs_per_worker': 1, 'num_rollout_workers': 0}, 'stop': {'training_iteration': 100}, 'train': {'lr': 1e-05, 'gamma': 0.9, 'grad_clip': 40.0, 'train_batch_size': 2000, 'sgd_minibatch_size': 64, 'num_sgd_iter': 30, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh'}}, 'agent': {'data_saving_interval': 10}}
2024-03-19 19:11:50,529 INFO =============TRAINING ENDED=============
2024-03-19 19:12:43,047 INFO overall average coverage reward for trained maps: 7196.81668013468, average optimal reward: 11217.0, ratio: 0.6415990621498333
2024-03-19 19:12:43,048 INFO overall average coverage reward for new maps: 6836.183351177355, average optimal reward: 10783.0, ratio: 0.6339778680494625
2024-03-21 01:41:13,734 INFO ===========PPO train and eval started at 0321_0141===========
2024-03-21 04:08:30,979 INFO agent_timesteps_total: 200000
connector_metrics:
  StateBufferConnector_ms: 0.0022325515747070312
  ViewRequirementAgentConnector_ms: 0.11675572395324707
counters:
  num_agent_steps_sampled: 200000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 200000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2024-03-21_04-08-30
done: false
episode_len_mean: 2000.0
episode_media: {}
episode_reward_max: 18673459.0
episode_reward_mean: 13718365.98
episode_reward_min: 8478987.0
episodes_this_iter: 1
episodes_total: 100
evaluation:
  connector_metrics:
    StateBufferConnector_ms: 0.002288818359375
    ViewRequirementAgentConnector_ms: 0.1178741455078125
  custom_metrics: {}
  episode_len_mean: 2000.0
  episode_media: {}
  episode_reward_max: 13716637.0
  episode_reward_mean: 13716637.0
  episode_reward_min: 13716637.0
  episodes_this_iter: 1
  hist_stats:
    episode_lengths:
    - 2000
    episode_reward:
    - 13716637.0
  num_agent_steps_sampled_this_iter: 2000
  num_env_steps_sampled_this_iter: 2000
  num_faulty_episodes: 0
  num_healthy_workers: 0
  num_in_flight_async_reqs: 0
  num_remote_worker_restarts: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.0482999051541961
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1.64392556474464
    mean_inference_ms: 2.798333251116583
    mean_raw_obs_processing_ms: 0.6596196612990864
  sampler_results:
    connector_metrics:
      StateBufferConnector_ms: 0.002288818359375
      ViewRequirementAgentConnector_ms: 0.1178741455078125
    custom_metrics: {}
    episode_len_mean: 2000.0
    episode_media: {}
    episode_reward_max: 13716637.0
    episode_reward_mean: 13716637.0
    episode_reward_min: 13716637.0
    episodes_this_iter: 1
    hist_stats:
      episode_lengths:
      - 2000
      episode_reward:
      - 13716637.0
    num_faulty_episodes: 0
    policy_reward_max: {}
    policy_reward_mean: {}
    policy_reward_min: {}
    sampler_perf:
      mean_action_processing_ms: 0.0482999051541961
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 1.64392556474464
      mean_inference_ms: 2.798333251116583
      mean_raw_obs_processing_ms: 0.6596196612990864
  timesteps_this_iter: 2000
hostname: OrangeBook-Pro-14.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 64.0
      num_env_steps_trained: 2000.0
      total_loss: 9.904384642521709
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 5.125781536102295
      curr_lr: 1.0e-05
      default_optimizer_lr: 9.999999999999999e-06
      entropy: 5.157443539165993
      gradients_default_optimizer_global_norm: 1.927816749000346
      mean_kl_loss: 0.011213151980667432
      policy_loss: -0.15309151532803614
      total_loss: 9.904384642521709
      vf_explained_var: -1.2335238426224763e-06
      vf_loss: 10.0
      vf_loss_unclipped: 72004640.66950959
  num_agent_steps_sampled: 200000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 200000
  num_env_steps_trained: 0
iterations_since_restore: 100
node_ip: 127.0.0.1
num_agent_steps_sampled: 200000
num_agent_steps_trained: 0
num_env_steps_sampled: 200000
num_env_steps_sampled_this_iter: 2000
num_env_steps_sampled_throughput_per_sec: 22.945071175099248
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 94.1978417266187
  ram_util_percent: 79.41151079136691
pid: 24896
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.04632376735850174
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 1.655067767394991
  mean_inference_ms: 2.784546986642765
  mean_raw_obs_processing_ms: 0.6988193749442114
sampler_results:
  connector_metrics:
    StateBufferConnector_ms: 0.0022325515747070312
    ViewRequirementAgentConnector_ms: 0.11675572395324707
  custom_metrics: {}
  episode_len_mean: 2000.0
  episode_media: {}
  episode_reward_max: 18673459.0
  episode_reward_mean: 13718365.98
  episode_reward_min: 8478987.0
  episodes_this_iter: 1
  hist_stats:
    episode_lengths: [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000,
      2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000,
      2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000,
      2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000,
      2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000,
      2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000,
      2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000,
      2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000]
    episode_reward: [11464392.0, 12709067.0, 12739187.0, 13412483.0, 13119152.0, 14322921.0,
      13010839.0, 14174905.0, 12898298.0, 13258919.0, 13298247.0, 13139538.0, 13253054.0,
      13147356.0, 12642445.0, 13095740.0, 13311622.0, 16478009.0, 14290535.0, 14230360.0,
      13868570.0, 12923403.0, 13244134.0, 15450014.0, 14308683.0, 13440544.0, 14146108.0,
      16625869.0, 14993417.0, 15127398.0, 13268383.0, 13991221.0, 12894398.0, 13724701.0,
      14060389.0, 18120729.0, 15755690.0, 14490577.0, 13888637.0, 14876546.0, 14089758.0,
      12906174.0, 16156527.0, 16248863.0, 14508496.0, 16059795.0, 18673459.0, 13031842.0,
      17219411.0, 9836307.0, 10133004.0, 9275563.0, 12091413.0, 13157757.0, 10966314.0,
      9998709.0, 10140885.0, 13455314.0, 12771314.0, 13754654.0, 11395968.0, 10227836.0,
      14389614.0, 14135047.0, 14605602.0, 12104596.0, 14240904.0, 15189719.0, 14297333.0,
      13860421.0, 13487665.0, 14842705.0, 15941547.0, 14068051.0, 14553838.0, 14797897.0,
      14170532.0, 14330070.0, 13283301.0, 12117543.0, 9559736.0, 11275377.0, 11883930.0,
      14156174.0, 14487468.0, 8478987.0, 9867030.0, 17842118.0, 15541002.0, 17471004.0,
      11271498.0, 13357410.0, 16305664.0, 14568026.0, 12677161.0, 15073354.0, 14467886.0,
      15329379.0, 15486026.0, 15025140.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.04632376735850174
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1.655067767394991
    mean_inference_ms: 2.784546986642765
    mean_raw_obs_processing_ms: 0.6988193749442114
time_since_restore: 8836.405380487442
time_this_iter_s: 97.84524416923523
time_total_s: 8836.405380487442
timers:
  sample_time_ms: 11434.061
  synch_weights_time_ms: 53.675
  training_iteration_time_ms: 87976.943
timestamp: 1711019310
timesteps_total: 200000
training_iteration: 100
trial_id: default

2024-03-21 04:08:30,979 DEBUG {'env': {'coefficient_dict': {'p_d': 1.0, 'r_c': 1.0}, 'map_size': 256, 'action_space_size': 64, 'n_maps': 100, 'n_steps_per_map': 2000, 'dataset_dir': 'resource/usc_old', 'no_masking': False}, 'eval': {'evaluation_config': {'env_config': {'evaluation': True, 'n_maps': 50}, 'exploration': {'explore': False}}, 'evaluation_duration': 1, 'evaluation_interval': 5, 'evaluation_num_workers': 0}, 'explore': {'exploration_config': {'type': 'StochasticSampling'}, 'explore': True}, 'report': {'min_sample_timesteps_per_iteration': 2000}, 'resource': {'num_cpus_per_worker': 1, 'num_gpus': 0, 'num_gpus_per_worker': 0}, 'rollout': {'num_envs_per_worker': 1, 'num_rollout_workers': 0}, 'stop': {'training_iteration': 100}, 'train': {'lr': 1e-05, 'gamma': 0.1, 'grad_clip': 40.0, 'train_batch_size': 2000, 'sgd_minibatch_size': 64, 'num_sgd_iter': 30, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh'}}, 'agent': {'data_saving_interval': 10}}
2024-03-21 04:08:30,979 INFO =============TRAINING ENDED=============
2024-03-21 04:08:34,109 INFO average coverage reward for trained map 0 in 10 steps: 7641.4, optimal reward: 9821, ratio: 0.7780674065777415
2024-03-21 04:08:35,240 INFO average coverage reward for trained map 1 in 10 steps: 9105.6, optimal reward: 11241, ratio: 0.8100346944222044
2024-03-21 04:08:36,385 INFO average coverage reward for trained map 2 in 10 steps: 7982.6, optimal reward: 11212, ratio: 0.711969318587228
2024-03-21 04:08:38,125 INFO average coverage reward for trained map 3 in 10 steps: 6156.6, optimal reward: 8816, ratio: 0.6983439201451906
2024-03-21 04:08:39,434 INFO average coverage reward for trained map 4 in 10 steps: 7790.1, optimal reward: 11646, ratio: 0.6689077794951056
2024-03-21 04:08:40,692 INFO average coverage reward for new map 0: 6671.6, optimal reward: 10397, ratio: 0.6416851014715784
2024-03-21 04:08:41,937 INFO average coverage reward for new map 1: 7100.1, optimal reward: 10159, ratio: 0.6988975292843784
2024-03-21 04:08:42,883 INFO average coverage reward for new map 2: 6886.2, optimal reward: 10159, ratio: 0.677842307313712
2024-03-21 04:08:44,275 INFO average coverage reward for new map 3: 6442.8, optimal reward: 10278, ratio: 0.6268534734384121
2024-03-21 04:08:45,216 INFO average coverage reward for new map 4: 7326.2, optimal reward: 9388, ratio: 0.7803792074989347
2024-03-21 04:08:45,218 INFO overall average coverage reward for trained maps: 7735.26, average optimal reward: 10547.2, ratio: 0.7333946450242718
2024-03-21 04:08:45,218 INFO overall average coverage reward for test maps: 6885.379999999999, average optimal reward: 10076.2, ratio: 0.6833310176455408
2024-03-21 05:00:04,047 INFO ===========PPO train and eval started at 0321_0500===========
2024-03-21 06:22:12,454 INFO ===========PPO train and eval started at 0321_0622===========
2024-03-21 09:03:43,698 INFO agent_timesteps_total: 200000
connector_metrics:
  StateBufferConnector_ms: 0.0018067359924316406
  ViewRequirementAgentConnector_ms: 0.1297900676727295
counters:
  num_agent_steps_sampled: 200000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 200000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2024-03-21_09-03-43
done: false
episode_len_mean: 2000.0
episode_media: {}
episode_reward_max: 18585981.0
episode_reward_mean: 13743224.37
episode_reward_min: 8362104.0
episodes_this_iter: 1
episodes_total: 100
evaluation:
  connector_metrics:
    StateBufferConnector_ms: 0.0019073486328125
    ViewRequirementAgentConnector_ms: 0.12807846069335938
  custom_metrics: {}
  episode_len_mean: 2000.0
  episode_media: {}
  episode_reward_max: 13414757.0
  episode_reward_mean: 13414757.0
  episode_reward_min: 13414757.0
  episodes_this_iter: 1
  hist_stats:
    episode_lengths:
    - 2000
    episode_reward:
    - 13414757.0
  num_agent_steps_sampled_this_iter: 2000
  num_env_steps_sampled_this_iter: 2000
  num_faulty_episodes: 0
  num_healthy_workers: 0
  num_in_flight_async_reqs: 0
  num_remote_worker_restarts: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.040641281235882926
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1.6028838695155059
    mean_inference_ms: 2.6366416938495454
    mean_raw_obs_processing_ms: 1.2057650843541838
  sampler_results:
    connector_metrics:
      StateBufferConnector_ms: 0.0019073486328125
      ViewRequirementAgentConnector_ms: 0.12807846069335938
    custom_metrics: {}
    episode_len_mean: 2000.0
    episode_media: {}
    episode_reward_max: 13414757.0
    episode_reward_mean: 13414757.0
    episode_reward_min: 13414757.0
    episodes_this_iter: 1
    hist_stats:
      episode_lengths:
      - 2000
      episode_reward:
      - 13414757.0
    num_faulty_episodes: 0
    policy_reward_max: {}
    policy_reward_mean: {}
    policy_reward_min: {}
    sampler_perf:
      mean_action_processing_ms: 0.040641281235882926
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 1.6028838695155059
      mean_inference_ms: 2.6366416938495454
      mean_raw_obs_processing_ms: 1.2057650843541838
  timesteps_this_iter: 2000
hostname: OrangeBookPro14.lan
info:
  learner:
    __all__:
      num_agent_steps_trained: 64.0
      num_env_steps_trained: 2000.0
      total_loss: 9.903628684818617
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 5.125781536102295
      curr_lr: 1.0e-05
      default_optimizer_lr: 9.999999999999999e-06
      entropy: 5.232505900519235
      gradients_default_optimizer_global_norm: 1.9646906856534831
      mean_kl_loss: 0.011406259314590803
      policy_loss: -0.15483727877034242
      total_loss: 9.903628684818617
      vf_explained_var: -9.5367431640625e-07
      vf_loss: 10.0
      vf_loss_unclipped: 71157507.27078891
  num_agent_steps_sampled: 200000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 200000
  num_env_steps_trained: 0
iterations_since_restore: 100
node_ip: 127.0.0.1
num_agent_steps_sampled: 200000
num_agent_steps_trained: 0
num_env_steps_sampled: 200000
num_env_steps_sampled_this_iter: 2000
num_env_steps_sampled_throughput_per_sec: 20.596120939915945
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 86.92402597402598
  ram_util_percent: 69.9038961038961
pid: 27525
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.04132186143612127
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 1.5569648270910934
  mean_inference_ms: 2.6545754745465597
  mean_raw_obs_processing_ms: 1.1792642440095016
sampler_results:
  connector_metrics:
    StateBufferConnector_ms: 0.0018067359924316406
    ViewRequirementAgentConnector_ms: 0.1297900676727295
  custom_metrics: {}
  episode_len_mean: 2000.0
  episode_media: {}
  episode_reward_max: 18585981.0
  episode_reward_mean: 13743224.37
  episode_reward_min: 8362104.0
  episodes_this_iter: 1
  hist_stats:
    episode_lengths: [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000,
      2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000,
      2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000,
      2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000,
      2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000,
      2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000,
      2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000,
      2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000]
    episode_reward: [11460693.0, 12536559.0, 12696548.0, 13357516.0, 13190174.0, 14246808.0,
      13018426.0, 14123548.0, 12792667.0, 13190192.0, 13337532.0, 13004375.0, 13274017.0,
      13070173.0, 12647891.0, 13094646.0, 13575783.0, 16562496.0, 14646833.0, 13860461.0,
      13709941.0, 12910599.0, 13506326.0, 15497050.0, 14410981.0, 13235949.0, 14219342.0,
      16665236.0, 15109534.0, 14808808.0, 13266495.0, 13723051.0, 12897919.0, 13550239.0,
      14148486.0, 18567992.0, 15839173.0, 14401069.0, 13836884.0, 14686408.0, 13968521.0,
      12872105.0, 16050696.0, 16623241.0, 14571786.0, 15989018.0, 18585981.0, 13066844.0,
      17276241.0, 9735860.0, 10439555.0, 9180092.0, 12080434.0, 13239893.0, 11285855.0,
      9893573.0, 10015655.0, 13441724.0, 12868651.0, 13824779.0, 11537551.0, 10433414.0,
      14283119.0, 14116551.0, 14773499.0, 12135017.0, 14410445.0, 15380367.0, 14276670.0,
      13756543.0, 13484363.0, 15043347.0, 16051733.0, 14352165.0, 14712605.0, 14829334.0,
      13701358.0, 14706576.0, 13322389.0, 12238876.0, 9770855.0, 11277003.0, 12095183.0,
      14258637.0, 14538113.0, 8362104.0, 10150088.0, 17697271.0, 15642061.0, 17533907.0,
      11538172.0, 13212642.0, 16731686.0, 14951537.0, 12633979.0, 15181894.0, 14379653.0,
      15215265.0, 15003003.0, 14914138.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.04132186143612127
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1.5569648270910934
    mean_inference_ms: 2.6545754745465597
    mean_raw_obs_processing_ms: 1.1792642440095016
time_since_restore: 9690.510426998138
time_this_iter_s: 108.41275787353516
time_total_s: 9690.510426998138
timers:
  sample_time_ms: 11174.497
  synch_weights_time_ms: 27.226
  training_iteration_time_ms: 95319.075
timestamp: 1711037023
timesteps_total: 200000
training_iteration: 100
trial_id: default

2024-03-21 09:03:43,698 DEBUG {'env': {'coefficient_dict': {'p_d': 1.0, 'r_c': 1.0}, 'map_size': 256, 'action_space_size': 64, 'n_maps': 100, 'n_steps_per_map': 2000, 'dataset_dir': 'resource/usc_old', 'no_masking': False}, 'eval': {'evaluation_config': {'env_config': {'evaluation': True, 'n_maps': 50}, 'exploration': {'explore': False}}, 'evaluation_duration': 1, 'evaluation_interval': 5, 'evaluation_num_workers': 0}, 'explore': {'exploration_config': {'type': 'StochasticSampling'}, 'explore': True}, 'report': {'min_sample_timesteps_per_iteration': 2000}, 'resource': {'num_cpus_per_worker': 1, 'num_gpus': 0, 'num_gpus_per_worker': 0}, 'rollout': {'num_envs_per_worker': 1, 'num_rollout_workers': 0}, 'stop': {'training_iteration': 100}, 'train': {'lr': 1e-05, 'gamma': 0.1, 'grad_clip': 40.0, 'train_batch_size': 2000, 'sgd_minibatch_size': 64, 'num_sgd_iter': 30, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh'}}, 'agent': {'data_saving_interval': 10}}
2024-03-21 09:03:43,698 INFO =============TRAINING ENDED=============
2024-03-21 09:03:46,490 INFO average coverage reward for trained map 0 in 10 steps: 9453.8, optimal reward: 11878, ratio: 0.7959084020878935
2024-03-21 09:03:49,483 INFO average coverage reward for trained map 1 in 10 steps: 5199.1, optimal reward: 9139, ratio: 0.5688915636284058
2024-03-21 09:03:51,744 INFO average coverage reward for trained map 2 in 10 steps: 7793.5, optimal reward: 12468, ratio: 0.6250802053256336
2024-03-21 09:03:54,485 INFO average coverage reward for trained map 3 in 10 steps: 6117.1, optimal reward: 9921, ratio: 0.6165809898195747
2024-03-21 09:03:56,670 INFO average coverage reward for trained map 4 in 10 steps: 8334.9, optimal reward: 13679, ratio: 0.6093208567877768
2024-03-21 09:03:58,888 INFO average coverage reward for test map 0 in 10 steps: 7092.3, optimal reward: 11258, ratio: 0.6299786818262569
2024-03-21 09:04:01,191 INFO average coverage reward for test map 1 in 10 steps: 6853.0, optimal reward: 10918, ratio: 0.6276790620992856
2024-03-21 09:04:02,998 INFO average coverage reward for test map 2 in 10 steps: 7438.4, optimal reward: 9612, ratio: 0.773866000832293
2024-03-21 09:04:05,498 INFO average coverage reward for test map 3 in 10 steps: 7103.4, optimal reward: 11824, ratio: 0.6007611637347767
2024-03-21 09:04:08,121 INFO average coverage reward for test map 4 in 10 steps: 7250.9, optimal reward: 11632, ratio: 0.6233579779917469
2024-03-21 09:04:08,124 INFO overall average coverage reward for trained maps: 7379.68, average optimal reward: 11417.0, ratio: 0.6463764561618639
2024-03-21 09:04:08,124 INFO overall average coverage reward for test maps: 7147.6, average optimal reward: 11048.800000000001, ratio: 0.6469118818333213
2024-03-21 14:57:55,049 INFO ===========PPO train and eval started at 0321_1457===========
2024-03-21 17:46:53,291 INFO agent_timesteps_total: 200000
connector_metrics:
  StateBufferConnector_ms: 0.0017867088317871094
  ViewRequirementAgentConnector_ms: 0.1278235912322998
counters:
  num_agent_steps_sampled: 200000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 200000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2024-03-21_17-46-53
done: false
episode_len_mean: 200.0
episode_media: {}
episode_reward_max: 1886702.0
episode_reward_mean: 1673367.72
episode_reward_min: 1159248.0
episodes_this_iter: 1
episodes_total: 1000
evaluation:
  connector_metrics:
    StateBufferConnector_ms: 0.002002716064453125
    ViewRequirementAgentConnector_ms: 0.12404918670654295
  custom_metrics: {}
  episode_len_mean: 200.0
  episode_media: {}
  episode_reward_max: 1503061.0
  episode_reward_mean: 1503061.0
  episode_reward_min: 1503061.0
  episodes_this_iter: 1
  hist_stats:
    episode_lengths:
    - 200
    episode_reward:
    - 1503061.0
  num_agent_steps_sampled_this_iter: 200
  num_env_steps_sampled_this_iter: 200
  num_faulty_episodes: 0
  num_healthy_workers: 0
  num_in_flight_async_reqs: 0
  num_remote_worker_restarts: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.0406736338282522
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1.593275721772117
    mean_inference_ms: 2.6492643545861276
    mean_raw_obs_processing_ms: 0.4505853540184933
  sampler_results:
    connector_metrics:
      StateBufferConnector_ms: 0.002002716064453125
      ViewRequirementAgentConnector_ms: 0.12404918670654295
    custom_metrics: {}
    episode_len_mean: 200.0
    episode_media: {}
    episode_reward_max: 1503061.0
    episode_reward_mean: 1503061.0
    episode_reward_min: 1503061.0
    episodes_this_iter: 1
    hist_stats:
      episode_lengths:
      - 200
      episode_reward:
      - 1503061.0
    num_faulty_episodes: 0
    policy_reward_max: {}
    policy_reward_mean: {}
    policy_reward_min: {}
    sampler_perf:
      mean_action_processing_ms: 0.0406736338282522
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 1.593275721772117
      mean_inference_ms: 2.6492643545861276
      mean_raw_obs_processing_ms: 0.4505853540184933
  timesteps_this_iter: 200
hostname: OrangeBookPro14.lan
info:
  learner:
    __all__:
      num_agent_steps_trained: 64.0
      num_env_steps_trained: 200.0
      total_loss: 9.827460552783723
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 9.604095038986591e-29
      curr_lr: 1.0e-05
      default_optimizer_lr: 1.0000000000000003e-05
      entropy: 4.862964072126023
      gradients_default_optimizer_global_norm: 1.048992173468813
      mean_kl_loss: 0.049300512490903045
      policy_loss: -0.17253952655703464
      total_loss: 9.827460552783723
      vf_explained_var: -9.5367431640625e-07
      vf_loss: 10.0
      vf_loss_unclipped: 96301392.42553191
  num_agent_steps_sampled: 200000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 200000
  num_env_steps_trained: 0
iterations_since_restore: 1000
node_ip: 127.0.0.1
num_agent_steps_sampled: 200000
num_agent_steps_trained: 0
num_env_steps_sampled: 200000
num_env_steps_sampled_this_iter: 200
num_env_steps_sampled_throughput_per_sec: 20.419376983460676
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 95.51333333333334
  ram_util_percent: 74.98
pid: 30011
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.04294983605735094
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 1.5268913797670738
  mean_inference_ms: 2.71030614194587
  mean_raw_obs_processing_ms: 0.4563202151924994
sampler_results:
  connector_metrics:
    StateBufferConnector_ms: 0.0017867088317871094
    ViewRequirementAgentConnector_ms: 0.1278235912322998
  custom_metrics: {}
  episode_len_mean: 200.0
  episode_media: {}
  episode_reward_max: 1886702.0
  episode_reward_mean: 1673367.72
  episode_reward_min: 1159248.0
  episodes_this_iter: 1
  hist_stats:
    episode_lengths: [200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,
      200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,
      200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,
      200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,
      200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,
      200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,
      200, 200, 200, 200, 200, 200, 200, 200]
    episode_reward: [1713880.0, 1712693.0, 1714942.0, 1713592.0, 1713366.0, 1715490.0,
      1715000.0, 1159248.0, 1306047.0, 1377792.0, 1467344.0, 1494905.0, 1624701.0,
      1694098.0, 1712697.0, 1799268.0, 1855973.0, 1879203.0, 1873263.0, 1873555.0,
      1884949.0, 1883806.0, 1886702.0, 1746089.0, 1774473.0, 1871354.0, 1881490.0,
      1201384.0, 1348544.0, 1451035.0, 1579475.0, 1614971.0, 1614024.0, 1619996.0,
      1617107.0, 1613945.0, 1626603.0, 1319388.0, 1319319.0, 1321796.0, 1325169.0,
      1328103.0, 1330532.0, 1331000.0, 1330844.0, 1329827.0, 1330948.0, 1703621.0,
      1715981.0, 1715879.0, 1717988.0, 1708876.0, 1720179.0, 1725936.0, 1731424.0,
      1728994.0, 1738482.0, 1385230.0, 1504916.0, 1621736.0, 1642359.0, 1671388.0,
      1663393.0, 1660482.0, 1675011.0, 1692375.0, 1741026.0, 1503594.0, 1589956.0,
      1745661.0, 1806454.0, 1865212.0, 1872249.0, 1876843.0, 1884025.0, 1886443.0,
      1882929.0, 1592672.0, 1819433.0, 1870032.0, 1872314.0, 1871916.0, 1872170.0,
      1865747.0, 1871916.0, 1870442.0, 1877010.0, 1512384.0, 1621505.0, 1769887.0,
      1840286.0, 1875260.0, 1880034.0, 1875877.0, 1863019.0, 1877782.0, 1880182.0,
      1579615.0, 1657402.0, 1741315.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.04294983605735094
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1.5268913797670738
    mean_inference_ms: 2.71030614194587
    mean_raw_obs_processing_ms: 0.4563202151924994
time_since_restore: 10131.459293603897
time_this_iter_s: 10.739691019058228
time_total_s: 10131.459293603897
timers:
  sample_time_ms: 970.762
  synch_weights_time_ms: 26.547
  training_iteration_time_ms: 9827.558
timestamp: 1711068413
timesteps_total: 200000
training_iteration: 1000
trial_id: default

2024-03-21 17:46:53,293 DEBUG {'env': {'coefficient_dict': {'p_d': 1.0, 'r_c': 1.0}, 'map_size': 256, 'action_space_size': 64, 'n_maps': 100, 'n_episodes_per_map': 10, 'n_steps_truncate': 200, 'dataset_dir': 'resource/usc_old', 'no_masking': False}, 'eval': {'evaluation_config': {'env_config': {'evaluation': True, 'n_maps': 50, 'n_episodes_per_map': 1}, 'exploration': {'explore': False}}, 'evaluation_duration': 1, 'evaluation_interval': 5, 'evaluation_num_workers': 0}, 'explore': {'exploration_config': {'type': 'StochasticSampling'}, 'explore': True}, 'report': {'min_sample_timesteps_per_iteration': 200}, 'resource': {'num_cpus_per_worker': 1, 'num_gpus': 0, 'num_gpus_per_worker': 0}, 'rollout': {'num_envs_per_worker': 1, 'num_rollout_workers': 0}, 'stop': {'training_iteration': 1000}, 'train': {'lr': 1e-05, 'gamma': 0.1, 'grad_clip': 40.0, 'train_batch_size': 200, 'sgd_minibatch_size': 64, 'num_sgd_iter': 30, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh'}}, 'agent': {'data_saving_interval': 10}}
2024-03-21 17:46:53,293 INFO =============TRAINING ENDED=============
2024-03-21 18:09:09,938 INFO average coverage reward for trained map 0 in 10 steps: 9383.9, optimal reward: 10826, ratio: 0.8667929059671162
2024-03-21 18:09:11,102 INFO average coverage reward for trained map 1 in 10 steps: 8472.6, optimal reward: 13206, ratio: 0.6415720127214902
2024-03-21 18:09:12,315 INFO average coverage reward for trained map 2 in 10 steps: 6384.8, optimal reward: 12468, ratio: 0.5120949631055502
2024-03-21 18:09:13,616 INFO average coverage reward for trained map 3 in 10 steps: 8476.6, optimal reward: 8795, ratio: 0.9637976122797044
2024-03-21 18:09:14,855 INFO average coverage reward for trained map 4 in 10 steps: 6915.6, optimal reward: 10790, ratio: 0.6409267840593142
2024-03-21 18:09:15,908 INFO average coverage reward for test map 0 in 10 steps: 7707.7, optimal reward: 10103, ratio: 0.7629120063347521
2024-03-21 18:09:17,052 INFO average coverage reward for test map 1 in 10 steps: 7738.5, optimal reward: 11258, ratio: 0.6873778646295967
2024-03-21 18:09:18,247 INFO average coverage reward for test map 2 in 10 steps: 7433.8, optimal reward: 11977, ratio: 0.6206729564999582
2024-03-21 18:09:19,477 INFO average coverage reward for test map 3 in 10 steps: 6349.4, optimal reward: 10868, ratio: 0.584228928965771
2024-03-21 18:09:20,688 INFO average coverage reward for test map 4 in 10 steps: 7281.6, optimal reward: 10814, ratio: 0.6733493619382283
2024-03-21 18:09:20,690 INFO overall average coverage reward for trained maps: 7926.7, average optimal reward: 11217.0, ratio: 0.7066684496746011
2024-03-21 18:09:20,690 INFO overall average coverage reward for test maps: 7302.200000000001, average optimal reward: 11004.0, ratio: 0.663595056343148
2024-03-21 20:24:15,147 INFO ===========PPO train and eval started at 0321_2024===========
2024-03-21 22:45:20,747 INFO ===========PPO train and eval started at 0321_2245===========
2024-03-22 01:34:40,940 INFO agent_timesteps_total: 200000
connector_metrics:
  StateBufferConnector_ms: 0.0017402172088623047
  ViewRequirementAgentConnector_ms: 0.13006901741027832
counters:
  num_agent_steps_sampled: 200000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 200000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2024-03-22_01-34-40
done: false
episode_len_mean: 200.0
episode_media: {}
episode_reward_max: 2065200.0
episode_reward_mean: 1672330.96
episode_reward_min: 1120483.0
episodes_this_iter: 1
episodes_total: 1000
evaluation:
  connector_metrics:
    StateBufferConnector_ms: 0.0016927719116210938
    ViewRequirementAgentConnector_ms: 0.12521743774414062
  custom_metrics: {}
  episode_len_mean: 200.0
  episode_media: {}
  episode_reward_max: 1381390.0
  episode_reward_mean: 1381390.0
  episode_reward_min: 1381390.0
  episodes_this_iter: 1
  hist_stats:
    episode_lengths:
    - 200
    episode_reward:
    - 1381390.0
  num_agent_steps_sampled_this_iter: 200
  num_env_steps_sampled_this_iter: 200
  num_faulty_episodes: 0
  num_healthy_workers: 0
  num_in_flight_async_reqs: 0
  num_remote_worker_restarts: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.041887535518063464
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1.6119933236238644
    mean_inference_ms: 2.696079086212542
    mean_raw_obs_processing_ms: 0.4580206508764741
  sampler_results:
    connector_metrics:
      StateBufferConnector_ms: 0.0016927719116210938
      ViewRequirementAgentConnector_ms: 0.12521743774414062
    custom_metrics: {}
    episode_len_mean: 200.0
    episode_media: {}
    episode_reward_max: 1381390.0
    episode_reward_mean: 1381390.0
    episode_reward_min: 1381390.0
    episodes_this_iter: 1
    hist_stats:
      episode_lengths:
      - 200
      episode_reward:
      - 1381390.0
    num_faulty_episodes: 0
    policy_reward_max: {}
    policy_reward_mean: {}
    policy_reward_min: {}
    sampler_perf:
      mean_action_processing_ms: 0.041887535518063464
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 1.6119933236238644
      mean_inference_ms: 2.696079086212542
      mean_raw_obs_processing_ms: 0.4580206508764741
  timesteps_this_iter: 200
hostname: OrangeBookPro14.lan
info:
  learner:
    __all__:
      num_agent_steps_trained: 64.0
      num_env_steps_trained: 200.0
      total_loss: 9.819954232966646
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 6.223453103780825e-26
      curr_lr: 1.0e-05
      default_optimizer_lr: 1.0000000000000003e-05
      entropy: 5.59208789277584
      gradients_default_optimizer_global_norm: 1.0667243612573503
      mean_kl_loss: 0.03885914539383288
      policy_loss: -0.1800457608509571
      total_loss: 9.819954232966646
      vf_explained_var: -9.5367431640625e-07
      vf_loss: 10.0
      vf_loss_unclipped: 65546857.70212766
  num_agent_steps_sampled: 200000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 200000
  num_env_steps_trained: 0
iterations_since_restore: 1000
node_ip: 127.0.0.1
num_agent_steps_sampled: 200000
num_agent_steps_trained: 0
num_env_steps_sampled: 200000
num_env_steps_sampled_this_iter: 200
num_env_steps_sampled_throughput_per_sec: 20.43827219359225
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 96.07999999999998
  ram_util_percent: 72.64666666666668
pid: 33582
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.04326030910186657
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 1.5232836040181568
  mean_inference_ms: 2.7227909426513968
  mean_raw_obs_processing_ms: 0.4566269735134679
sampler_results:
  connector_metrics:
    StateBufferConnector_ms: 0.0017402172088623047
    ViewRequirementAgentConnector_ms: 0.13006901741027832
  custom_metrics: {}
  episode_len_mean: 200.0
  episode_media: {}
  episode_reward_max: 2065200.0
  episode_reward_mean: 1672330.96
  episode_reward_min: 1120483.0
  episodes_this_iter: 1
  hist_stats:
    episode_lengths: [200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,
      200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,
      200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,
      200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,
      200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,
      200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,
      200, 200, 200, 200, 200, 200, 200, 200]
    episode_reward: [2065200.0, 2065200.0, 2065200.0, 2065200.0, 2065200.0, 2065200.0,
      2065200.0, 1421106.0, 1466417.0, 1534478.0, 1711585.0, 1826730.0, 1977586.0,
      2020415.0, 2002118.0, 2011533.0, 2029322.0, 1632838.0, 1688240.0, 1812296.0,
      1858962.0, 1871321.0, 1929041.0, 1900125.0, 1959615.0, 1948261.0, 1967561.0,
      1120483.0, 1281794.0, 1402410.0, 1436227.0, 1450092.0, 1475302.0, 1465911.0,
      1493616.0, 1508820.0, 1507998.0, 1646272.0, 1685600.0, 1293583.0, 1398318.0,
      1438827.0, 1472465.0, 1551045.0, 1615717.0, 1686747.0, 1660227.0, 1673769.0,
      1690126.0, 1700564.0, 1705306.0, 1701964.0, 1721730.0, 1704827.0, 1731259.0,
      1731922.0, 1761593.0, 1393099.0, 1598047.0, 1603264.0, 1609072.0, 1609252.0,
      1606167.0, 1604582.0, 1604599.0, 1602932.0, 1608968.0, 1257841.0, 1306071.0,
      1401176.0, 1490977.0, 1532343.0, 1549097.0, 1565511.0, 1593828.0, 1609838.0,
      1642601.0, 1832391.0, 1855711.0, 1857748.0, 1855005.0, 1858549.0, 1860428.0,
      1851581.0, 1857983.0, 1858688.0, 1859937.0, 1453688.0, 1475224.0, 1497735.0,
      1510502.0, 1549071.0, 1545385.0, 1570595.0, 1637704.0, 1668075.0, 1689260.0,
      1472718.0, 1503757.0, 1581632.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.04326030910186657
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1.5232836040181568
    mean_inference_ms: 2.7227909426513968
    mean_raw_obs_processing_ms: 0.4566269735134679
time_since_restore: 10153.607854127884
time_this_iter_s: 10.753561019897461
time_total_s: 10153.607854127884
timers:
  sample_time_ms: 978.839
  synch_weights_time_ms: 27.493
  training_iteration_time_ms: 9795.334
timestamp: 1711096480
timesteps_total: 200000
training_iteration: 1000
trial_id: default

2024-03-22 01:34:40,941 DEBUG {'env': {'coefficient_dict': {'p_d': 1.0, 'r_c': 1.0}, 'map_size': 256, 'action_space_size': 64, 'n_maps': 100, 'n_episodes_per_map': 10, 'n_steps_truncate': 200, 'dataset_dir': 'resource/usc_old', 'no_masking': False}, 'eval': {'evaluation_config': {'env_config': {'evaluation': True, 'n_maps': 50, 'n_episodes_per_map': 1}, 'exploration': {'explore': False}}, 'evaluation_duration': 1, 'evaluation_interval': 5, 'evaluation_num_workers': 0}, 'explore': {'exploration_config': {'type': 'StochasticSampling'}, 'explore': True}, 'report': {'min_sample_timesteps_per_iteration': 200}, 'resource': {'num_cpus_per_worker': 1, 'num_gpus': 0, 'num_gpus_per_worker': 0}, 'rollout': {'num_envs_per_worker': 1, 'num_rollout_workers': 0}, 'stop': {'training_iteration': 1000}, 'train': {'lr': 1e-05, 'gamma': 0.0, 'grad_clip': 40.0, 'train_batch_size': 200, 'sgd_minibatch_size': 64, 'num_sgd_iter': 30, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh'}}, 'agent': {'data_saving_interval': 50}}
2024-03-22 01:34:40,941 INFO =============TRAINING ENDED=============
2024-03-22 01:34:43,963 INFO average coverage reward for trained map 0 in 10 steps: 7915.6, optimal reward: 10826, ratio: 0.731165712174395
2024-03-22 01:34:45,197 INFO average coverage reward for trained map 1 in 10 steps: 8954.0, optimal reward: 13206, ratio: 0.6780251400878389
2024-03-22 01:34:46,450 INFO average coverage reward for trained map 2 in 10 steps: 7612.4, optimal reward: 12468, ratio: 0.6105550208533846
2024-03-22 01:34:47,748 INFO average coverage reward for trained map 3 in 10 steps: 8559.3, optimal reward: 8795, ratio: 0.9732006822057987
2024-03-22 01:34:49,015 INFO average coverage reward for trained map 4 in 10 steps: 8630.8, optimal reward: 10790, ratio: 0.7998887859128823
2024-03-22 01:34:50,185 INFO average coverage reward for test map 0 in 10 steps: 6015.1, optimal reward: 10103, ratio: 0.5953776106107097
2024-03-22 01:34:51,403 INFO average coverage reward for test map 1 in 10 steps: 6894.0, optimal reward: 11258, ratio: 0.6123645407710073
2024-03-22 01:34:52,653 INFO average coverage reward for test map 2 in 10 steps: 6988.9, optimal reward: 11977, ratio: 0.58352675962261
2024-03-22 01:34:53,931 INFO average coverage reward for test map 3 in 10 steps: 7389.6, optimal reward: 10868, ratio: 0.6799411115200589
2024-03-22 01:34:55,242 INFO average coverage reward for test map 4 in 10 steps: 5472.3, optimal reward: 10814, ratio: 0.5060384686517477
2024-03-22 01:34:55,245 INFO overall average coverage reward for trained maps: 8334.419999999998, average optimal reward: 11217.0, ratio: 0.7430168494249798
2024-03-22 01:34:55,245 INFO overall average coverage reward for test maps: 6551.98, average optimal reward: 11004.0, ratio: 0.5954180298073427
2024-03-23 20:22:59,877 INFO ===========SAC train and eval started at 0323_2022===========
2024-03-23 20:27:27,609 INFO ===========SAC train and eval started at 0323_2027===========
2024-03-23 20:32:31,902 INFO ===========SAC train and eval started at 0323_2032===========
2024-03-23 20:36:26,586 INFO ===========SAC train and eval started at 0323_2036===========
2024-03-23 20:39:46,864 INFO ===========SAC train and eval started at 0323_2039===========
2024-03-23 20:40:16,952 INFO ===========SAC train and eval started at 0323_2040===========
2024-03-24 02:23:11,976 INFO ===========SAC train and eval started at 0324_0223===========
2024-03-24 02:26:59,818 INFO ===========SAC train and eval started at 0324_0226===========
2024-03-25 22:35:51,730 INFO ===========PPO train and eval started at 0325_2235===========
