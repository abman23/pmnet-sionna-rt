2024-02-15 23:02:22,514 INFO =============A WHOLE TRAINING PERIOD ENDED=============
2024-02-15 23:02:22,522 INFO agent_timesteps_total: 100000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.032321929931640625
  StateBufferConnector_ms: 0.0030863285064697266
  ViewRequirementAgentConnector_ms: 0.08814215660095215
counters:
  last_target_update_ts: 99501
  num_agent_steps_sampled: 100000
  num_agent_steps_trained: 2880000
  num_env_steps_sampled: 100000
  num_env_steps_trained: 2880000
  num_target_updates: 180
custom_metrics: {}
date: 2024-02-15_23-02-18
done: false
episode_len_mean: 96.04
episode_media: {}
episode_reward_max: 1935.583556256892
episode_reward_mean: -3155.0365932275063
episode_reward_min: -17991.152121860694
episodes_this_iter: 11
episodes_total: 1005
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.024358431498209637
    StateBufferConnector_ms: 0.0041484832763671875
    ViewRequirementAgentConnector_ms: 0.08197625478108723
  custom_metrics: {}
  episode_len_mean: 100.0
  episode_media: {}
  episode_reward_max: -7408.257076010191
  episode_reward_mean: -8966.638439563143
  episode_reward_min: -10261.093570132763
  episodes_this_iter: 3
  hist_stats:
    episode_lengths:
    - 100
    - 100
    - 100
    episode_reward:
    - -9230.564672546474
    - -10261.093570132763
    - -7408.257076010191
  num_agent_steps_sampled_this_iter: 300
  num_env_steps_sampled_this_iter: 300
  num_faulty_episodes: 0
  num_healthy_workers: 0
  num_in_flight_async_reqs: 0
  num_remote_worker_restarts: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.03973693520601738
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 11.630257329406055
    mean_inference_ms: 0.6656130745620449
    mean_raw_obs_processing_ms: 0.15927913952888223
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.024358431498209637
      StateBufferConnector_ms: 0.0041484832763671875
      ViewRequirementAgentConnector_ms: 0.08197625478108723
    custom_metrics: {}
    episode_len_mean: 100.0
    episode_media: {}
    episode_reward_max: -7408.257076010191
    episode_reward_mean: -8966.638439563143
    episode_reward_min: -10261.093570132763
    episodes_this_iter: 3
    hist_stats:
      episode_lengths:
      - 100
      - 100
      - 100
      episode_reward:
      - -9230.564672546474
      - -10261.093570132763
      - -7408.257076010191
    num_faulty_episodes: 0
    policy_reward_max: {}
    policy_reward_mean: {}
    policy_reward_min: {}
    sampler_perf:
      mean_action_processing_ms: 0.03973693520601738
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 11.630257329406055
      mean_inference_ms: 0.6656130745620449
      mean_raw_obs_processing_ms: 0.15927913952888223
  timesteps_this_iter: 300
hostname: OrangeBookPro14.lan
info:
  last_target_update_ts: 99501
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 89999.0
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 0.0005
        grad_gnorm: 0.9226540923118591
        max_q: 540.774658203125
        mean_q: -1492.714111328125
        min_q: -3200.27587890625
      mean_td_error: -4.221076965332031
      model: {}
      num_agent_steps_trained: 32.0
      num_grad_updates_lifetime: 90000.0
      td_error: [-15.0657958984375, -31.2978515625, -22.976806640625, 4.023193359375,
        -3.58660888671875, 12.4736328125, -18.964111328125, 14.3331298828125, 3.41339111328125,
        -23.5263671875, 13.2242431640625, -21.0455322265625, 5.8837890625, -6.58660888671875,
        -17.65380859375, 6.00634765625, -22.976806640625, -23.9912109375, -5.6083984375,
        9.62939453125, 10.5635986328125, 1.41339111328125, -26.37060546875, -21.0455322265625,
        9.62939453125, -6.58660888671875, 11.5635986328125, 5.02215576171875, -1.380859375,
        12.2242431640625, 2.7939453125, 11.3916015625]
  num_agent_steps_sampled: 100000
  num_agent_steps_trained: 2880000
  num_env_steps_sampled: 100000
  num_env_steps_trained: 2880000
  num_target_updates: 180
iterations_since_restore: 100
node_ip: 127.0.0.1
num_agent_steps_sampled: 100000
num_agent_steps_trained: 2880000
num_env_steps_sampled: 100000
num_env_steps_sampled_this_iter: 1000
num_env_steps_sampled_throughput_per_sec: 22.68818253469561
num_env_steps_trained: 2880000
num_env_steps_trained_this_iter: 32000
num_env_steps_trained_throughput_per_sec: 726.0218411102595
num_faulty_episodes: 0
num_healthy_workers: 1
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 32000
perf:
  cpu_util_percent: 40.20540540540541
  ram_util_percent: 83.2554054054054
pid: 7251
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.04726314337241336
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 11.407527977633352
  mean_inference_ms: 0.8474313479946306
  mean_raw_obs_processing_ms: 0.3752429247143253
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.032321929931640625
    StateBufferConnector_ms: 0.0030863285064697266
    ViewRequirementAgentConnector_ms: 0.08814215660095215
  custom_metrics: {}
  episode_len_mean: 96.04
  episode_media: {}
  episode_reward_max: 1935.583556256892
  episode_reward_mean: -3155.0365932275063
  episode_reward_min: -17991.152121860694
  episodes_this_iter: 11
  hist_stats:
    episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,
      100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,
      100, 1, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 1, 100, 100,
      100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,
      100, 100, 100, 100, 100, 100, 100, 100, 100, 1, 100, 100, 100, 100, 100, 100,
      100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 1,
      100, 100, 100, 100, 100, 100, 100, 100]
    episode_reward: [-813.8365594638165, -587.5864289953989, -1948.838224039047, -2282.3195537023626,
      -960.4741933915535, -2475.2592735924736, -2200.0, -1627.421103189291, -13827.211075676463,
      1852.2965926011464, -1217.5655567819658, -9786.165372506635, -10813.836857845137,
      -3692.0135110466435, -123.97057550292607, -2806.933952676976, -11726.031122289158,
      402.10041333561327, 1058.9772361378984, 1935.583556256892, -1289.0192236625153,
      910.4508353989229, 495.7237427591831, -453.4611017734121, -1091.372780160811,
      -4070.0619640402297, -2424.8800651012307, -1784.5155527772718, -821.4181860508214,
      0.0, -1247.0, -12401.523106658586, -13611.090301090695, -13687.904542744658,
      273.8258419634338, -3031.83811817302, -3713.1002210077004, -1462.3024419719393,
      -11343.533662935595, -10692.514563749397, -9318.122111509869, 0.0, -7194.843341992451,
      -2406.370729772476, -768.9146834366135, -669.4048218260818, -366.05386865369076,
      -7644.227135109605, -982.422205101856, -513.8164172549574, -552.2843926894398,
      1357.3314574584886, -2159.152261451566, -2323.6067977499756, 44.24718477629855,
      -2221.3238203898172, -11172.828676578194, -400.0, -847.8016366881542, -2713.43795125691,
      -1727.5116061352555, 917.7753217102429, -17991.152121860694, -4056.8648240578846,
      456.35668302290674, -9682.88567787557, -3822.5756827100126, -541.2415402771893,
      -1430.4617267848591, 0.0, -921.4138126514911, 1492.8137708232355, -12623.35599042031,
      -483.70709666459516, -1535.9444101084885, -1574.8788386129256, -1368.3837019329912,
      382.10241988685897, -1045.8658273404442, -8097.7890872028765, -1400.0, 1192.9767943850218,
      -3228.049843258692, -10389.832546818256, -3109.31791638523, -2814.3016992778116,
      -1000.0, 211.51950113721756, -844.210472425239, -11129.02424933873, -1572.2138807121016,
      0.0, -1248.6677883132581, -6966.398791357788, -1013.9412119919391, -2265.073972741362,
      -10111.16866266934, -1649.4181967970446, 727.520546377744, -1300.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.04726314337241336
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 11.407527977633352
    mean_inference_ms: 0.8474313479946306
    mean_raw_obs_processing_ms: 0.3752429247143253
time_since_restore: 4005.8324122428894
time_this_iter_s: 48.08819007873535
time_total_s: 4005.8324122428894
timers:
  learn_throughput: 2096.107
  learn_time_ms: 15.266
  load_throughput: 445757.981
  load_time_ms: 0.072
  sample_time_ms: 22.377
  synch_weights_time_ms: 1.269
  training_iteration_time_ms: 44.138
timestamp: 1708066938
timesteps_total: 100000
training_iteration: 100
trial_id: default

2024-02-15 23:02:22,522 DEBUG {'env': {'cropped_map_size': 64, 'ratio_coverage': 0.0125, 'original_map_path': '/Users/ylu/Documents/USC/WiDeS/BS_Deployment/resource/usc.png', 'original_map_scale': 3.4375, 'n_maps': 10, 'n_steps_per_map': 100, 'no_masking': True, 'coefficient_dict': {'r_c': 1.0, 'p_d': 1.0, 'p_b': 1.0}}, 'explore': {'explore': True, 'exploration_config': {'type': 'EpsilonGreedy', 'initial_epsilon': 1.0, 'final_epsilon': 0.02, 'epsilon_timesteps': 20000}}, 'train': {'train_batch_size': 32, 'lr': 0.0005, 'num_steps_sampled_before_learning_starts': 10000, 'replay_buffer_config': {'capacity': 50000}}, 'eval': {'evaluation_interval': 1, 'evaluation_duration': 3, 'evaluation_config': {'explore': True}}, 'stop': {'training_iteration': 20}}
2024-02-16 01:51:02,429 INFO =============A WHOLE TRAINING PERIOD ENDED=============
2024-02-16 01:51:02,437 INFO agent_timesteps_total: 100000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.029901981353759766
  StateBufferConnector_ms: 0.003033876419067383
  ViewRequirementAgentConnector_ms: 0.08017230033874512
counters:
  last_target_update_ts: 99501
  num_agent_steps_sampled: 100000
  num_agent_steps_trained: 2880000
  num_env_steps_sampled: 100000
  num_env_steps_trained: 2880000
  num_target_updates: 180
custom_metrics: {}
date: 2024-02-16_01-51-02
done: false
episode_len_mean: 93.94
episode_media: {}
episode_reward_max: 1995.6779708278077
episode_reward_mean: -4138.661629699824
episode_reward_min: -20263.26377021602
episodes_this_iter: 10
episodes_total: 1051
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.014948844909667969
    StateBufferConnector_ms: 0.002495447794596354
    ViewRequirementAgentConnector_ms: 0.04967848459879557
  custom_metrics: {}
  episode_len_mean: 100.0
  episode_media: {}
  episode_reward_max: -11714.962686336246
  episode_reward_mean: -13001.499450211448
  episode_reward_min: -14674.57297796185
  episodes_this_iter: 3
  hist_stats:
    episode_lengths:
    - 100
    - 100
    - 100
    episode_reward:
    - -14674.57297796185
    - -11714.962686336246
    - -12614.962686336243
  num_agent_steps_sampled_this_iter: 300
  num_env_steps_sampled_this_iter: 300
  num_faulty_episodes: 0
  num_healthy_workers: 0
  num_in_flight_async_reqs: 0
  num_remote_worker_restarts: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.037972193821871794
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 11.444946046010807
    mean_inference_ms: 0.5664547373185909
    mean_raw_obs_processing_ms: 0.15558166061416118
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.014948844909667969
      StateBufferConnector_ms: 0.002495447794596354
      ViewRequirementAgentConnector_ms: 0.04967848459879557
    custom_metrics: {}
    episode_len_mean: 100.0
    episode_media: {}
    episode_reward_max: -11714.962686336246
    episode_reward_mean: -13001.499450211448
    episode_reward_min: -14674.57297796185
    episodes_this_iter: 3
    hist_stats:
      episode_lengths:
      - 100
      - 100
      - 100
      episode_reward:
      - -14674.57297796185
      - -11714.962686336246
      - -12614.962686336243
    num_faulty_episodes: 0
    policy_reward_max: {}
    policy_reward_mean: {}
    policy_reward_min: {}
    sampler_perf:
      mean_action_processing_ms: 0.037972193821871794
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 11.444946046010807
      mean_inference_ms: 0.5664547373185909
      mean_raw_obs_processing_ms: 0.15558166061416118
  timesteps_this_iter: 300
hostname: OrangeBookPro14.lan
info:
  last_target_update_ts: 99501
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 89999.0
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 0.001
        grad_gnorm: 4.6135149002075195
        max_q: 479.9879150390625
        mean_q: -1391.132568359375
        min_q: -2856.21240234375
      mean_td_error: 17.60885238647461
      model: {}
      num_agent_steps_trained: 32.0
      num_grad_updates_lifetime: 90000.0
      td_error: [479.9879150390625, 4.927734375, 2.708740234375, -13.3199462890625,
        -6.26708984375, 9.4786376953125, -13.5455322265625, 2.125244140625, 6.927734375,
        9.6800537109375, 11.4544677734375, 4.35302734375, -4.072265625, -2.0201416015625,
        15.35302734375, 13.35302734375, 6.18701171875, -18.072265625, 18.549911499023438,
        0.35302734375, 6.927734375, -10.4959716796875, 5.350341796875, 9.73291015625,
        5.5040283203125, 6.884429931640625, -6.81298828125, 11.708740234375, -2.81298828125,
        13.708740234375, -3.3199462890625, -1.0340576171875]
  num_agent_steps_sampled: 100000
  num_agent_steps_trained: 2880000
  num_env_steps_sampled: 100000
  num_env_steps_trained: 2880000
  num_target_updates: 180
iterations_since_restore: 100
node_ip: 127.0.0.1
num_agent_steps_sampled: 100000
num_agent_steps_trained: 2880000
num_env_steps_sampled: 100000
num_env_steps_sampled_this_iter: 1000
num_env_steps_sampled_throughput_per_sec: 25.021078313013135
num_env_steps_trained: 2880000
num_env_steps_trained_this_iter: 32000
num_env_steps_trained_throughput_per_sec: 800.6745060164203
num_faulty_episodes: 0
num_healthy_workers: 1
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 32000
perf:
  cpu_util_percent: 27.070967741935487
  ram_util_percent: 79.63870967741934
pid: 10265
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.04780288993477209
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 11.327375230193566
  mean_inference_ms: 0.8378122062309282
  mean_raw_obs_processing_ms: 0.37403535248454206
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.029901981353759766
    StateBufferConnector_ms: 0.003033876419067383
    ViewRequirementAgentConnector_ms: 0.08017230033874512
  custom_metrics: {}
  episode_len_mean: 93.94
  episode_media: {}
  episode_reward_max: 1995.6779708278077
  episode_reward_mean: -4138.661629699824
  episode_reward_min: -20263.26377021602
  episodes_this_iter: 10
  hist_stats:
    episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,
      100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,
      100, 83, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 1, 100, 100, 100,
      100, 100, 100, 100, 100, 100, 100, 100, 100, 1, 100, 69, 100, 100, 100, 100,
      100, 100, 100, 100, 100, 100, 100, 100, 1, 100, 100, 100, 100, 87, 100, 100,
      100, 100, 100, 1, 100, 100, 100, 50, 100, 100, 100, 100, 100, 1, 100, 100, 100,
      100, 100, 100, 100, 100, 100, 100]
    episode_reward: [-1498.40807723397, 566.6701108594716, -11643.08900261508, -1240.2788205960996,
      -5594.443448453516, -9743.914450307566, -2230.7209352590003, -20263.26377021602,
      861.4178734853, -17444.87197810573, -812.3105625617656, -6537.979280256627,
      -2151.1101240737285, 777.7650591923673, -2735.3326557104247, -2050.7330425722776,
      -186.43196883742382, 129.35590061056192, -16960.070487242538, 1179.3772233983161,
      -11017.39871758713, -6625.517509904362, -1223.3820239269676, -16265.504168792768,
      -11387.34524147553, -9003.187624186583, -699.3859416799853, -1635.4170796804428,
      1387.3088755824253, -10649.041630560343, -6204.712787367176, -139.83968197564428,
      -2984.3333000606776, -3688.225562203244, 113.0, -17041.485711775833, 842.6821789367236,
      1287.5721220608757, -10492.958129269693, -9069.879460166474, 0.0, -1980.3709184918262,
      -3644.4399785546366, -9509.08622126092, -812.3105625617656, -9514.458189518029,
      -2308.1079596193526, -2253.580314369845, 1955.1836142850473, -1451.9087900089298,
      -900.0, -14674.805485862318, -1256.451813560248, 0.0, -8534.428625386576, -204.0,
      545.1157064882743, -1890.7190226353987, 766.7704842518746, -10818.90182232756,
      -2420.481879299133, -2308.1403061621504, -1100.0, -5987.997801582678, -812.3105625617656,
      -1391.6960846136437, -15375.89957965053, -274.6945805858402, 0.0, -1570.0517141775317,
      -17015.993137044316, -13100.109044864943, -4327.791321555271, -10740.864082984535,
      -941.3648849284539, -1758.3274426822468, -1860.559139368529, -14041.648783894747,
      1086.0326838493943, 0.0, -1338.0388270422784, -3531.9686262424984, -5155.088148699576,
      -4694.702234119423, -1650.5069374239276, -2500.0, -3361.8719296916643, -56.56722356464127,
      667.5848659814319, 0.0, -1300.0, -3101.388621073826, -1541.4213562373104, -406.0490751474588,
      -2550.944271909999, -10574.926601240804, -1956.9683270018754, 306.41319661818517,
      1995.6779708278077, -611.5754322468125]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.04780288993477209
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 11.327375230193566
    mean_inference_ms: 0.8378122062309282
    mean_raw_obs_processing_ms: 0.37403535248454206
time_since_restore: 3990.585314512253
time_this_iter_s: 43.729389905929565
time_total_s: 3990.585314512253
timers:
  learn_throughput: 2549.032
  learn_time_ms: 12.554
  load_throughput: 523265.996
  load_time_ms: 0.061
  sample_time_ms: 19.121
  synch_weights_time_ms: 1.159
  training_iteration_time_ms: 37.303
timestamp: 1708077062
timesteps_total: 100000
training_iteration: 100
trial_id: default

2024-02-16 01:51:02,437 DEBUG {'env': {'cropped_map_size': 64, 'ratio_coverage': 0.0125, 'original_map_path': '/Users/ylu/Documents/USC/WiDeS/BS_Deployment/resource/usc.png', 'original_map_scale': 3.4375, 'n_maps': 10, 'n_steps_per_map': 100, 'no_masking': True, 'coefficient_dict': {'r_c': 1.0, 'p_d': 1.0, 'p_b': 1.0}}, 'explore': {'explore': True, 'exploration_config': {'type': 'EpsilonGreedy', 'initial_epsilon': 1.0, 'final_epsilon': 0.02, 'epsilon_timesteps': 20000}}, 'train': {'train_batch_size': 32, 'lr': 0.001, 'num_steps_sampled_before_learning_starts': 10000, 'replay_buffer_config': {'capacity': 20000}}, 'eval': {'evaluation_interval': 1, 'evaluation_duration': 3, 'evaluation_config': {'explore': False}}, 'stop': {'training_iteration': 20}}
2024-02-19 15:28:36,772 INFO =============A WHOLE TRAINING PERIOD ENDED=============
2024-02-19 15:28:36,781 INFO agent_timesteps_total: 1000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.009740829467773438
  StateBufferConnector_ms: 0.005631685256958008
  ViewRequirementAgentConnector_ms: 0.15839791297912598
counters:
  last_target_update_ts: 1000
  num_agent_steps_sampled: 1000
  num_agent_steps_trained: 1800
  num_env_steps_sampled: 1000
  num_env_steps_trained: 1800
  num_target_updates: 2
custom_metrics: {}
date: 2024-02-19_15-28-36
done: false
episode_len_mean: 9.91
episode_media: {}
episode_reward_max: 284.55996254682475
episode_reward_mean: -1846.8268655649244
episode_reward_min: -4415.106118490523
episodes_this_iter: 11
episodes_total: 100
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0043074289957682295
    StateBufferConnector_ms: 0.002495447794596354
    ViewRequirementAgentConnector_ms: 0.048820177714029946
  custom_metrics: {}
  episode_len_mean: 10.0
  episode_media: {}
  episode_reward_max: 437.51543268683406
  episode_reward_mean: -721.6508790548096
  episode_reward_min: -1676.0232526704265
  episodes_this_iter: 3
  hist_stats:
    episode_lengths:
    - 10
    - 10
    - 10
    episode_reward:
    - -926.4448171808364
    - -1676.0232526704265
    - 437.51543268683406
  num_agent_steps_sampled_this_iter: 30
  num_env_steps_sampled_this_iter: 30
  num_faulty_episodes: 0
  num_healthy_workers: 0
  num_in_flight_async_reqs: 0
  num_remote_worker_restarts: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.04458818279328893
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 9.03888608588547
    mean_inference_ms: 0.5863220965275998
    mean_raw_obs_processing_ms: 0.20906964286428983
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.0043074289957682295
      StateBufferConnector_ms: 0.002495447794596354
      ViewRequirementAgentConnector_ms: 0.048820177714029946
    custom_metrics: {}
    episode_len_mean: 10.0
    episode_media: {}
    episode_reward_max: 437.51543268683406
    episode_reward_mean: -721.6508790548096
    episode_reward_min: -1676.0232526704265
    episodes_this_iter: 3
    hist_stats:
      episode_lengths:
      - 10
      - 10
      - 10
      episode_reward:
      - -926.4448171808364
      - -1676.0232526704265
      - 437.51543268683406
    num_faulty_episodes: 0
    policy_reward_max: {}
    policy_reward_mean: {}
    policy_reward_min: {}
    sampler_perf:
      mean_action_processing_ms: 0.04458818279328893
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 9.03888608588547
      mean_inference_ms: 0.5863220965275998
      mean_raw_obs_processing_ms: 0.20906964286428983
  timesteps_this_iter: 30
hostname: OrangeBook-Pro-14.local
info:
  last_target_update_ts: 1000
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 224.0
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 0.0005
        grad_gnorm: 40.0
        max_q: -284.6845703125
        mean_q: -404.7083740234375
        min_q: -553.0948486328125
      mean_td_error: 58.745399475097656
      model: {}
      num_agent_steps_trained: 8.0
      num_grad_updates_lifetime: 225.0
      td_error: [-68.09562683105469, 181.0797119140625, 87.98211669921875, 67.09811401367188,
        226.50006103515625, -59.94775390625, 39.4996337890625, -4.153076171875]
  num_agent_steps_sampled: 1000
  num_agent_steps_trained: 1800
  num_env_steps_sampled: 1000
  num_env_steps_trained: 1800
  num_target_updates: 2
iterations_since_restore: 10
node_ip: 127.0.0.1
num_agent_steps_sampled: 1000
num_agent_steps_trained: 1800
num_env_steps_sampled: 1000
num_env_steps_sampled_this_iter: 100
num_env_steps_sampled_throughput_per_sec: 65.93087442933607
num_env_steps_trained: 1800
num_env_steps_trained_this_iter: 200
num_env_steps_trained_throughput_per_sec: 131.86174885867214
num_faulty_episodes: 0
num_healthy_workers: 4
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 200
perf:
  cpu_util_percent: 75.1
  ram_util_percent: 83.19999999999999
pid: 58089
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.07919362498169939
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 10.411826971483048
  mean_inference_ms: 1.5176445765721396
  mean_raw_obs_processing_ms: 0.6773188303380919
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.009740829467773438
    StateBufferConnector_ms: 0.005631685256958008
    ViewRequirementAgentConnector_ms: 0.15839791297912598
  custom_metrics: {}
  episode_len_mean: 9.91
  episode_media: {}
  episode_reward_max: 284.55996254682475
  episode_reward_mean: -1846.8268655649244
  episode_reward_min: -4415.106118490523
  episodes_this_iter: 11
  hist_stats:
    episode_lengths: [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,
      10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 1, 10, 10, 10, 10, 10, 10, 10, 10, 10,
      10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,
      10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,
      10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,
      10, 10, 10, 10, 10, 10, 10, 10]
    episode_reward: [-3046.669749661072, -3358.794578571302, -2295.2233822837948,
      -3430.0406382749015, -3988.712229470671, -3028.9167844851577, -2889.2409084622464,
      -2477.1261800186535, -2660.2120421118484, -3973.561963473473, -4037.250863593415,
      -4299.346050527976, -2451.242006109177, -4415.106118490523, -1899.2839325627926,
      -2801.8274620102893, -3878.2867090388672, -1154.1539181195028, -2259.9504638788594,
      -3907.306006776409, -1241.6759010503556, -2889.1182458896565, -2202.2599654825453,
      -3879.1602409679663, -2717.7017570696153, 0.0, -2497.509524646053, -2426.8829897381515,
      -3678.755826491786, -1080.0, -1392.462112512353, -1470.199920063936, -2210.768096208106,
      -1142.1267040355187, -1582.8246473557422, -1125.095107149504, -1258.309518948453,
      -1644.1228935750487, -1890.3566884761815, -2684.0639102274663, -2246.7156809750923,
      -1172.0007069294636, -1742.4880949681342, -1416.2056246293446, -1117.0469991071966,
      -1610.4647723677451, -1943.5159132377182, -1100.0, -485.26637596458363, -1117.0469991071966,
      -1741.8033988749899, -2277.8829422805593, -2326.9771560359227, -742.1320343559642,
      -1122.8858397486192, -1000.0, -602.8010988928053, -1662.0465053408523, -2156.8154169226937,
      -544.6424919657296, -1150.415945787923, -1749.8101701635958, -927.8488797889962,
      -1144.3398113205658, -756.6190378969059, -1117.0469991071966, -1816.2224712242912,
      -1919.4224354214568, -1473.2355896651052, -884.8175516182639, -1831.4482300479492,
      -1020.0, -1282.8837943815327, -143.44155827794896, -1891.2451549659706, 284.55996254682475,
      -836.8154169226942, -1559.7619668804423, -2230.587727318528, -1188.2983138313673,
      -3361.448230047949, -2218.395587738682, -2230.587727318528, -1929.5854983242398,
      -983.5931792539567, -1352.132034355964, -1594.779692210759, -2505.068036361263,
      40.0, -873.6899230902958, -3509.2582403567253, -410.66339351696763, -1217.9876538505325,
      -1800.4834939252007, -1057.3210979175494, -1200.7882977473878, -1121.2803487908084,
      -1066.8043536087869, -1158.0584360149871, -1096.6761504739773]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.07919362498169939
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 10.411826971483048
    mean_inference_ms: 1.5176445765721396
    mean_raw_obs_processing_ms: 0.6773188303380919
time_since_restore: 14.220519065856934
time_this_iter_s: 1.854612112045288
time_total_s: 14.220519065856934
timers:
  learn_throughput: 720.247
  learn_time_ms: 11.107
  load_throughput: 100282.224
  load_time_ms: 0.08
  sample_time_ms: 31.811
  synch_weights_time_ms: 8.909
  training_iteration_time_ms: 54.853
timestamp: 1708385316
timesteps_total: 1000
training_iteration: 10
trial_id: default

2024-02-19 15:28:36,781 DEBUG {'env': {'cropped_map_size': 64, 'ratio_coverage': 0.0125, 'original_map_path': './resource/usc.png', 'original_map_scale': 3.4375, 'n_maps': 100, 'n_steps_per_map': 10, 'no_masking': True, 'coefficient_dict': {'r_c': 1.0, 'p_d': 1.0, 'p_b': 1.0}, 'preset_map_path': './resource/setup.json'}, 'explore': {'explore': True, 'exploration_config': {'type': 'EpsilonGreedy', 'initial_epsilon': 1.0, 'final_epsilon': 0.02, 'epsilon_timesteps': 300}}, 'train': {'train_batch_size': 8, 'num_steps_sampled_before_learning_starts': 100, 'replay_buffer_config': {'capacity': 50000}}, 'eval': {'evaluation_interval': 5, 'evaluation_duration': 3, 'evaluation_config': {'explore': False, 'env_config': {'evaluation': True}}}, 'report': {'min_sample_timesteps_per_iteration': 100}, 'stop': {'training_iteration': 10}, 'rollout': {'num_rollout_workers': 4, 'num_envs_per_worker': 1}, 'resource': {'num_gpus': 0, 'num_cpus_per_worker': 2, 'num_gpus_per_worker': 0}}
2024-02-19 15:34:16,753 INFO =============A WHOLE TRAINING PERIOD ENDED=============
2024-02-19 15:34:16,762 INFO agent_timesteps_total: 1000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.009578704833984375
  StateBufferConnector_ms: 0.005437135696411133
  ViewRequirementAgentConnector_ms: 0.15080523490905762
counters:
  last_target_update_ts: 1000
  num_agent_steps_sampled: 1000
  num_agent_steps_trained: 1800
  num_env_steps_sampled: 1000
  num_env_steps_trained: 1800
  num_target_updates: 2
custom_metrics: {}
date: 2024-02-19_15-34-16
done: false
episode_len_mean: 10.0
episode_media: {}
episode_reward_max: 2037.4516600406096
episode_reward_mean: -1749.9737954875297
episode_reward_min: -4284.576326543828
episodes_this_iter: 12
episodes_total: 100
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0036160151163736978
    StateBufferConnector_ms: 0.0026226043701171875
    ViewRequirementAgentConnector_ms: 0.05828539530436198
  custom_metrics: {}
  episode_len_mean: 10.0
  episode_media: {}
  episode_reward_max: -270.8276253029822
  episode_reward_mean: -532.8630125667531
  episode_reward_min: -930.0
  episodes_this_iter: 3
  hist_stats:
    episode_lengths:
    - 10
    - 10
    - 10
    episode_reward:
    - -397.7614123972772
    - -270.8276253029822
    - -930.0
  num_agent_steps_sampled_this_iter: 30
  num_env_steps_sampled_this_iter: 30
  num_faulty_episodes: 0
  num_healthy_workers: 0
  num_in_flight_async_reqs: 0
  num_remote_worker_restarts: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.0548558156998431
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 9.90049174574555
    mean_inference_ms: 0.7315151026991545
    mean_raw_obs_processing_ms: 0.2498314028880635
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.0036160151163736978
      StateBufferConnector_ms: 0.0026226043701171875
      ViewRequirementAgentConnector_ms: 0.05828539530436198
    custom_metrics: {}
    episode_len_mean: 10.0
    episode_media: {}
    episode_reward_max: -270.8276253029822
    episode_reward_mean: -532.8630125667531
    episode_reward_min: -930.0
    episodes_this_iter: 3
    hist_stats:
      episode_lengths:
      - 10
      - 10
      - 10
      episode_reward:
      - -397.7614123972772
      - -270.8276253029822
      - -930.0
    num_faulty_episodes: 0
    policy_reward_max: {}
    policy_reward_mean: {}
    policy_reward_min: {}
    sampler_perf:
      mean_action_processing_ms: 0.0548558156998431
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 9.90049174574555
      mean_inference_ms: 0.7315151026991545
      mean_raw_obs_processing_ms: 0.2498314028880635
  timesteps_this_iter: 30
hostname: OrangeBook-Pro-14.local
info:
  last_target_update_ts: 1000
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 224.0
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 0.0005
        grad_gnorm: 40.0
        max_q: -292.0897216796875
        mean_q: -416.52911376953125
        min_q: -611.5111694335938
      mean_td_error: 95.81438446044922
      model: {}
      num_agent_steps_trained: 8.0
      num_grad_updates_lifetime: 225.0
      td_error: [29.683837890625, -32.477386474609375, 7.071136474609375, 116.7821044921875,
        156.04998779296875, 151.7457275390625, 194.97274780273438, 142.68695068359375]
  num_agent_steps_sampled: 1000
  num_agent_steps_trained: 1800
  num_env_steps_sampled: 1000
  num_env_steps_trained: 1800
  num_target_updates: 2
iterations_since_restore: 10
node_ip: 127.0.0.1
num_agent_steps_sampled: 1000
num_agent_steps_trained: 1800
num_env_steps_sampled: 1000
num_env_steps_sampled_this_iter: 100
num_env_steps_sampled_throughput_per_sec: 84.36026907245628
num_env_steps_trained: 1800
num_env_steps_trained_this_iter: 200
num_env_steps_trained_throughput_per_sec: 168.72053814491255
num_faulty_episodes: 0
num_healthy_workers: 4
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 200
perf:
  cpu_util_percent: 57.75
  ram_util_percent: 83.25
pid: 58263
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.07545948767741863
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 9.615072706898282
  mean_inference_ms: 1.495888312853138
  mean_raw_obs_processing_ms: 0.6842369720981131
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.009578704833984375
    StateBufferConnector_ms: 0.005437135696411133
    ViewRequirementAgentConnector_ms: 0.15080523490905762
  custom_metrics: {}
  episode_len_mean: 10.0
  episode_media: {}
  episode_reward_max: 2037.4516600406096
  episode_reward_mean: -1749.9737954875297
  episode_reward_min: -4284.576326543828
  episodes_this_iter: 12
  hist_stats:
    episode_lengths: [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,
      10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,
      10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,
      10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,
      10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,
      10, 10, 10, 10, 10, 10, 10, 10, 10]
    episode_reward: [-3165.4172567935757, -3059.271428160642, -3574.795705753796,
      -3179.2924695939323, -2701.3275503993973, 625.6277106333401, -2971.6550817578845,
      -3363.3682222665493, -3050.7368530174026, -3394.3333589468625, -2147.6926938048387,
      -2954.528597074577, -2636.4823201627687, -2687.9501063475377, -2946.508176626544,
      -3154.646700407544, -1458.4494241079942, -2073.01757252337, -2852.1483445601916,
      -2762.3421401473306, -4284.576326543828, -1288.0592940069184, -2922.6102221807687,
      -2354.3247226031676, -2802.9433940600925, -1778.832789447697, -3416.525656083055,
      -3025.4719739232714, -2430.5174819226872, -1464.1640786499875, -1072.7882059609967,
      -1884.5534888629013, -1188.4885780179613, -583.4166406412634, -95.41019662496836,
      -732.745623366089, -2242.132034355964, -1911.204395571221, -3311.780056072107,
      -583.4166406412634, -95.41019662496836, -1434.0939982143927, -1120.4159457879232,
      -1890.0, -2494.093998214393, -1889.615099714943, -1434.0939982143927, -1684.3908891458575,
      -2200.691829171611, -1043.851648071345, -891.5729949498046, -727.7776628369419,
      -1020.0, -2023.1128874149274, -1600.0, -2165.2867504494748, -1020.0, -219.31712199461316,
      -2513.2380757938113, -1466.4911064067353, -411.2310562561766, -260.0, -1072.7882059609967,
      -1059.5449840010017, -2165.2867504494748, -2051.7935662402833, -2208.2220510185593,
      -1263.6445290137794, -1030.0, -562.1954445729289, -1301.6025568065745, -1263.6445290137794,
      -880.0, -2510.1774378963282, -1586.7572330035593, -1022.9916149650395, -1341.6025568065743,
      -1512.5197137120413, -1550.7680962081056, -1133.0165161069342, -370.68883707497264,
      -2563.3219106185293, -1030.0, -829.6035225442041, -1030.0, -1528.6205100903642,
      -2338.806130178211, -514.1878230385279, 2037.4516600406096, -1530.0, -1304.0046326802137,
      -1235.4065922853804, -1235.4065922853804, -1097.389277270367, -2649.3171219946134,
      -2024.0075756488818, -2464.119856344567, -2430.7987240796892, -2075.7066051117276,
      -779.9063611540967]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.07545948767741863
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 9.615072706898282
    mean_inference_ms: 1.495888312853138
    mean_raw_obs_processing_ms: 0.6842369720981131
time_since_restore: 13.550511121749878
time_this_iter_s: 1.5313429832458496
time_total_s: 13.550511121749878
timers:
  learn_throughput: 846.453
  learn_time_ms: 9.451
  load_throughput: 108872.265
  load_time_ms: 0.073
  sample_time_ms: 32.492
  synch_weights_time_ms: 6.519
  training_iteration_time_ms: 50.673
timestamp: 1708385656
timesteps_total: 1000
training_iteration: 10
trial_id: default

2024-02-19 15:34:16,762 DEBUG {'env': {'cropped_map_size': 64, 'ratio_coverage': 0.0125, 'original_map_path': './resource/usc.png', 'original_map_scale': 3.4375, 'n_maps': 100, 'n_steps_per_map': 10, 'no_masking': True, 'coefficient_dict': {'r_c': 1.0, 'p_d': 1.0, 'p_b': 1.0}, 'preset_map_path': './resource/setup.json'}, 'explore': {'explore': True, 'exploration_config': {'type': 'EpsilonGreedy', 'initial_epsilon': 1.0, 'final_epsilon': 0.02, 'epsilon_timesteps': 300}}, 'train': {'train_batch_size': 8, 'num_steps_sampled_before_learning_starts': 100, 'replay_buffer_config': {'capacity': 50000}}, 'eval': {'evaluation_interval': 5, 'evaluation_duration': 3, 'evaluation_config': {'explore': False, 'env_config': {'evaluation': True}}}, 'report': {'min_sample_timesteps_per_iteration': 100}, 'stop': {'training_iteration': 10}, 'rollout': {'num_rollout_workers': 4, 'num_envs_per_worker': 1}, 'resource': {'num_gpus': 0, 'num_cpus_per_worker': 2, 'num_gpus_per_worker': 0}}
2024-02-19 15:39:57,077 INFO =============A WHOLE TRAINING PERIOD ENDED=============
2024-02-19 15:39:57,085 INFO agent_timesteps_total: 1000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.010541915893554688
  StateBufferConnector_ms: 0.005388498306274414
  ViewRequirementAgentConnector_ms: 0.16030359268188477
counters:
  last_target_update_ts: 1000
  num_agent_steps_sampled: 1000
  num_agent_steps_trained: 1800
  num_env_steps_sampled: 1000
  num_env_steps_trained: 1800
  num_target_updates: 2
custom_metrics: {}
date: 2024-02-19_15-39-57
done: false
episode_len_mean: 10.0
episode_media: {}
episode_reward_max: 221.31157169083366
episode_reward_mean: -1816.286729669848
episode_reward_min: -4125.710624651023
episodes_this_iter: 12
episodes_total: 100
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0032742818196614585
    StateBufferConnector_ms: 0.0020345052083333335
    ViewRequirementAgentConnector_ms: 0.0507195790608724
  custom_metrics: {}
  episode_len_mean: 10.0
  episode_media: {}
  episode_reward_max: -371.8033988749894
  episode_reward_mean: -2001.5030645593422
  episode_reward_min: -3010.599281722834
  episodes_this_iter: 3
  hist_stats:
    episode_lengths:
    - 10
    - 10
    - 10
    episode_reward:
    - -2622.106513080203
    - -371.8033988749894
    - -3010.599281722834
  num_agent_steps_sampled_this_iter: 30
  num_env_steps_sampled_this_iter: 30
  num_faulty_episodes: 0
  num_healthy_workers: 0
  num_in_flight_async_reqs: 0
  num_remote_worker_restarts: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.043763489019675327
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 9.244484979598248
    mean_inference_ms: 0.6797743625328189
    mean_raw_obs_processing_ms: 0.32901373065885947
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.0032742818196614585
      StateBufferConnector_ms: 0.0020345052083333335
      ViewRequirementAgentConnector_ms: 0.0507195790608724
    custom_metrics: {}
    episode_len_mean: 10.0
    episode_media: {}
    episode_reward_max: -371.8033988749894
    episode_reward_mean: -2001.5030645593422
    episode_reward_min: -3010.599281722834
    episodes_this_iter: 3
    hist_stats:
      episode_lengths:
      - 10
      - 10
      - 10
      episode_reward:
      - -2622.106513080203
      - -371.8033988749894
      - -3010.599281722834
    num_faulty_episodes: 0
    policy_reward_max: {}
    policy_reward_mean: {}
    policy_reward_min: {}
    sampler_perf:
      mean_action_processing_ms: 0.043763489019675327
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 9.244484979598248
      mean_inference_ms: 0.6797743625328189
      mean_raw_obs_processing_ms: 0.32901373065885947
  timesteps_this_iter: 30
hostname: OrangeBook-Pro-14.local
info:
  last_target_update_ts: 1000
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 224.0
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 0.0005
        grad_gnorm: 40.0
        max_q: -282.3973693847656
        mean_q: -442.556396484375
        min_q: -645.0791015625
      mean_td_error: 29.474807739257812
      model: {}
      num_agent_steps_trained: 8.0
      num_grad_updates_lifetime: 225.0
      td_error: [120.93896484375, -70.07891845703125, 203.67822265625, 28.73223876953125,
        -115.40841674804688, 1.31048583984375, 52.571685791015625, 14.05419921875]
  num_agent_steps_sampled: 1000
  num_agent_steps_trained: 1800
  num_env_steps_sampled: 1000
  num_env_steps_trained: 1800
  num_target_updates: 2
iterations_since_restore: 10
node_ip: 127.0.0.1
num_agent_steps_sampled: 1000
num_agent_steps_trained: 1800
num_env_steps_sampled: 1000
num_env_steps_sampled_this_iter: 100
num_env_steps_sampled_throughput_per_sec: 88.80141597716822
num_env_steps_trained: 1800
num_env_steps_trained_this_iter: 200
num_env_steps_trained_throughput_per_sec: 177.60283195433644
num_faulty_episodes: 0
num_healthy_workers: 4
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 200
perf:
  cpu_util_percent: 50.7
  ram_util_percent: 81.55
pid: 58703
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.09327160456738076
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 10.616903415170157
  mean_inference_ms: 1.541152235788115
  mean_raw_obs_processing_ms: 0.7171683963058233
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.010541915893554688
    StateBufferConnector_ms: 0.005388498306274414
    ViewRequirementAgentConnector_ms: 0.16030359268188477
  custom_metrics: {}
  episode_len_mean: 10.0
  episode_media: {}
  episode_reward_max: 221.31157169083366
  episode_reward_mean: -1816.286729669848
  episode_reward_min: -4125.710624651023
  episodes_this_iter: 12
  hist_stats:
    episode_lengths: [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,
      10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,
      10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,
      10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,
      10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,
      10, 10, 10, 10, 10, 10, 10, 10, 10]
    episode_reward: [-3817.0757718061664, -2969.309439097835, -3384.752559293608,
      -3668.4269132080553, -3249.5513399322417, -3804.4657087065516, -1881.6814031370209,
      -2427.9613549202627, -2549.290098807675, -2367.038638041743, -3322.3680044386356,
      -3066.5190903863504, -2690.90730586656, -2596.026368484112, -2195.2053560495374,
      -2264.2223089936883, -1631.3299170487167, -4125.710624651023, -2929.2952504389414,
      -1466.551460980071, -3879.3557950369423, -1742.0692801113673, -1651.4510520798383,
      -2740.160478433786, -1379.5138611028742, -2483.8004460990874, -3849.5864463874013,
      -2181.8987305374844, -390.0, -2548.218696620299, -1658.24824277294, -1575.9395512530791,
      -1072.3885892824794, 221.31157169083366, -2021.247837363769, -1240.95023109729,
      -1800.0, -541.8033988749893, -1696.0084744241187, -3747.9034765068454, -1352.11102550928,
      -1995.373831248081, -1897.6482306023336, 221.31157169083366, -1560.903600462544,
      -1350.5538513813738, -1976.98484809835, -1760.6243908376282, -1610.710678118655,
      -1060.827625302982, -1516.0232526704265, -2145.821648252299, -2141.344154376882,
      -2130.0, -1942.569992161756, -804.2856834486539, -1826.98484809835, -2322.4099870362666,
      -1431.0179957261726, -1306.0232526704265, -1060.827625302982, -2130.0, -965.4065922853804,
      -1420.4536101718722, -533.8286205470782, -975.4065922853804, -2088.2542442102663,
      -1279.224445373014, -1130.867927612304, -1387.6972864800944, -111.14099732158877,
      -1166.568542494924, -2202.8645954981166, -1099.2838827718413, -560.3840481040528,
      -1116.6190378969056, -801.2451549659709, -1159.777744614864, -1850.0293569325745,
      -2828.035085019828, -1486.4688270438849, -1366.9817807045692, -1622.237484161567,
      -1100.0, -965.4065922853804, -90.0, -2130.0, -1823.6067977499786, -1056.5247584249855,
      -1010.0, -1268.6622302019725, -1499.442719099991, -1090.0, -2828.035085019828,
      -2247.70329614269, -732.126704035519, -1166.568542494924, -2731.725046566048,
      -1826.98484809835, -1420.4536101718722]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09327160456738076
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 10.616903415170157
    mean_inference_ms: 1.541152235788115
    mean_raw_obs_processing_ms: 0.7171683963058233
time_since_restore: 13.710776329040527
time_this_iter_s: 1.4390149116516113
time_total_s: 13.710776329040527
timers:
  learn_throughput: 910.319
  learn_time_ms: 8.788
  load_throughput: 116711.068
  load_time_ms: 0.069
  sample_time_ms: 27.288
  synch_weights_time_ms: 6.323
  training_iteration_time_ms: 44.493
timestamp: 1708385997
timesteps_total: 1000
training_iteration: 10
trial_id: default

2024-02-19 15:39:57,085 DEBUG {'env': {'cropped_map_size': 64, 'ratio_coverage': 0.0125, 'original_map_path': './resource/usc.png', 'original_map_scale': 3.4375, 'n_maps': 100, 'n_steps_per_map': 10, 'no_masking': True, 'coefficient_dict': {'r_c': 1.0, 'p_d': 1.0, 'p_b': 1.0}, 'preset_map_path': './resource/setup.json'}, 'explore': {'explore': True, 'exploration_config': {'type': 'EpsilonGreedy', 'initial_epsilon': 1.0, 'final_epsilon': 0.02, 'epsilon_timesteps': 300}}, 'train': {'train_batch_size': 8, 'num_steps_sampled_before_learning_starts': 100, 'replay_buffer_config': {'capacity': 50000}}, 'eval': {'evaluation_interval': 5, 'evaluation_duration': 3, 'evaluation_config': {'explore': False, 'env_config': {'evaluation': True}}}, 'report': {'min_sample_timesteps_per_iteration': 100}, 'stop': {'training_iteration': 10}, 'rollout': {'num_rollout_workers': 4, 'num_envs_per_worker': 1}, 'resource': {'num_gpus': 0, 'num_cpus_per_worker': 2, 'num_gpus_per_worker': 0}}
2024-02-21 16:27:47,825 INFO ================EVALUATION AT # 5================
2024-02-21 16:27:48,570 INFO ================EVALUATION AT # 10================
2024-02-21 16:27:49,151 INFO =============A WHOLE TRAINING PERIOD ENDED=============
2024-02-21 16:27:49,155 INFO agent_timesteps_total: 120
connector_metrics:
  ObsPreprocessorConnector_ms: 0.005594889322916667
  StateBufferConnector_ms: 0.00351866086324056
  ViewRequirementAgentConnector_ms: 0.09784897168477376
counters:
  last_target_update_ts: 0
  num_agent_steps_sampled: 120
  num_agent_steps_trained: 40
  num_env_steps_sampled: 120
  num_env_steps_trained: 40
custom_metrics: {}
date: 2024-02-21_16-27-49
done: false
episode_len_mean: 10.0
episode_media: {}
episode_reward_max: -1264.979219017866
episode_reward_mean: -1383.0779960162592
episode_reward_min: -1528.901993817563
episodes_this_iter: 4
episodes_total: 12
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0035047531127929688
    StateBufferConnector_ms: 0.0038067499796549478
    ViewRequirementAgentConnector_ms: 0.06035168965657552
  custom_metrics: {}
  episode_len_mean: 10.0
  episode_media: {}
  episode_reward_max: -649.2478373637688
  episode_reward_mean: -1288.7981806768728
  episode_reward_min: -1675.4375991310112
  episodes_this_iter: 3
  hist_stats:
    episode_lengths:
    - 10
    - 10
    - 10
    episode_reward:
    - -1675.4375991310112
    - -649.2478373637688
    - -1541.7091055358387
  num_agent_steps_sampled_this_iter: 30
  num_env_steps_sampled_this_iter: 30
  num_faulty_episodes: 0
  num_healthy_workers: 0
  num_in_flight_async_reqs: 0
  num_remote_worker_restarts: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.046319648867747816
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 9.67315767632156
    mean_inference_ms: 0.6968349706931192
    mean_raw_obs_processing_ms: 0.2174494696445152
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.0035047531127929688
      StateBufferConnector_ms: 0.0038067499796549478
      ViewRequirementAgentConnector_ms: 0.06035168965657552
    custom_metrics: {}
    episode_len_mean: 10.0
    episode_media: {}
    episode_reward_max: -649.2478373637688
    episode_reward_mean: -1288.7981806768728
    episode_reward_min: -1675.4375991310112
    episodes_this_iter: 3
    hist_stats:
      episode_lengths:
      - 10
      - 10
      - 10
      episode_reward:
      - -1675.4375991310112
      - -649.2478373637688
      - -1541.7091055358387
    num_faulty_episodes: 0
    policy_reward_max: {}
    policy_reward_mean: {}
    policy_reward_min: {}
    sampler_perf:
      mean_action_processing_ms: 0.046319648867747816
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 9.67315767632156
      mean_inference_ms: 0.6968349706931192
      mean_raw_obs_processing_ms: 0.2174494696445152
  timesteps_this_iter: 30
hostname: OrangeBook-Pro-14.local
info:
  last_target_update_ts: 0
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 4.0
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 0.0003
        grad_gnorm: 4.298661231994629
        max_q: -0.11174232512712479
        mean_q: -1.1938517093658447
        min_q: -1.6580101251602173
      mean_td_error: 148.90887451171875
      model: {}
      num_agent_steps_trained: 8.0
      num_grad_updates_lifetime: 5.0
      td_error: [114.87922668457031, 127.472900390625, 165.2073516845703, 106.95957946777344,
        161.73846435546875, 161.73846435546875, 156.0069122314453, 197.26817321777344]
  num_agent_steps_sampled: 120
  num_agent_steps_trained: 40
  num_env_steps_sampled: 120
  num_env_steps_trained: 40
iterations_since_restore: 10
node_ip: 127.0.0.1
num_agent_steps_sampled: 120
num_agent_steps_trained: 40
num_env_steps_sampled: 120
num_env_steps_sampled_this_iter: 12
num_env_steps_sampled_throughput_per_sec: 51.334257380906486
num_env_steps_trained: 40
num_env_steps_trained_this_iter: 24
num_env_steps_trained_throughput_per_sec: 102.66851476181297
num_faulty_episodes: 0
num_healthy_workers: 4
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 24
perf:
  cpu_util_percent: 50.1
  ram_util_percent: 84.1
pid: 89575
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.09251133428766221
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 9.670275444374541
  mean_inference_ms: 1.4415535252563432
  mean_raw_obs_processing_ms: 0.488493066325619
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.005594889322916667
    StateBufferConnector_ms: 0.00351866086324056
    ViewRequirementAgentConnector_ms: 0.09784897168477376
  custom_metrics: {}
  episode_len_mean: 10.0
  episode_media: {}
  episode_reward_max: -1264.979219017866
  episode_reward_mean: -1383.0779960162592
  episode_reward_min: -1528.901993817563
  episodes_this_iter: 4
  hist_stats:
    episode_lengths: [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]
    episode_reward: [-1277.021639157404, -1528.901993817563, -1398.6923410698491,
      -1318.2911513573758, -1382.623431549863, -1264.979219017866, -1296.655733316318,
      -1484.1235083146994, -1417.456177291377, -1406.9406344776235, -1341.921184542203,
      -1479.3289382829712]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09251133428766221
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 9.670275444374541
    mean_inference_ms: 1.4415535252563432
    mean_raw_obs_processing_ms: 0.488493066325619
time_since_restore: 1.493851661682129
time_this_iter_s: 0.5731911659240723
time_total_s: 1.493851661682129
timers:
  learn_throughput: 401.962
  learn_time_ms: 19.902
  load_throughput: 111328.573
  load_time_ms: 0.072
  sample_time_ms: 18.829
  synch_weights_time_ms: 37.424
  training_iteration_time_ms: 48.522
timestamp: 1708561669
timesteps_total: 120
training_iteration: 10
trial_id: default

2024-02-21 16:27:49,155 DEBUG {'env': {'coefficient_dict': {'p_b': 1.0, 'p_d': 1.0, 'r_c': 0.1}, 'cropped_map_size': 64, 'n_maps': 100, 'n_steps_per_map': 10, 'no_masking': True, 'original_map_path': './resource/usc.png', 'original_map_scale': 3.4375, 'preset_map_path': './resource/setup.json', 'ratio_coverage': 0.0125}, 'eval': {'evaluation_config': {'env_config': {'evaluation': True}, 'explore': False}, 'evaluation_duration': 3, 'evaluation_interval': 5}, 'explore': {'exploration_config': {'epsilon_timesteps': 300, 'final_epsilon': 0.02, 'initial_epsilon': 1.0, 'type': 'EpsilonGreedy'}, 'explore': True}, 'report': {'min_sample_timesteps_per_iteration': 10}, 'resource': {'num_cpus_per_worker': 2, 'num_gpus': 0, 'num_gpus_per_worker': 0}, 'rollout': {'num_envs_per_worker': 1, 'num_rollout_workers': 4}, 'stop': {'training_iteration': 10}, 'train': {'num_steps_sampled_before_learning_starts': 100, 'replay_buffer_config': {'capacity': 50000}, 'train_batch_size': 8}}
2024-02-21 16:43:03,037 INFO ================EVALUATION AT # 5================
2024-02-21 16:43:03,842 INFO ================EVALUATION AT # 10================
2024-02-21 16:43:04,429 INFO =============A WHOLE TRAINING PERIOD ENDED=============
2024-02-21 16:43:04,434 INFO agent_timesteps_total: 120
connector_metrics:
  ObsPreprocessorConnector_ms: 0.005813439687093099
  StateBufferConnector_ms: 0.0034709771474202475
  ViewRequirementAgentConnector_ms: 0.09875893592834473
counters:
  last_target_update_ts: 0
  num_agent_steps_sampled: 120
  num_agent_steps_trained: 40
  num_env_steps_sampled: 120
  num_env_steps_trained: 40
custom_metrics: {}
date: 2024-02-21_16-43-04
done: false
episode_len_mean: 10.0
episode_media: {}
episode_reward_max: -823.4567348425909
episode_reward_mean: -1266.710278333799
episode_reward_min: -1456.5632898613424
episodes_this_iter: 4
episodes_total: 12
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0030120213826497397
    StateBufferConnector_ms: 0.0022172927856445312
    ViewRequirementAgentConnector_ms: 0.04480679829915365
  custom_metrics: {}
  episode_len_mean: 10.0
  episode_media: {}
  episode_reward_max: -1230.7640731561885
  episode_reward_mean: -1321.0136964437495
  episode_reward_min: -1489.4292530665577
  episodes_this_iter: 3
  hist_stats:
    episode_lengths:
    - 10
    - 10
    - 10
    episode_reward:
    - -1230.7640731561885
    - -1489.4292530665577
    - -1242.8477631085022
  num_agent_steps_sampled_this_iter: 30
  num_env_steps_sampled_this_iter: 30
  num_faulty_episodes: 0
  num_healthy_workers: 0
  num_in_flight_async_reqs: 0
  num_remote_worker_restarts: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.0484341480692879
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 11.340981624165517
    mean_inference_ms: 0.624035225539911
    mean_raw_obs_processing_ms: 0.21946625631363662
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.0030120213826497397
      StateBufferConnector_ms: 0.0022172927856445312
      ViewRequirementAgentConnector_ms: 0.04480679829915365
    custom_metrics: {}
    episode_len_mean: 10.0
    episode_media: {}
    episode_reward_max: -1230.7640731561885
    episode_reward_mean: -1321.0136964437495
    episode_reward_min: -1489.4292530665577
    episodes_this_iter: 3
    hist_stats:
      episode_lengths:
      - 10
      - 10
      - 10
      episode_reward:
      - -1230.7640731561885
      - -1489.4292530665577
      - -1242.8477631085022
    num_faulty_episodes: 0
    policy_reward_max: {}
    policy_reward_mean: {}
    policy_reward_min: {}
    sampler_perf:
      mean_action_processing_ms: 0.0484341480692879
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 11.340981624165517
      mean_inference_ms: 0.624035225539911
      mean_raw_obs_processing_ms: 0.21946625631363662
  timesteps_this_iter: 30
hostname: OrangeBook-Pro-14.local
info:
  last_target_update_ts: 0
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 4.0
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 0.0003
        grad_gnorm: 7.335890769958496
        max_q: -0.7840121984481812
        mean_q: -1.252914547920227
        min_q: -1.5984454154968262
      mean_td_error: 149.04693603515625
      model: {}
      num_agent_steps_trained: 8.0
      num_grad_updates_lifetime: 5.0
      td_error: [129.7115020751953, 134.43228149414062, 158.63861083984375, 128.5892333984375,
        151.7035675048828, 179.29531860351562, 149.2401885986328, 160.7648162841797]
  num_agent_steps_sampled: 120
  num_agent_steps_trained: 40
  num_env_steps_sampled: 120
  num_env_steps_trained: 40
iterations_since_restore: 10
node_ip: 127.0.0.1
num_agent_steps_sampled: 120
num_agent_steps_trained: 40
num_env_steps_sampled: 120
num_env_steps_sampled_this_iter: 12
num_env_steps_sampled_throughput_per_sec: 59.05508850933317
num_env_steps_trained: 40
num_env_steps_trained_this_iter: 24
num_env_steps_trained_throughput_per_sec: 118.11017701866633
num_faulty_episodes: 0
num_healthy_workers: 4
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 24
perf:
  cpu_util_percent: 59.7
  ram_util_percent: 83.8
pid: 89837
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.16001302305491616
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 9.971010200026628
  mean_inference_ms: 1.913414311880491
  mean_raw_obs_processing_ms: 0.997885468938546
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.005813439687093099
    StateBufferConnector_ms: 0.0034709771474202475
    ViewRequirementAgentConnector_ms: 0.09875893592834473
  custom_metrics: {}
  episode_len_mean: 10.0
  episode_media: {}
  episode_reward_max: -823.4567348425909
  episode_reward_mean: -1266.710278333799
  episode_reward_min: -1456.5632898613424
  episodes_this_iter: 4
  hist_stats:
    episode_lengths: [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]
    episode_reward: [-823.4567348425909, -1373.5893191548028, -1162.52491648647, -1454.8917730042333,
      -1456.5632898613424, -1387.5770353054045, -1307.2695237469552, -1265.7417229220764,
      -1243.3714561200932, -1308.3534465441073, -1040.5011718440428, -1376.6829501734676]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16001302305491616
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 9.971010200026628
    mean_inference_ms: 1.913414311880491
    mean_raw_obs_processing_ms: 0.997885468938546
time_since_restore: 1.6047723293304443
time_this_iter_s: 0.5800399780273438
time_total_s: 1.6047723293304443
timers:
  learn_throughput: 456.867
  learn_time_ms: 17.511
  load_throughput: 116589.409
  load_time_ms: 0.069
  sample_time_ms: 20.585
  synch_weights_time_ms: 32.342
  training_iteration_time_ms: 46.443
timestamp: 1708562584
timesteps_total: 120
training_iteration: 10
trial_id: default

2024-02-21 16:43:04,434 DEBUG {'env': {'coefficient_dict': {'p_b': 1.0, 'p_d': 1.0, 'r_c': 0.1}, 'cropped_map_size': 64, 'n_maps': 100, 'n_steps_per_map': 10, 'no_masking': True, 'original_map_path': './resource/usc.png', 'original_map_scale': 3.4375, 'preset_map_path': './resource/setup.json', 'ratio_coverage': 0.0125}, 'eval': {'evaluation_config': {'env_config': {'evaluation': True, 'n_maps': 3, 'preset_map_path': None}, 'explore': False}, 'evaluation_duration': 3, 'evaluation_interval': 5}, 'explore': {'exploration_config': {'epsilon_timesteps': 300, 'final_epsilon': 0.02, 'initial_epsilon': 1.0, 'type': 'EpsilonGreedy'}, 'explore': True}, 'report': {'min_sample_timesteps_per_iteration': 10}, 'resource': {'num_cpus_per_worker': 2, 'num_gpus': 0, 'num_gpus_per_worker': 0}, 'rollout': {'num_envs_per_worker': 1, 'num_rollout_workers': 4}, 'stop': {'training_iteration': 10}, 'train': {'num_steps_sampled_before_learning_starts': 100, 'replay_buffer_config': {'capacity': 50000}, 'train_batch_size': 8}}
2024-03-31 02:00:58,681 INFO 
=============Test for DQN AFTER Training=============
2024-03-31 02:00:58,988 INFO average coverage reward for trained map 0 in 1 steps: -100000.0, std: 0.0, optimal reward: 4124, ratio: -24.248302618816684, num_roi: 54592
2024-03-31 02:00:59,310 INFO average coverage reward for trained map 1 in 1 steps: -100000.0, std: 0.0, optimal reward: 4586, ratio: -21.805494984736153, num_roi: 53472
2024-03-31 02:00:59,642 INFO average coverage reward for trained map 2 in 1 steps: -100000.0, std: 0.0, optimal reward: 4616, ratio: -21.663778162911612, num_roi: 52976
2024-03-31 02:00:59,988 INFO average coverage reward for trained map 3 in 1 steps: 3338.0, std: 0.0, optimal reward: 4332, ratio: 0.7705447830101569, num_roi: 52224
2024-03-31 02:01:00,331 INFO average coverage reward for trained map 4 in 1 steps: -100000.0, std: 0.0, optimal reward: 4250, ratio: -23.529411764705884, num_roi: 52688
2024-03-31 02:01:00,694 INFO average coverage reward for trained map 5 in 1 steps: -100000.0, std: 0.0, optimal reward: 4352, ratio: -22.977941176470587, num_roi: 51024
2024-03-31 02:01:00,994 INFO average coverage reward for trained map 6 in 1 steps: -100000.0, std: 0.0, optimal reward: 4043, ratio: -24.73410833539451, num_roi: 53968
2024-03-31 02:01:01,351 INFO average coverage reward for trained map 7 in 1 steps: -100000.0, std: 0.0, optimal reward: 4517, ratio: -22.13858755811379, num_roi: 51472
2024-03-31 02:01:01,569 INFO average coverage reward for trained map 8 in 1 steps: -100000.0, std: 0.0, optimal reward: 4411, ratio: -22.670596236681025, num_roi: 56112
2024-03-31 02:01:01,914 INFO average coverage reward for trained map 9 in 1 steps: -100000.0, std: 0.0, optimal reward: 4249, ratio: -23.53494939985879, num_roi: 51760
2024-03-31 02:01:02,166 INFO average coverage reward for trained map 10 in 1 steps: -100000.0, std: 0.0, optimal reward: 4146, ratio: -24.1196333815726, num_roi: 55168
2024-03-31 02:01:02,472 INFO average coverage reward for trained map 11 in 1 steps: -100000.0, std: 0.0, optimal reward: 4162, ratio: -24.02691013935608, num_roi: 52944
2024-03-31 02:01:02,766 INFO average coverage reward for trained map 12 in 1 steps: -100000.0, std: 0.0, optimal reward: 4267, ratio: -23.435669088352473, num_roi: 53104
2024-03-31 02:01:03,078 INFO average coverage reward for trained map 13 in 1 steps: -100000.0, std: 0.0, optimal reward: 4315, ratio: -23.174971031286212, num_roi: 53744
2024-03-31 02:01:03,394 INFO average coverage reward for trained map 14 in 1 steps: -100000.0, std: 0.0, optimal reward: 4307, ratio: -23.218017181332716, num_roi: 52624
2024-03-31 02:01:03,729 INFO average coverage reward for trained map 15 in 1 steps: -100000.0, std: 0.0, optimal reward: 4081, ratio: -24.50379808870375, num_roi: 51648
2024-03-31 02:01:04,025 INFO average coverage reward for trained map 16 in 1 steps: -100000.0, std: 0.0, optimal reward: 3995, ratio: -25.031289111389235, num_roi: 53392
2024-03-31 02:01:04,399 INFO average coverage reward for trained map 17 in 1 steps: -100000.0, std: 0.0, optimal reward: 4038, ratio: -24.764735017335315, num_roi: 50304
2024-03-31 02:01:04,782 INFO average coverage reward for trained map 18 in 1 steps: 3573.0, std: 0.0, optimal reward: 4197, ratio: 0.8513223731236598, num_roi: 51344
2024-03-31 02:01:05,152 INFO average coverage reward for trained map 19 in 1 steps: -100000.0, std: 0.0, optimal reward: 4090, ratio: -24.449877750611247, num_roi: 50608
2024-03-31 02:01:05,371 INFO average coverage reward for trained map 20 in 1 steps: -100000.0, std: 0.0, optimal reward: 4198, ratio: -23.820867079561697, num_roi: 57088
2024-03-31 02:01:05,662 INFO average coverage reward for trained map 21 in 1 steps: 3429.0, std: 0.0, optimal reward: 4387, ratio: 0.7816275359015272, num_roi: 54592
2024-03-31 02:01:05,971 INFO average coverage reward for trained map 22 in 1 steps: -100000.0, std: 0.0, optimal reward: 4588, ratio: -21.79598953792502, num_roi: 53664
2024-03-31 02:01:06,274 INFO average coverage reward for trained map 23 in 1 steps: -100000.0, std: 0.0, optimal reward: 4583, ratio: -21.81976871045167, num_roi: 53536
2024-03-31 02:01:06,637 INFO average coverage reward for trained map 24 in 1 steps: -100000.0, std: 0.0, optimal reward: 4077, ratio: -24.527839097375523, num_roi: 50688
2024-03-31 02:01:06,896 INFO average coverage reward for test map 0 in 1 steps: -100000.0, std:0.0, optimal reward: 4417, ratio: -22.639800769753226, num_roi: 54592
2024-03-31 02:01:07,184 INFO average coverage reward for test map 1 in 1 steps: -100000.0, std:0.0, optimal reward: 4231, ratio: -23.63507445048452, num_roi: 53472
2024-03-31 02:01:07,492 INFO average coverage reward for test map 2 in 1 steps: -100000.0, std:0.0, optimal reward: 4786, ratio: -20.89427496865859, num_roi: 52976
2024-03-31 02:01:07,820 INFO average coverage reward for test map 3 in 1 steps: 3208.0, std:0.0, optimal reward: 4319, ratio: 0.7427645288261172, num_roi: 52224
2024-03-31 02:01:08,130 INFO average coverage reward for test map 4 in 1 steps: -100000.0, std:0.0, optimal reward: 4162, ratio: -24.02691013935608, num_roi: 52688
2024-03-31 02:01:08,477 INFO average coverage reward for test map 5 in 1 steps: -100000.0, std:0.0, optimal reward: 4877, ratio: -20.50440844781628, num_roi: 51024
2024-03-31 02:01:08,760 INFO average coverage reward for test map 6 in 1 steps: 3676.0, std:0.0, optimal reward: 4356, ratio: 0.8438934802571166, num_roi: 53968
2024-03-31 02:01:09,091 INFO average coverage reward for test map 7 in 1 steps: -100000.0, std:0.0, optimal reward: 4279, ratio: -23.369946249123625, num_roi: 51472
2024-03-31 02:01:09,336 INFO average coverage reward for test map 8 in 1 steps: -100000.0, std:0.0, optimal reward: 4332, ratio: -23.08402585410896, num_roi: 56112
2024-03-31 02:01:09,674 INFO average coverage reward for test map 9 in 1 steps: -100000.0, std:0.0, optimal reward: 4781, ratio: -20.916126333403053, num_roi: 51760
2024-03-31 02:01:09,931 INFO average coverage reward for test map 10 in 1 steps: -100000.0, std:0.0, optimal reward: 4322, ratio: -23.137436372049976, num_roi: 55168
2024-03-31 02:01:10,240 INFO average coverage reward for test map 11 in 1 steps: -100000.0, std:0.0, optimal reward: 4175, ratio: -23.952095808383234, num_roi: 52944
2024-03-31 02:01:10,543 INFO average coverage reward for test map 12 in 1 steps: -100000.0, std:0.0, optimal reward: 4529, ratio: -22.079929344226098, num_roi: 53104
2024-03-31 02:01:10,818 INFO average coverage reward for test map 13 in 1 steps: -100000.0, std:0.0, optimal reward: 4699, ratio: -21.281123643328367, num_roi: 53744
2024-03-31 02:01:11,139 INFO average coverage reward for test map 14 in 1 steps: 2958.0, std:0.0, optimal reward: 4459, ratio: 0.663377438887643, num_roi: 52624
2024-03-31 02:01:11,478 INFO average coverage reward for test map 15 in 1 steps: -100000.0, std:0.0, optimal reward: 4465, ratio: -22.396416573348265, num_roi: 51648
2024-03-31 02:01:11,770 INFO average coverage reward for test map 16 in 1 steps: -100000.0, std:0.0, optimal reward: 4321, ratio: -23.142791020597084, num_roi: 53392
2024-03-31 02:01:12,138 INFO average coverage reward for test map 17 in 1 steps: 2745.0, std:0.0, optimal reward: 4303, ratio: 0.6379270276551243, num_roi: 50304
2024-03-31 02:01:12,476 INFO average coverage reward for test map 18 in 1 steps: -100000.0, std:0.0, optimal reward: 4603, ratio: -21.724961981316532, num_roi: 51344
2024-03-31 02:01:12,836 INFO average coverage reward for test map 19 in 1 steps: -100000.0, std:0.0, optimal reward: 4995, ratio: -20.02002002002002, num_roi: 50608
2024-03-31 02:01:13,021 INFO average coverage reward for test map 20 in 1 steps: 2976.0, std:0.0, optimal reward: 4219, ratio: 0.7053804219009244, num_roi: 57088
2024-03-31 02:01:13,276 INFO average coverage reward for test map 21 in 1 steps: -100000.0, std:0.0, optimal reward: 4513, ratio: -22.158209616662973, num_roi: 54592
2024-03-31 02:01:13,556 INFO average coverage reward for test map 22 in 1 steps: 2700.0, std:0.0, optimal reward: 4226, ratio: 0.6389020350212967, num_roi: 53664
2024-03-31 02:01:13,850 INFO average coverage reward for test map 23 in 1 steps: -100000.0, std:0.0, optimal reward: 4817, ratio: -20.75980900975711, num_roi: 53536
2024-03-31 02:01:14,213 INFO average coverage reward for test map 24 in 1 steps: -100000.0, std:0.0, optimal reward: 4072, ratio: -24.557956777996072, num_roi: 50688
2024-03-31 02:01:14,213 INFO overall average coverage reward for trained maps: -87586.4, average optimal reward: 4276.44, ratio: -20.481147870658774, average number of RoI pixels: 52989.439999999995, percentage of coverage: -165.2902918015363
2024-03-31 02:01:14,213 INFO overall average coverage reward for test maps: -75269.48, average optimal reward: 4450.32, ratio: -16.91327365223175, average number of RoI pixels: 52989.439999999995, percentage of coverage: -142.04618882554715
